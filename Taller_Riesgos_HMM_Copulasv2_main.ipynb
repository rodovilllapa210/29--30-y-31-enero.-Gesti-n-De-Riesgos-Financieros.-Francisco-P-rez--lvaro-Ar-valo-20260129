{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8c7813",
   "metadata": {},
   "source": [
    "# Taller - HMM, Cópulas y Stress Testing (2006-hoy)\n",
    "\n",
    "Notebook técnico para modelar cambios de régimen, dependencia y riesgo de cola en una cartera multi-activo long-only.\n",
    "\n",
    "Universo de activos: AAPL, AMZN, BAC, BRK-B, CVX, ENPH, GME, GOOGL, JNJ, JPM, MSFT, NVDA, PG, XOM, GLD, IEF, SHY, HYG.\n",
    "\n",
    "Rango temporal objetivo: desde 2006-01-01 hasta fecha disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751edcc",
   "metadata": {},
   "source": [
    "## Ejecución y reproducibilidad\n",
    "\n",
    "Objetivo técnico: que el notebook corra de principio a fin en cualquier equipo sin rutas absolutas.\n",
    "\n",
    "- El directorio de proyecto se detecta automáticamente.\n",
    "- Los datos se descargan desde Yahoo Finance solo si no hay cache local.\n",
    "- Se guardan CSV en `outputs_taller/` para acelerar ejecuciones futuras.\n",
    "- Se fija semilla en simulaciones para reproducibilidad.\n",
    "\n",
    "Nota de proxies (bonos/crédito):\n",
    "\n",
    "- 10Y UST -> `IEF`\n",
    "- 2Y UST -> `SHY`\n",
    "- High Yield -> `HYG`\n",
    "\n",
    "Estos proxies se usan para mantener frecuencia diaria uniforme y evitar incompatibilidades de fuentes heterogéneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b6254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:49.208384Z",
     "iopub.status.busy": "2026-02-11T18:18:49.208384Z",
     "iopub.status.idle": "2026-02-11T18:18:49.997989Z",
     "shell.execute_reply": "2026-02-11T18:18:49.997476Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importaciones necesarias para el análisis de riesgos financieros\n",
    "from __future__ import annotations  # Permite usar anotaciones de tipo futuras\n",
    "\n",
    "# Librerías estándar de Python\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Librerías científicas y de análisis de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf  # Para descargar datos financieros de Yahoo Finance\n",
    "\n",
    "# Control de advertencias durante la ejecución\n",
    "import warnings\n",
    "\n",
    "# Machine Learning - Modelos Hidden Markov\n",
    "from hmmlearn.hmm import GaussianHMM  # Modelo HMM con distribuciones gaussianas\n",
    "from sklearn.preprocessing import StandardScaler  # Para estandarizar características\n",
    "\n",
    "# Funciones matemáticas y estadísticas\n",
    "from scipy.special import logsumexp  # Para cálculo estable de log-sum-exp\n",
    "\n",
    "# Visualización de datos\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Manejo de datos JSON y distribuciones estadísticas\n",
    "import json\n",
    "from scipy.stats import norm, t  # Distribuciones normal y t-Student\n",
    "\n",
    "\n",
    "def find_project_root(\n",
    "    start_dir: Path,\n",
    "    markers: tuple[str, ...],\n",
    "    max_depth: int = 8,\n",
    " ) -> Path | None:\n",
    "    \"\"\"Encuentra el directorio raíz del proyecto de forma automática.\n",
    "\n",
    "    Evitamos codificar rutas absolutas. La raíz es el primer directorio que\n",
    "    contiene cualquiera de los archivos/marcadores especificados.\n",
    "    \"\"\"\n",
    "    current = start_dir.resolve()\n",
    "    for _ in range(max_depth + 1):\n",
    "        for marker in markers:\n",
    "            if (current / marker).exists():\n",
    "                return current\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Configuración de rutas (portable) ---\n",
    "# Marcadores que identifican el directorio raíz del proyecto\n",
    "MARKERS = (\n",
    "    \"Taller_Riesgos_HMM_Copulasv2_main.ipynb\",\n",
    "    \"Taller_Riesgos_HMM_Copulasv2.pdf\",\n",
    "    \"Taller_Riesgos_HMM_Copulasv2_analisis.md\",\n",
    " )\n",
    "\n",
    "# Override opcional (útil si se lanza Jupyter desde una carpeta diferente)\n",
    "# Ejemplo: set TALLER_DIR=\"C:\\\\...\\\\tema 6 Gestion de riesgos\"\n",
    "override = os.environ.get(\"TALLER_DIR\")\n",
    "if override:\n",
    "    PROJECT_DIR = Path(override).expanduser().resolve()\n",
    "else:\n",
    "    PROJECT_DIR = find_project_root(Path.cwd(), MARKERS) or Path.cwd().resolve()\n",
    "\n",
    "# Directorio de salida para resultados y datos cacheados\n",
    "OUT_DIR = PROJECT_DIR / \"outputs_taller\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rutas de archivos CSV para datos cacheados\n",
    "PRICES_CSV = OUT_DIR / \"prices_adj_close.csv\"  # Precios ajustados\n",
    "RETURNS_CSV = OUT_DIR / \"returns_log.csv\"      # Retornos logarítmicos\n",
    "\n",
    "# --- Universo de activos solicitado (acciones + bonos + HY + oro) ---\n",
    "# Tickers de acciones estadounidenses\n",
    "EQUITY_TICKERS = [\n",
    "    \"AAPL\",   # Apple Inc.\n",
    "    \"AMZN\",   # Amazon.com Inc.\n",
    "    \"BAC\",    # Bank of America Corp.\n",
    "    \"BRK-B\",  # Berkshire Hathaway Inc. Class B\n",
    "    \"CVX\",    # Chevron Corp.\n",
    "    \"ENPH\",   # Enphase Energy Inc.\n",
    "    \"GME\",    # GameStop Corp.\n",
    "    \"GOOGL\",  # Alphabet Inc. Class A\n",
    "    \"JNJ\",    # Johnson & Johnson\n",
    "    \"JPM\",    # JPMorgan Chase & Co.\n",
    "    \"MSFT\",   # Microsoft Corp.\n",
    "    \"NVDA\",   # NVIDIA Corp.\n",
    "    \"PG\",     # Procter & Gamble Co.\n",
    "    \"XOM\",    # Exxon Mobil Corp.\n",
    "]\n",
    "\n",
    "# Tickers de activos defensivos y de deuda\n",
    "GOLD_TICKER = \"GLD\"        # SPDR Gold Shares (ETF de oro)\n",
    "BOND_10Y_TICKER = \"IEF\"    # iShares 7-10 Year Treasury Bond ETF (proxy bonos 7-10a)\n",
    "BOND_2Y_TICKER = \"SHY\"     # iShares 1-3 Year Treasury Bond ETF (proxy bonos 1-3a)\n",
    "HY_TICKER = \"HYG\"          # iShares iBoxx $ High Yield Corporate Bond ETF (high yield)\n",
    "\n",
    "# Lista completa de todos los tickers a analizar\n",
    "TICKERS = EQUITY_TICKERS + [GOLD_TICKER, BOND_10Y_TICKER, BOND_2Y_TICKER, HY_TICKER]\n",
    "\n",
    "# Fechas de análisis\n",
    "START = \"2006-01-01\"  # Fecha de inicio del análisis\n",
    "END = None           # None => hasta la fecha actual (hoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32721b7",
   "metadata": {},
   "source": [
    "## 0) Descarga y preparación de datos\n",
    "\n",
    "Trabajamos con `Adj Close` para construir retornos logarítmicos diarios.\n",
    "\n",
    "Se construyen tres paneles de trabajo (CORE, FULL y diagnostico 2006) y el flujo principal usa CORE para preservar el episodio de 2008 sin imputar precios faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ed68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:50.000112Z",
     "iopub.status.busy": "2026-02-11T18:18:50.000112Z",
     "iopub.status.idle": "2026-02-11T18:18:50.045641Z",
     "shell.execute_reply": "2026-02-11T18:18:50.045641Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_prices(tickers: list[str], start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"Descarga precios históricos desde Yahoo Finance.\n",
    "    \n",
    "    Args:\n",
    "        tickers: Lista de símbolos de activos a descargar\n",
    "        start: Fecha de inicio (formato YYYY-MM-DD)\n",
    "        end: Fecha de fin (formato YYYY-MM-DD) o None para hoy\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con precios de cierre ajustados por activo\n",
    "    \"\"\"\n",
    "    # Descarga datos usando yfinance\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        auto_adjust=False,      # Mantener columnas originales (Adj Close, Close, etc.)\n",
    "        progress=False,         # No mostrar barra de progreso\n",
    "        group_by=\"column\",       # Agrupar por ticker (más fácil de procesar)\n",
    "    )\n",
    "    \n",
    "    # yfinance devuelve columnas MultiIndex cuando hay varios tickers\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Preferimos precios de cierre ajustados (Adj Close) que incluyen dividendos y splits\n",
    "        if \"Adj Close\" in data.columns.get_level_values(0):\n",
    "            prices = data[\"Adj Close\"].copy()\n",
    "        else:\n",
    "            # Fallback a Close si no hay Adj Close disponible\n",
    "            prices = data[\"Close\"].copy()\n",
    "    else:\n",
    "        # Caso especial: solo un ticker (estructura diferente)\n",
    "        prices = data[[\"Adj Close\"]].rename(columns={\"Adj Close\": tickers[0]})\n",
    "\n",
    "    # Asegurar formato correcto de fechas y orden cronológico\n",
    "    prices.index = pd.to_datetime(prices.index)\n",
    "    prices = prices.sort_index()\n",
    "    return prices\n",
    "\n",
    "\n",
    "def _normalize_cached_prices(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normaliza DataFrame cargado desde cache CSV.\n",
    "    \n",
    "    Limpia y estandariza el formato de datos cargados desde archivo.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame cargado desde CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame normalizado con fechas y columnas limpias\n",
    "    \"\"\"\n",
    "    # Algunos CSV pueden incluir columnas sin nombre; asegurar columnas/índice limpios\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)      # Convertir índice a datetime\n",
    "    df = df.sort_index()                     # Ordenar por fecha cronológica\n",
    "    df.columns = [str(c).strip() for c in df.columns]  # Limpiar nombres de columnas\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_or_download_prices(\n",
    "    tickers: list[str],\n",
    "    start: str,\n",
    "    end: str,\n",
    "    cache_path: Path,\n",
    " ) -> pd.DataFrame:\n",
    "    \"\"\"Carga precios desde cache o descarga si es necesario.\n",
    "    \n",
    "    Implementa sistema de cache para evitar descargas repetidas\n",
    "    y acelerar ejecuciones posteriores.\n",
    "    \n",
    "    Args:\n",
    "        tickers: Lista de símbolos de activos\n",
    "        start: Fecha de inicio\n",
    "        end: Fecha de fin\n",
    "        cache_path: Ruta del archivo cache\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con precios de los tickers solicitados\n",
    "    \"\"\"\n",
    "    tickers_set = set(tickers)\n",
    "    \n",
    "    # Verificar si existe cache válido\n",
    "    if cache_path.exists():\n",
    "        # Cargar datos cacheados y normalizar formato\n",
    "        cached = _normalize_cached_prices(pd.read_csv(cache_path, index_col=0, parse_dates=True))\n",
    "        cached_cols = set(cached.columns)\n",
    "        \n",
    "        # Si el cache contiene al menos los tickers necesarios, reutilizarlo (subset)\n",
    "        if tickers_set.issubset(cached_cols):\n",
    "            print(f\"✓ Cargando {len(tickers)} tickers desde cache: {cache_path.name}\")\n",
    "            return cached.loc[:, tickers]\n",
    "        # Si no, refrescar cache para el universo exacto solicitado\n",
    "        \n",
    "    # Descargar datos frescos si no hay cache o está incompleto\n",
    "    print(f\"⬇ Descargando {len(tickers)} tickers desde Yahoo Finance...\")\n",
    "    prices = download_prices(tickers, start, end)\n",
    "    \n",
    "    # Guardar en cache para futuras ejecuciones\n",
    "    prices.to_csv(cache_path, encoding=\"utf-8\")\n",
    "    print(f\" Cache guardado en: {cache_path}\")\n",
    "    \n",
    "    return prices\n",
    "\n",
    "\n",
    "# Ejecutar carga/descarga de precios para el universo completo\n",
    "prices = load_or_download_prices(TICKERS, START, END, PRICES_CSV)\n",
    "\n",
    "# Mostrar últimos precios y dimensiones del dataset\n",
    "print(f\"\\n Dataset de precios:\")\n",
    "print(f\"   - Período: {prices.index.min().date()} → {prices.index.max().date()}\")\n",
    "print(f\"   - Activos: {prices.shape[1]}\")\n",
    "print(f\"   - Días de trading: {prices.shape[0]:,}\")\n",
    "\n",
    "# Mostrar últimas filas y forma del DataFrame\n",
    "prices.tail(), prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967b8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:50.048785Z",
     "iopub.status.busy": "2026-02-11T18:18:50.047779Z",
     "iopub.status.idle": "2026-02-11T18:18:50.123892Z",
     "shell.execute_reply": "2026-02-11T18:18:50.123892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construcción de paneles temporales\n",
    "# ---------------------------------\n",
    "# Panel CORE:\n",
    "#   - Incluye solo tickers con histórico previo al shock de 2008.\n",
    "#   - Se usa para HMM, copulas, simulación y escenarios.\n",
    "# Panel FULL:\n",
    "#   - Mantiene el universo completo para chequeos de robustez.\n",
    "# Panel 2006:\n",
    "#   - Sirve como diagnóstico de cobertura desde 2006.\n",
    "#\n",
    "# En el resto del notebook, `common`, `prices` y `returns` se refieren al panel CORE.\n",
    "\n",
    "# Guardar copia del universo completo descargado\n",
    "prices_full = prices.copy()  # descargado (universo completo)\n",
    "\n",
    "# Determinar primera fecha disponible por ticker (para decidir composición de paneles)\n",
    "first_date = prices_full.apply(lambda s: s.first_valid_index()).dropna()\n",
    "first_date = first_date.sort_values()\n",
    "\n",
    "# Preservar listas originales del universo declarado\n",
    "TICKERS_FULL = list(TICKERS)\n",
    "EQUITY_TICKERS_FULL = list(EQUITY_TICKERS)\n",
    "\n",
    "# --- Definición de criterios temporales para paneles ---\n",
    "CORE_CUTOFF = pd.Timestamp(\"2008-09-01\")      # Antes del shock de Lehman (aprox.)\n",
    "LONG2006_CUTOFF = pd.Timestamp(\"2006-01-03\")  # Primeros días de trading de 2006\n",
    "\n",
    "# Panel CORE: tickers con datos disponibles antes de la crisis de 2008\n",
    "# Incluye HYG porque empezó en 2007, antes del colapso de Lehman\n",
    "TICKERS_CORE = [t for t in TICKERS_FULL if (t in first_date.index and first_date.loc[t] <= CORE_CUTOFF)]\n",
    "\n",
    "# Panel 2006: tickers con datos desde principios de 2006\n",
    "# No necesariamente incluye HYG (que empezó más tarde)\n",
    "TICKERS_2006 = [t for t in TICKERS_FULL if (t in first_date.index and first_date.loc[t] <= LONG2006_CUTOFF)]\n",
    "\n",
    "\n",
    "def build_panel(px: pd.DataFrame, tickers: list[str], start: str | None = None, end: str | None = None):\n",
    "    \"\"\"Construye panel temporal con precios y retornos logarítmicos.\n",
    "    \n",
    "    Args:\n",
    "        px: DataFrame de precios completos\n",
    "        tickers: Lista de tickers a incluir en el panel\n",
    "        start: Fecha de inicio (opcional)\n",
    "        end: Fecha de fin (opcional)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (precios_comunes, retornos_logaritmicos)\n",
    "    \"\"\"\n",
    "    # Seleccionar tickers específicos del universo completo\n",
    "    p = px.loc[:, tickers].copy()\n",
    "    \n",
    "    # Aplicar filtros temporales si se especifican\n",
    "    if start is not None:\n",
    "        p = p.loc[pd.to_datetime(start):]\n",
    "    if end is not None:\n",
    "        p = p.loc[:pd.to_datetime(end)]\n",
    "    \n",
    "    # Eliminar días con datos faltantes (requiere datos completos para todos los tickers)\n",
    "    common = p.dropna(how=\"any\")\n",
    "    \n",
    "    # Calcular retornos logarítmicos: r_t = log(P_t / P_{t-1})\n",
    "    returns = np.log(common / common.shift(1)).dropna()\n",
    "    \n",
    "    return common, returns\n",
    "\n",
    "\n",
    "# Construir los tres paneles temporales\n",
    "print(\" Construyendo paneles temporales...\")\n",
    "common_core, returns_core = build_panel(prices_full, TICKERS_CORE, start=START, end=END)\n",
    "common_full, returns_full = build_panel(prices_full, TICKERS_FULL, start=START, end=END)\n",
    "common_2006, returns_2006 = build_panel(prices_full, TICKERS_2006, start=START, end=END)\n",
    "\n",
    "\n",
    "def _panel_summary(name: str, tickers: list[str], common: pd.DataFrame, returns: pd.DataFrame):\n",
    "    \"\"\"Muestra resumen estadístico de un panel temporal.\n",
    "    \n",
    "    Args:\n",
    "        name: Nombre descriptivo del panel\n",
    "        tickers: Lista de tickers en el panel\n",
    "        common: DataFrame de precios comunes\n",
    "        returns: DataFrame de retornos\n",
    "    \"\"\"\n",
    "    if len(tickers) == 0:\n",
    "        print(f\"\\n[{name}] (vacío)\")\n",
    "        return\n",
    "    \n",
    "    # Identificar el ticker que limita el rango temporal (el que empieza más tarde)\n",
    "    limiter = first_date.loc[tickers].idxmax()\n",
    "    limiter_date = first_date.loc[tickers].max()\n",
    "    \n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\" tickers: {len(tickers)}\")\n",
    "    print(f\" range (prices):  {common.index.min().date()} -> {common.index.max().date()}  rows: {len(common):,}\")\n",
    "    print(f\" range (returns): {returns.index.min().date()} -> {returns.index.max().date()}  rows: {len(returns):,}\")\n",
    "    print(f\" limiting ticker (último en arrancar): {limiter}  first_date={limiter_date.date()}\")\n",
    "\n",
    "\n",
    "# Mostrar resúmenes de los tres paneles\n",
    "_panel_summary(\"CORE (<=2008)\", TICKERS_CORE, common_core, returns_core)\n",
    "_panel_summary(\"FULL (universo completo)\", TICKERS_FULL, common_full, returns_full)\n",
    "_panel_summary(\"2006 (diagnóstico)\", TICKERS_2006, common_2006, returns_2006)\n",
    "\n",
    "# Identificar y mostrar tickers excluidos del panel CORE\n",
    "excluded = sorted(set(TICKERS_FULL) - set(TICKERS_CORE))\n",
    "if excluded:\n",
    "    print(\"\\n Tickers EXCLUIDOS del CORE (no existían antes de 2008 o no tienen histórico suficiente):\")\n",
    "    print(excluded)\n",
    "\n",
    "# --- Selección del panel principal para el resto del análisis (CORE) ---\n",
    "# A partir de aquí, todas las variables principales se refieren al panel CORE\n",
    "common = common_core\n",
    "prices = common_core\n",
    "returns = returns_core\n",
    "\n",
    "# Actualizar listas de tickers para evitar errores en celdas posteriores\n",
    "TICKERS = list(common.columns)\n",
    "EQUITY_TICKERS = [t for t in EQUITY_TICKERS_FULL if t in TICKERS]\n",
    "\n",
    "print(f\"\\n Panel principal seleccionado: CORE con {len(TICKERS)} activos\")\n",
    "print(f\" Período de análisis: {returns.index.min().date()} → {returns.index.max().date()}\")\n",
    "\n",
    "# Sistema de cache para retornos del panel CORE\n",
    "rebuild_returns_cache = True\n",
    "if RETURNS_CSV.exists():\n",
    "    try:\n",
    "        cached_r = pd.read_csv(RETURNS_CSV, index_col=0, parse_dates=True)\n",
    "        cached_cols = [str(c).strip() for c in cached_r.columns]\n",
    "        rebuild_returns_cache = set(cached_cols) != set(returns.columns)\n",
    "    except Exception:\n",
    "        rebuild_returns_cache = True\n",
    "\n",
    "if rebuild_returns_cache:\n",
    "    print(f\" Guardando cache de retornos CORE en: {RETURNS_CSV.name}\")\n",
    "    returns.to_csv(RETURNS_CSV, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(f\"✓ Reutilizando cache de retornos CORE: {RETURNS_CSV.name}\")\n",
    "\n",
    "# Cache opcional del universo completo para análisis de robustez\n",
    "RETURNS_FULL_CSV = OUT_DIR / \"returns_log_full_universe.csv\"\n",
    "if not RETURNS_FULL_CSV.exists():\n",
    "    print(f\" Guardando cache de retornos FULL en: {RETURNS_FULL_CSV.name}\")\n",
    "    returns_full.to_csv(RETURNS_FULL_CSV, encoding=\"utf-8\")\n",
    "\n",
    "# Estadísticas descriptivas de los retornos del panel principal\n",
    "print(f\"\\n Estadísticas descriptivas - Retornos diarios del panel CORE:\")\n",
    "returns.describe().T[[\"mean\", \"std\", \"min\", \"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7618d",
   "metadata": {},
   "source": [
    "### Paneles de trabajo: CORE vs FULL\n",
    "\n",
    "Para cubrir bien los bloques de crisis y mantener robustez del universo:\n",
    "\n",
    "- `prices/common/returns` = panel **CORE** (activos con histórico util para crisis 2008). Este panel alimenta HMM, copulas, Monte Carlo y stress.\n",
    "- `prices_full/common_full/returns_full` = universo completo (robustez/apendice).\n",
    "- `common_2006/returns_2006` = diagnóstico de cobertura temporal desde 2006.\n",
    "\n",
    "Criterio metodológico: no se imputan precios faltantes para forzar activos que no existían."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f12cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:50.125977Z",
     "iopub.status.busy": "2026-02-11T18:18:50.125977Z",
     "iopub.status.idle": "2026-02-11T18:18:50.174744Z",
     "shell.execute_reply": "2026-02-11T18:18:50.174744Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Validación de datos y chequeos de integridad (versión rápida) ---\n",
    "print(\"\\n=== Validación de datos (precios/retornos) ===\")\n",
    "\n",
    "# 1) Dimensiones básicas y rango temporal\n",
    "print(f\"Precios:  {prices.shape[0]:,} filas x {prices.shape[1]} activos\")\n",
    "print(f\"Retornos: {returns.shape[0]:,} filas x {returns.shape[1]} activos\")\n",
    "print(f\"Rango de retornos: {returns.index.min().date()} -> {returns.index.max().date()}\")\n",
    "\n",
    "# 2) Integridad del índice temporal\n",
    "if not returns.index.is_monotonic_increasing:\n",
    "    raise ValueError(\"El índice de retornos no es creciente (monotónico)\")\n",
    "if returns.index.has_duplicates:\n",
    "    dup = returns.index[returns.index.duplicated()].unique()[:10]\n",
    "    raise ValueError(f\"El índice de retornos tiene duplicados (ej. {list(dup)})\")\n",
    "\n",
    "# 3) Verificación de valores faltantes e infinitos\n",
    "nan_prices = int(prices.isna().sum().sum())\n",
    "nan_returns = int(returns.isna().sum().sum())\n",
    "inf_returns = int(np.isinf(returns.to_numpy()).sum())\n",
    "print(f\"NaNs en precios:  {nan_prices}\")\n",
    "print(f\"NaNs en retornos: {nan_returns}\")\n",
    "print(f\"Infinitos en retornos:  {inf_returns}\")\n",
    "if nan_returns > 0 or inf_returns > 0:\n",
    "    raise ValueError(\"Los retornos contienen NaN/Inf después de la limpieza. Verificar paso de descarga/alineación.\")\n",
    "\n",
    "# 4) Verificación de precios no positivos (no debería ocurrir para ETFs)\n",
    "nonpos = (prices <= 0).sum().sum()\n",
    "print(f\"Precios no positivos: {int(nonpos)}\")\n",
    "if nonpos > 0:\n",
    "    raise ValueError(\"Se encontraron precios no positivos. Los datos están corruptos o la serie ajustada es inconsistente.\")\n",
    "\n",
    "# 5) Diagnóstico de cobertura: ¿qué tickers limitan la fecha de inicio común?\n",
    "coverage = pd.DataFrame({\n",
    "    \"first_date\": prices.apply(lambda s: s.first_valid_index()),\n",
    "    \"last_date\": prices.apply(lambda s: s.last_valid_index()),\n",
    "})\n",
    "coverage[\"n_obs\"] = prices.notna().sum(axis=0)\n",
    "coverage = coverage.sort_values(\"first_date\")\n",
    "display(coverage)\n",
    "print(\"\\nActivos con fecha de inicio más temprana (estos tienden a reducir el rango común porque empezaron más tarde):\")\n",
    "display(coverage.head(5))\n",
    "\n",
    "# 6) Análisis de valores extremos en retornos diarios (informativo)\n",
    "abs_r = returns.abs()\n",
    "max_abs = abs_r.max().sort_values(ascending=False)\n",
    "display(max_abs.to_frame(\"max_abs_daily_return\"))\n",
    "\n",
    "# Identificar movimientos diarios inusualmente grandes por activo\n",
    "# (movimientos >20% diarios son raros en ETFs, aunque algunos productos pueden superar este umbral)\n",
    "threshold = 0.20\n",
    "flagged = (abs_r > threshold).sum().sort_values(ascending=False)\n",
    "flagged = flagged[flagged > 0]\n",
    "print(f\"\\nConteo(|r|>{threshold:.0%}) por activo (solo >0 mostrados):\")\n",
    "display(flagged.to_frame(\"n_days\"))\n",
    "\n",
    "# Mostrar los 10 días con mayores movimientos absolutos en todo el universo\n",
    "top_moves = (abs_r.stack().sort_values(ascending=False).head(10)).reset_index()\n",
    "top_moves.columns = [\"date\", \"ticker\", \"abs_return\"]\n",
    "top_moves[\"return\"] = returns.stack().reindex(pd.MultiIndex.from_frame(top_moves[[\"date\",\"ticker\"]])).to_numpy()\n",
    "display(top_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a677945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:50.176793Z",
     "iopub.status.busy": "2026-02-11T18:18:50.176793Z",
     "iopub.status.idle": "2026-02-11T18:18:50.298830Z",
     "shell.execute_reply": "2026-02-11T18:18:50.298830Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Validación de datos y chequeos de integridad (versión avanzada, rápida y compacta) ---\n",
    "\n",
    "print(\"\\n=== Validación de datos (avanzada) ===\")\n",
    "\n",
    "# A) Análisis de gaps calendario (informativo: los festivos son esperados)\n",
    "idx = returns.index\n",
    "bday_index = pd.bdate_range(idx.min(), idx.max())\n",
    "missing_bdays = bday_index.difference(idx)\n",
    "print(f\"Días laborables faltantes en el índice de retornos (informativo): {len(missing_bdays)}\")\n",
    "if len(missing_bdays) > 0:\n",
    "    print(\"Primeros días laborables faltantes (generalmente festivos):\", [d.date().isoformat() for d in missing_bdays[:10]])\n",
    "\n",
    "# Identificar gaps grandes (>=7 días calendario) que podrían indicar problemas\n",
    "gap_days = idx.to_series().diff().dt.days.dropna()\n",
    "large_gaps = gap_days[gap_days >= 7]\n",
    "print(f\"Gaps >=7 días calendario en el índice de retornos: {len(large_gaps)}\")\n",
    "if len(large_gaps) > 0:\n",
    "    print(\"Primeros gaps grandes:\")\n",
    "    display(large_gaps.head(10).to_frame(\"gap_days\"))\n",
    "\n",
    "# B) Consistencia: recalcular retornos desde precios alineados y comparar\n",
    "common_prices = prices.loc[idx.min() : idx.max()].copy()\n",
    "common_prices = common_prices.reindex(idx)\n",
    "recalc = np.log(common_prices / common_prices.shift(1)).dropna()\n",
    "recalc = recalc.reindex_like(returns)\n",
    "diff = (returns - recalc).abs()\n",
    "max_diff = float(np.nanmax(diff.to_numpy()))\n",
    "print(f\"Máxima diferencia |retornos - recalculados|: {max_diff:.3e}\")\n",
    "if not np.isfinite(max_diff):\n",
    "    raise ValueError(\"Diferencia no finita al recalcular retornos\")\n",
    "if max_diff > 1e-10:\n",
    "    warnings.warn(\"Los retornos difieren de la recomputación más que la tolerancia. Verificar alineación/series de precios.\")\n",
    "\n",
    "# C) Precios estancados/planos: contar cierres repetidos (redondeados para evitar ruido de punto flotante)\n",
    "flat_counts = {}\n",
    "for col in prices.columns:\n",
    "    s = prices[col].dropna().round(6)\n",
    "    flat_counts[col] = int(s.diff().eq(0).sum()) if len(s) else 0\n",
    "flat = pd.Series(flat_counts, name=\"n_flat_days\").sort_values(ascending=False)\n",
    "print(\"\\nDías con precios planos (top 10):\")\n",
    "display(flat.head(10).to_frame())\n",
    "\n",
    "# D) Detección robusta de outliers via z-score MAD (vectorizado)\n",
    "# MAD = Median Absolute Deviation, más robusto que desviación estándar\n",
    "med = returns.median(axis=0)\n",
    "mad = (returns.sub(med, axis=1)).abs().median(axis=0)\n",
    "scale = 1.4826 * mad  # Factor para consistencia con desviación estándar en distribución normal\n",
    "scale = scale.replace(0, np.nan)\n",
    "z = returns.sub(med, axis=1).div(scale, axis=1)\n",
    "z_abs = z.abs()\n",
    "z_max = z_abs.max(axis=0).sort_values(ascending=False)\n",
    "print(\"\\nZ-score robusto máximo por activo:\")\n",
    "display(z_max.to_frame(\"robust_z_max\"))\n",
    "\n",
    "# Identificar eventos extremos (z-score > 8)\n",
    "z_thr = 8.0\n",
    "z_hits = (z_abs > z_thr).sum(axis=0).sort_values(ascending=False)\n",
    "z_hits = z_hits[z_hits > 0]\n",
    "print(f\"Conteo(|z_robusto|>{z_thr}) por activo (solo >0 mostrados):\")\n",
    "display(z_hits.to_frame(\"n_days\"))\n",
    "\n",
    "if len(z_hits) > 0:\n",
    "    top_events = z_abs.stack().sort_values(ascending=False).head(10).reset_index()\n",
    "    top_events.columns = [\"date\", \"ticker\", \"abs_robust_z\"]\n",
    "    top_events[\"return\"] = returns.stack().reindex(pd.MultiIndex.from_frame(top_events[[\"date\", \"ticker\"]])).to_numpy()\n",
    "    print(\"Top eventos outliers robustos:\")\n",
    "    display(top_events)\n",
    "\n",
    "# E) Resumen estadístico de distribuciones\n",
    "stats = pd.DataFrame({\n",
    "    \"mean_daily\": returns.mean(),\n",
    "    \"vol_daily\": returns.std(ddof=1),\n",
    "    \"skew\": returns.skew(),\n",
    "    \"kurtosis\": returns.kurtosis(),\n",
    "    \"p01\": returns.quantile(0.01),\n",
    "    \"p99\": returns.quantile(0.99),\n",
    "})\n",
    "stats[\"vol_ann\"] = stats[\"vol_daily\"] * np.sqrt(252)  # Annualización (252 días laborables)\n",
    "print(\"\\nEstadísticas resumen (ordenadas por volatilidad anualizada):\")\n",
    "display(stats.sort_values(\"vol_ann\", ascending=False))\n",
    "\n",
    "# Identificar activos con volatilidades inusualmente bajas o altas\n",
    "low_vol = stats[stats[\"vol_ann\"] < 0.03]\n",
    "high_vol = stats[stats[\"vol_ann\"] > 1.50]\n",
    "if len(low_vol) > 0:\n",
    "    warnings.warn(f\"Activos con volatilidad anual muy baja (<3%): {list(low_vol.index)}\")\n",
    "if len(high_vol) > 0:\n",
    "    warnings.warn(f\"Activos con volatilidad anual muy alta (>150%): {list(high_vol.index)}\")\n",
    "\n",
    "# F) Verificaciones PSD para correlación/covarianza (crucial para modelos de riesgo)\n",
    "# PSD = Positive Semi-Definite, requisito matemático para matrices de correlación/covarianza\n",
    "corr = returns.corr()\n",
    "eig_corr = np.linalg.eigvalsh(corr.to_numpy())\n",
    "min_eig_corr = float(eig_corr.min())\n",
    "print(f\"\\nValor propio mínimo (correlación): {min_eig_corr:.3e}\")\n",
    "if min_eig_corr < -1e-8:\n",
    "    warnings.warn(\"La matriz de correlación no es PSD. Considerar shrinkage/proyección PSD.\")\n",
    "\n",
    "cov = returns.cov()\n",
    "eig_cov = np.linalg.eigvalsh(cov.to_numpy())\n",
    "min_eig_cov = float(eig_cov.min())\n",
    "print(f\"Valor propio mínimo (covarianza):  {min_eig_cov:.3e}\")\n",
    "if min_eig_cov < -1e-12:\n",
    "    warnings.warn(\"La matriz de covarianza tiene valores propios negativos. Considerar shrinkage/proyección PSD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1dc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:18:50.301998Z",
     "iopub.status.busy": "2026-02-11T18:18:50.300891Z",
     "iopub.status.idle": "2026-02-11T18:19:02.353234Z",
     "shell.execute_reply": "2026-02-11T18:19:02.351712Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- HMM: Entrenamiento y probabilidades de régimen (2 estados) ---\n",
    "\n",
    "# 1) Construir características aptas para regímenes (observables en tiempo real)\n",
    "#    Usando solo información del universo: cesta equity + tipos + HY + oro\n",
    "#    (Core-only: conjunto de features parsimonioso y estable.)\n",
    "\n",
    "idx = returns.index\n",
    "px = common.reindex(idx)  # precios alineados (rango común)\n",
    "\n",
    "# --- Señales principales (nuevo universo) ---\n",
    "# --- Proxy Equity SOLO para HMM: excluye GME (riesgo meme/outlier) ---\n",
    "EQUITY_TICKERS_HMM = [t for t in EQUITY_TICKERS if t != \"GME\" and t in returns.columns]\n",
    "\n",
    "if len(EQUITY_TICKERS_HMM) < 5:\n",
    "    raise ValueError(f\"EQUITY_TICKERS_HMM demasiado pequeño: {EQUITY_TICKERS_HMM}\")\n",
    "\n",
    "# Retorno diario de la cesta equity (promedio simple)\n",
    "r_equity = returns[EQUITY_TICKERS_HMM].mean(axis=1)\n",
    "\n",
    "# Retornos individuales de activos clave\n",
    "r_hy = returns[HY_TICKER]      # High Yield (crédito de alto riesgo)\n",
    "r_10y = returns[BOND_10Y_TICKER]  # Bonos 7-10a (tipos medios)\n",
    "r_2y = returns[BOND_2Y_TICKER]    # Bonos 1-3a (tipos cortos)\n",
    "r_gld = returns[GOLD_TICKER]      # Oro (refugio)\n",
    "\n",
    "# Volatilidad realizada (21 días hábiles ~ 1 mes)\n",
    "vol_equity_21 = r_equity.rolling(21).std(ddof=1)\n",
    "vol_hy_21 = r_hy.rolling(21).std(ddof=1)\n",
    "\n",
    "# Drawdown rolling para cesta equity (máximo rolling 1año)\n",
    "px_equity = px[EQUITY_TICKERS_HMM].mean(axis=1)  # consistencia: mismo universo ex GME para todo el HMM\n",
    "log_equity = np.log(px_equity)\n",
    "roll_max_1y = log_equity.rolling(252, min_periods=60).max()\n",
    "dd_equity = log_equity - roll_max_1y  # <= 0, más negativo = drawdown más profundo\n",
    "\n",
    "# Proxy de riesgo de crédito: High Yield vs Bonos 10a\n",
    "r_credit = r_hy - r_10y\n",
    "\n",
    "# Construir DataFrame de features para el modelo HMM\n",
    "features = pd.DataFrame(\n",
    "    {\n",
    "        \"r_equity\": r_equity,        # Retorno cesta equity\n",
    "        \"r_hy\": r_hy,                # Retorno High Yield\n",
    "        \"r_10y\": r_10y,              # Retorno Bonos 10a\n",
    "        \"r_2y\": r_2y,                # Retorno Bonos 2a\n",
    "        \"r_gld\": r_gld,              # Retorno Oro\n",
    "        \"r_credit\": r_credit,        # Spread crédito (HY - 10a)\n",
    "        \"vol_equity_21\": vol_equity_21,  # Volatilidad equity 21d\n",
    "        \"vol_hy_21\": vol_hy_21,      # Volatilidad HY 21d\n",
    "        \"dd_equity\": dd_equity,      # Drawdown equity\n",
    "    },\n",
    "    index=idx,\n",
    ")\n",
    "\n",
    "# Eliminar filas con datos faltantes (requerido para HMM)\n",
    "features = features.dropna()\n",
    "\n",
    "# Mantener slice de retornos alineado con features para resúmenes posteriores\n",
    "returns_hmm = returns.loc[features.index]\n",
    "\n",
    "# Convertir features a numpy array para el modelo\n",
    "X = features.to_numpy(dtype=float)\n",
    "\n",
    "# ---- Configuración de modo tiempo real / sin look-ahead ----\n",
    "REALTIME_MODE = True   # False para ajuste full-sample (más rápido, pero usa look-ahead)\n",
    "MIN_TRAIN = 252        # Ventana 1año: permite detectar 2008 con panel CORE (arranca en 2007-04)\n",
    "REFIT_EVERY = 63       # Frecuencia de reajuste (días, ~3 meses)\n",
    "\n",
    "\n",
    "def _make_hmm() -> GaussianHMM:\n",
    "    \"\"\"Crea instancia de HMM con configuración estándar.\n",
    "    \n",
    "    Returns:\n",
    "        GaussianHMM configurado con 2 estados y priors de persistencia\n",
    "    \"\"\"\n",
    "    # Prior de transición fuerte en diagonal (persistencia de regímenes)\n",
    "    trans_prior = np.array([[200.0, 1.0], [1.0, 200.0]])\n",
    "    return GaussianHMM(\n",
    "        n_components=2,              # 2 estados: Normal vs Crisis\n",
    "        covariance_type=\"diag\",      # Covarianza diagonal (más robusto)\n",
    "        n_iter=500,                  # Máximo iteraciones EM\n",
    "        tol=1e-4,                    # Tolerancia de convergencia\n",
    "        random_state=42,             # Semilla para reproducibilidad\n",
    "        transmat_prior=trans_prior,  # Prior de matriz de transición\n",
    "    )\n",
    "\n",
    "\n",
    "def _forward_filter_probs(model: GaussianHMM, X_scaled: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calcula probabilidades forward-filtered (sin información futura).\n",
    "    \n",
    "    Implementa el algoritmo forward para obtener P(S_t | X_{1:t})\n",
    "    sin usar información futura (crucial para tiempo real).\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo HMM entrenado\n",
    "        X_scaled: Features estandarizadas\n",
    "        \n",
    "    Returns:\n",
    "        Array de probabilidades de estado (T, K)\n",
    "    \"\"\"\n",
    "    # Calcular log-verosimilitud de observaciones\n",
    "    logB = model._compute_log_likelihood(X_scaled)\n",
    "    log_start = np.log(model.startprob_)\n",
    "    log_trans = np.log(model.transmat_)\n",
    "    T, K = logB.shape\n",
    "    \n",
    "    # Inicializar forward probabilities\n",
    "    log_alpha = np.zeros((T, K))\n",
    "    log_alpha[0] = log_start + logB[0]\n",
    "    log_alpha[0] -= logsumexp(log_alpha[0])\n",
    "    \n",
    "    # Recursión forward\n",
    "    for t in range(1, T):\n",
    "        log_alpha[t] = logB[t] + logsumexp(log_alpha[t - 1][:, None] + log_trans, axis=0)\n",
    "        log_alpha[t] -= logsumexp(log_alpha[t])\n",
    "    \n",
    "    return np.exp(log_alpha)\n",
    "\n",
    "\n",
    "if not REALTIME_MODE:\n",
    "    # --- MODO FULL-SAMPLE (más rápido, pero usa look-ahead) ---\n",
    "    print(\" Modo FULL-SAMPLE: ajuste con toda la historia disponible\")\n",
    "    \n",
    "    # Estandarizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Ajustar modelo HMM\n",
    "    hmm = _make_hmm()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        hmm.fit(X_scaled)\n",
    "\n",
    "    # Decodificar estados y calcular probabilidades posteriores\n",
    "    state_seq_raw = hmm.predict(X_scaled)\n",
    "    state_prob_raw = hmm.predict_proba(X_scaled)  # shape: (T, n_states)\n",
    "\n",
    "    # Determinar qué estado es \"crisis\" (heurística): mayor vol + peor retorno equity\n",
    "    tmp = pd.DataFrame({\n",
    "        \"equity_mean\": r_equity.loc[features.index].groupby(state_seq_raw).mean(),\n",
    "        \"equity_vol\": r_equity.loc[features.index].groupby(state_seq_raw).std(ddof=1),\n",
    "        \"vol_equity_21\": features[\"vol_equity_21\"].groupby(state_seq_raw).mean(),\n",
    "        \"dd_equity\": features[\"dd_equity\"].groupby(state_seq_raw).mean(),\n",
    "        \"credit_mean\": features[\"r_credit\"].groupby(state_seq_raw).mean(),\n",
    "    })\n",
    "    display(tmp)\n",
    "\n",
    "    # Criterio: Crisis = menor equity_mean; desempates: mayor vol_equity_21, dd_equity más negativo, peor credit_mean\n",
    "    crisis_state = tmp.sort_values([\"equity_mean\", \"vol_equity_21\", \"dd_equity\", \"credit_mean\"], ascending=[True, False, True, True]).index[0]\n",
    "    normal_state = [s for s in range(hmm.n_components) if s != crisis_state][0]\n",
    "    print(f\"Estado crisis = {crisis_state}; Estado normal = {normal_state}\")\n",
    "\n",
    "    # Construir DataFrame de régimen con nomenclatura consistente\n",
    "    regime = pd.DataFrame({\n",
    "        \"state_raw\": state_seq_raw,\n",
    "        \"p_state0\": state_prob_raw[:, 0],\n",
    "        \"p_state1\": state_prob_raw[:, 1],\n",
    "    }, index=features.index)\n",
    "    regime[\"p_crisis\"] = regime[\"p_state0\"] if crisis_state == 0 else regime[\"p_state1\"]\n",
    "    regime[\"state\"] = np.where(regime[\"state_raw\"] == crisis_state, \"crisis\", \"normal\")\n",
    "\n",
    "else:\n",
    "    # --- MODO TIEMPO REAL (walk-forward, sin look-ahead) ---\n",
    "    print(\" Modo TIEMPO REAL: ajuste walk-forward sin información futura\")\n",
    "    \n",
    "    T = len(X)\n",
    "    if T < MIN_TRAIN:\n",
    "        raise ValueError(f\"Datos insuficientes para REALTIME_MODE: se necesitan >= {MIN_TRAIN}, hay {T}\")\n",
    "\n",
    "    # Inicializar arrays para almacenar resultados\n",
    "    state_prob_raw = np.full((T, 2), np.nan)\n",
    "    state_seq_raw = np.full(T, np.nan)\n",
    "    p_crisis = np.full(T, np.nan)\n",
    "    state_label = np.full(T, None, dtype=object)\n",
    "\n",
    "    block_rows = []\n",
    "    hmm_last = None\n",
    "    crisis_state_last = None\n",
    "    tmp_last = None\n",
    "\n",
    "    # Loop walk-forward: ajustar cada REFIT_EVERY días\n",
    "    for t0 in range(MIN_TRAIN, T, REFIT_EVERY):\n",
    "        t1 = min(t0 + REFIT_EVERY, T)\n",
    "\n",
    "        # Ajustar scaler con datos hasta t0 (sin look-ahead)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X[:t0])\n",
    "\n",
    "        # Ajustar HMM con datos históricos hasta t0\n",
    "        hmm = _make_hmm()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            hmm.fit(X_train)\n",
    "\n",
    "        # Determinar estado crisis usando solo datos pasados (hasta t0)\n",
    "        state_seq_train = hmm.predict(X_train)\n",
    "        tmp = pd.DataFrame({\n",
    "            \"equity_mean\": r_equity.loc[features.index].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"equity_vol\": r_equity.loc[features.index].iloc[:t0].groupby(state_seq_train).std(ddof=1),\n",
    "            \"vol_equity_21\": features[\"vol_equity_21\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"dd_equity\": features[\"dd_equity\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"credit_mean\": features[\"r_credit\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "        })\n",
    "        crisis_state = tmp.sort_values([\"equity_mean\", \"vol_equity_21\", \"dd_equity\", \"credit_mean\"], ascending=[True, False, True, True]).index[0]\n",
    "\n",
    "        # Filtrar probabilidades forward en el siguiente bloque (sin info futura)\n",
    "        X_block = scaler.transform(X[t0:t1])\n",
    "        prob_block = _forward_filter_probs(hmm, X_block)\n",
    "\n",
    "        # Almacenar resultados del bloque\n",
    "        state_prob_raw[t0:t1] = prob_block\n",
    "        state_seq_raw[t0:t1] = prob_block.argmax(axis=1)\n",
    "        p_crisis[t0:t1] = prob_block[:, crisis_state]\n",
    "        state_label[t0:t1] = np.where(prob_block.argmax(axis=1) == crisis_state, \"crisis\", \"normal\")\n",
    "\n",
    "        # Guardar información del bloque para diagnóstico\n",
    "        block_rows.append({\n",
    "            \"start\": features.index[t0],\n",
    "            \"end\": features.index[t1 - 1],\n",
    "            \"train_end\": features.index[t0 - 1],\n",
    "            \"crisis_state\": int(crisis_state),\n",
    "            \"n_train\": int(t0),\n",
    "            \"n_block\": int(t1 - t0),\n",
    "        })\n",
    "\n",
    "        hmm_last = hmm\n",
    "        crisis_state_last = crisis_state\n",
    "        tmp_last = tmp\n",
    "\n",
    "    # Construir DataFrame final de régimen\n",
    "    regime = pd.DataFrame({\n",
    "        \"state_raw\": state_seq_raw,\n",
    "        \"p_state0\": state_prob_raw[:, 0],\n",
    "        \"p_state1\": state_prob_raw[:, 1],\n",
    "        \"p_crisis\": p_crisis,\n",
    "        \"state\": state_label,\n",
    "    }, index=features.index)\n",
    "\n",
    "    # Mostrar información de bloques para diagnóstico\n",
    "    block_info = pd.DataFrame(block_rows)\n",
    "    display(block_info.tail())\n",
    "\n",
    "    if hmm_last is None:\n",
    "        raise RuntimeError(\"REALTIME_MODE falló al ajustar cualquier bloque.\")\n",
    "\n",
    "    # Usar último modelo ajustado como referencia\n",
    "    hmm = hmm_last\n",
    "    crisis_state = crisis_state_last\n",
    "    normal_state = [s for s in range(hmm.n_components) if s != crisis_state][0]\n",
    "    print(f\"(Tiempo real) Último ajuste: estado crisis = {crisis_state}; Estado normal = {normal_state}\")\n",
    "    display(tmp_last)\n",
    "\n",
    "# Mostrar matriz de transición del último ajuste\n",
    "trans = pd.DataFrame(hmm.transmat_, columns=[\"to_state0\", \"to_state1\"], index=[\"from_state0\", \"from_state1\"])\n",
    "print(\"\\n Matriz de transición (estados crudos):\")\n",
    "display(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7e21d",
   "metadata": {},
   "source": [
    "### Decisión de modelado (HMM)\n",
    "\n",
    "- Modelo parsimonioso: se evita sobrecargar el HMM con demasiadas features.\n",
    "- El proxy de equity para detección de regímenes excluye GME para reducir ruido idiosincrático extremo.\n",
    "- En modo real-time se usa `MIN_TRAIN=252` para cubrir 2008 con el panel CORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d2a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:02.355756Z",
     "iopub.status.busy": "2026-02-11T18:19:02.355756Z",
     "iopub.status.idle": "2026-02-11T18:19:02.393954Z",
     "shell.execute_reply": "2026-02-11T18:19:02.393954Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Diagnósticos: cómo se ven los regímenes (medias/volatilidades) ---\n",
    "\n",
    "def summarize_by_state(returns_df: pd.DataFrame, state_labels: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Resume estadísticas de retornos por estado del HMM.\n",
    "    \n",
    "    Calcula métricas clave (media, volatilidad, percentiles) para cada activo\n",
    "    separando por régimen (normal vs crisis).\n",
    "    \n",
    "    Args:\n",
    "        returns_df: DataFrame de retornos\n",
    "        state_labels: Serie con etiquetas de estado por fecha\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame multi-índice con estadísticas por (estado, ticker)\n",
    "    \"\"\"\n",
    "    # Alinear etiquetas de estado con índice de retornos\n",
    "    state_labels = pd.Series(state_labels, index=returns_df.index).reindex(returns_df.index)\n",
    "    out = []\n",
    "    \n",
    "    # Calcular estadísticas para cada estado\n",
    "    for st in [\"normal\", \"crisis\"]:\n",
    "        mask = state_labels == st\n",
    "        chunk = returns_df.loc[mask]\n",
    "        \n",
    "        # Estadísticas descriptivas para el estado actual\n",
    "        stats = pd.DataFrame(\n",
    "            {\n",
    "                \"mean_daily\": chunk.mean(),                    # Media diaria\n",
    "                \"vol_daily\": chunk.std(ddof=1),               # Volatilidad diaria\n",
    "                \"mean_ann\": chunk.mean() * 252,               # Media anualizada (252 días)\n",
    "                \"vol_ann\": chunk.std(ddof=1) * np.sqrt(252),  # Volatilidad anualizada\n",
    "                \"p01\": chunk.quantile(0.01),                  # Percentil 1% (cola izquierda)\n",
    "                \"p99\": chunk.quantile(0.99),                  # Percentil 99% (cola derecha)\n",
    "            }\n",
    "        )\n",
    "        stats[\"state\"] = st\n",
    "        stats = stats.rename_axis(\"ticker\").reset_index()\n",
    "        out.append(stats)\n",
    "    \n",
    "    # Combinar resultados y estructurar como multi-índice\n",
    "    res = pd.concat(out, ignore_index=True)\n",
    "    return res.set_index([\"state\", \"ticker\"]).sort_index()\n",
    "\n",
    "\n",
    "# Calcular resumen estadístico por régimen para todos los activos\n",
    "print(\" Estadísticas de retornos por régimen:\")\n",
    "summary = summarize_by_state(returns_hmm, regime[\"state\"])\n",
    "display(summary)\n",
    "\n",
    "# Tabla comparativa: diferencias crisis vs normal (medias y volatilidades anualizadas)\n",
    "print(\"\\n Diferencias Crisis - Normal (impacto del régimen):\")\n",
    "cmp = (summary.xs(\"crisis\")[[\"mean_ann\", \"vol_ann\"]] - summary.xs(\"normal\")[[\"mean_ann\", \"vol_ann\"]])\n",
    "cmp = cmp.rename(columns={\"mean_ann\": \"delta_mean_ann\", \"vol_ann\": \"delta_vol_ann\"}).sort_values(\"delta_vol_ann\", ascending=False)\n",
    "display(cmp)\n",
    "\n",
    "# Interpretación de resultados clave\n",
    "print(\"\\n Interpretación de diferencias:\")\n",
    "print(\"• delta_mean_ann: Cambio en retorno esperado anualizado (Crisis - Normal)\")\n",
    "print(\"• delta_vol_ann: Cambio en volatilidad anualizada (Crisis - Normal)\")\n",
    "print(\"• Valores positivos en delta_vol_ann indican mayor riesgo en crisis\")\n",
    "print(\"• Valores negativos en delta_mean_ann indican peores retornos en crisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a1804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:02.396200Z",
     "iopub.status.busy": "2026-02-11T18:19:02.396200Z",
     "iopub.status.idle": "2026-02-11T18:19:03.484473Z",
     "shell.execute_reply": "2026-02-11T18:19:03.483962Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Gráficos (estilo informe): probabilidad + banda de régimen + cesta de acciones ---\n",
    "\n",
    "# Configurar tema visual para los gráficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ---- Controles de presentación (macro + cobertura 2008) ----\n",
    "# Se prioriza una señal macro estable que conserve el bloque 2008-2009 en el panel CORE.\n",
    "# Se prueba una cuadrícula de sensibilidad y se elige la primera configuración que detecte crisis en 2008.\n",
    "PARAM_CANDIDATES = [\n",
    "    {\"name\": \"macro_strict\", \"smooth_span\": 60, \"enter\": 0.70, \"exit\": 0.30, \"min_days\": 60, \"max_gap\": 10},\n",
    "    {\"name\": \"balanced\", \"smooth_span\": 30, \"enter\": 0.60, \"exit\": 0.40, \"min_days\": 30, \"max_gap\": 8},\n",
    "    {\"name\": \"sensitive_2008\", \"smooth_span\": 20, \"enter\": 0.55, \"exit\": 0.35, \"min_days\": 20, \"max_gap\": 6},\n",
    "    {\"name\": \"very_sensitive\", \"smooth_span\": 15, \"enter\": 0.50, \"exit\": 0.35, \"min_days\": 15, \"max_gap\": 5},\n",
    "]\n",
    "# Fechas objetivo para capturar la crisis de 2008\n",
    "TARGET_2008_START = pd.Timestamp(\"2008-09-01\")\n",
    "TARGET_2008_END = pd.Timestamp(\"2009-06-30\")\n",
    "\n",
    "def _runs_to_segments(flags: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    \"\"\"Convierte series de banderas booleanas en segmentos de tiempo continuos.\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana que indica períodos de crisis\n",
    "        \n",
    "    Returns:\n",
    "        Lista de tuplas (inicio, fin) para cada segmento continuo de True\n",
    "    \"\"\"\n",
    "    flags = flags.astype(bool)\n",
    "    if len(flags) == 0:\n",
    "        return []\n",
    "    x = flags.to_numpy(copy=True)\n",
    "    idx = flags.index\n",
    "    # Detectar cambios de estado\n",
    "    chg = np.flatnonzero(x[1:] != x[:-1]) + 1\n",
    "    starts = np.r_[0, chg]\n",
    "    ends = np.r_[chg, len(x)]\n",
    "    segs = []\n",
    "    # Extraer solo segmentos donde el estado es True (crisis)\n",
    "    for s, e in zip(starts, ends):\n",
    "        if x[s]:\n",
    "            segs.append((idx[s], idx[e - 1]))\n",
    "    return segs\n",
    "\n",
    "def clean_regime_flags(flags: pd.Series, *, min_true: int, max_false_gap: int) -> pd.Series:\n",
    "    \"\"\"Post-procesa regímenes booleanos para evitar gráficos tipo 'código de barras'.\n",
    "    \n",
    "    Elimina ruido en las señales de crisis mediante dos reglas:\n",
    "    - Rellena gaps cortos de False dentro de corridas de True (<= max_false_gap)\n",
    "    - Elimina corridas cortas de True (< min_true)\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana original de regímenes\n",
    "        min_true: Duración mínima en días para considerar una crisis válida\n",
    "        max_false_gap: Máximo número de días falsos permitidos dentro de una crisis\n",
    "        \n",
    "    Returns:\n",
    "        Serie booleana limpia y estabilizada\n",
    "    \"\"\"\n",
    "    flags = flags.astype(bool).copy()\n",
    "    x = flags.to_numpy(copy=True)\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return flags\n",
    "\n",
    "    def _rle(arr: np.ndarray):\n",
    "        \"\"\"Run-length encoding: codifica secuencias consecutivas iguales.\"\"\"\n",
    "        chg = np.flatnonzero(arr[1:] != arr[:-1]) + 1\n",
    "        starts = np.r_[0, chg]\n",
    "        ends = np.r_[chg, n]\n",
    "        vals = arr[starts]\n",
    "        lens = ends - starts\n",
    "        return starts, ends, vals, lens\n",
    "\n",
    "    # 1) Rellenar gaps cortos de False rodeados de True\n",
    "    starts, ends, vals, lens = _rle(x)\n",
    "    for s, e, v, L in zip(starts, ends, vals, lens):\n",
    "        if (not v) and (L <= max_false_gap):\n",
    "            left_true = (s > 0) and x[s - 1]\n",
    "            right_true = (e < n) and x[e]\n",
    "            if left_true and right_true:\n",
    "                x[s:e] = True\n",
    "\n",
    "    # 2) Eliminar corridas cortas de True\n",
    "    starts, ends, vals, lens = _rle(x)\n",
    "    for s, e, v, L in zip(starts, ends, vals, lens):\n",
    "        if v and (L < min_true):\n",
    "            x[s:e] = False\n",
    "\n",
    "    return pd.Series(x, index=flags.index, name=flags.name)\n",
    "\n",
    "def hysteresis_flags(p: pd.Series, *, enter: float, exit: float) -> pd.Series:\n",
    "    \"\"\"Convierte una serie de probabilidades en un régimen booleano estable usando histéresis.\n",
    "    \n",
    "    La histéresis evita cambios frecuentes de estado usando umbrales diferentes\n",
    "    para entrar y salir de crisis.\n",
    "    \n",
    "    Args:\n",
    "        p: Serie de probabilidades de crisis\n",
    "        enter: Umbral para entrar en estado de crisis\n",
    "        exit: Umbral para salir de estado de crisis (debe ser < enter)\n",
    "        \n",
    "    Returns:\n",
    "        Serie booleana indicando períodos de crisis\n",
    "    \"\"\"\n",
    "    p = p.astype(float)\n",
    "    out = np.zeros(len(p), dtype=bool)\n",
    "    in_crisis = False\n",
    "    for i, val in enumerate(p.to_numpy()):\n",
    "        if not np.isfinite(val):\n",
    "            out[i] = in_crisis\n",
    "            continue\n",
    "        # Lógica de histéresis: requiere umbral más alto para entrar, más bajo para salir\n",
    "        if (not in_crisis) and (val >= enter):\n",
    "            in_crisis = True\n",
    "        elif in_crisis and (val <= exit):\n",
    "            in_crisis = False\n",
    "        out[i] = in_crisis\n",
    "    return pd.Series(out, index=p.index, name=\"is_crisis_hysteresis\")\n",
    "\n",
    "def _build_crisis_flags(\n",
    "    p: pd.Series,\n",
    "    *,\n",
    "    smooth_span: int,\n",
    "    enter: float,\n",
    "    exit: float,\n",
    "    min_days: int,\n",
    "    max_gap: int,\n",
    ") -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Construye banderas de crisis aplicando suavizado, histéresis y limpieza.\n",
    "    \n",
    "    Pipeline completo para generar señales de crisis robustas:\n",
    "    1. Suavizado exponencial ponderado (EWMA)\n",
    "    2. Histéresis para evitar cambios frecuentes\n",
    "    3. Limpieza de ruido\n",
    "    \n",
    "    Args:\n",
    "        p: Serie de probabilidades crudas del HMM\n",
    "        smooth_span: Período de suavizado EWMA\n",
    "        enter: Umbral de entrada a crisis\n",
    "        exit: Umbral de salida de crisis\n",
    "        min_days: Duración mínima de crisis\n",
    "        max_gap: Gap máximo permitido dentro de crisis\n",
    "        \n",
    "    Returns:\n",
    "        Tupla: (probabilidades_suavizadas, banderas_crudas, banderas_limpias)\n",
    "    \"\"\"\n",
    "    # Suavizar probabilidades con media móvil exponencial ponderada\n",
    "    p_smooth = p.ewm(span=smooth_span, adjust=False).mean()\n",
    "    # Aplicar histéresis para obtener banderas iniciales\n",
    "    raw = hysteresis_flags(p_smooth, enter=enter, exit=exit)\n",
    "    # Limpiar ruido y estabilizar señales\n",
    "    clean = clean_regime_flags(raw, min_true=min_days, max_false_gap=max_gap)\n",
    "    return p_smooth, raw, clean\n",
    "\n",
    "# Búsqueda automática de configuración que capture la crisis de 2008\n",
    "selected_cfg = None\n",
    "selected_pack = None\n",
    "for cfg in PARAM_CANDIDATES:\n",
    "    p_smooth_i, raw_i, clean_i = _build_crisis_flags(\n",
    "        regime[\"p_crisis\"],\n",
    "        smooth_span=cfg[\"smooth_span\"],\n",
    "        enter=cfg[\"enter\"],\n",
    "        exit=cfg[\"exit\"],\n",
    "        min_days=cfg[\"min_days\"],\n",
    "        max_gap=cfg[\"max_gap\"],\n",
    "    )\n",
    "    # Verificar si la configuración captura el período de crisis 2008\n",
    "    window_i = clean_i.loc[(clean_i.index >= TARGET_2008_START) & (clean_i.index <= TARGET_2008_END)]\n",
    "    captures_2008 = bool(window_i.any()) if len(window_i) else False\n",
    "    if captures_2008:\n",
    "        selected_cfg = cfg\n",
    "        selected_pack = (p_smooth_i, raw_i, clean_i)\n",
    "        break\n",
    "\n",
    "# Plan B: usar la configuración más sensible si ninguna detecta 2008\n",
    "if selected_cfg is None:\n",
    "    selected_cfg = PARAM_CANDIDATES[-1]\n",
    "    selected_pack = _build_crisis_flags(\n",
    "        regime[\"p_crisis\"],\n",
    "        smooth_span=selected_cfg[\"smooth_span\"],\n",
    "        enter=selected_cfg[\"enter\"],\n",
    "        exit=selected_cfg[\"exit\"],\n",
    "        min_days=selected_cfg[\"min_days\"],\n",
    "        max_gap=selected_cfg[\"max_gap\"],\n",
    "    )\n",
    "\n",
    "# Extraer resultados de la configuración seleccionada\n",
    "p_crisis_smooth, is_crisis_raw, is_crisis = selected_pack\n",
    "\n",
    "# Exponer parámetros finales (se usan también en el gráfico)\n",
    "SMOOTH_SPAN = selected_cfg[\"smooth_span\"]\n",
    "ENTER_CRISIS = selected_cfg[\"enter\"]\n",
    "EXIT_CRISIS = selected_cfg[\"exit\"]\n",
    "MIN_CRISIS_DAYS = selected_cfg[\"min_days\"]\n",
    "MAX_FALSE_GAP = selected_cfg[\"max_gap\"]\n",
    "\n",
    "# Validación y diagnóstico de la configuración seleccionada\n",
    "window = is_crisis.loc[(is_crisis.index >= TARGET_2008_START) & (is_crisis.index <= TARGET_2008_END)]\n",
    "print(\n",
    "    f\"Configuración de sombreado de crisis: {selected_cfg['name']} \"\n",
    "    f\"(span={SMOOTH_SPAN}, enter={ENTER_CRISIS:.2f}, exit={EXIT_CRISIS:.2f}, \"\n",
    "    f\"min_days={MIN_CRISIS_DAYS}, max_gap={MAX_FALSE_GAP})\"\n",
    ")\n",
    "if len(window):\n",
    "    print(f\"Días marcados como crisis en ventana 2008: {int(window.sum())} / {len(window)}\")\n",
    "\n",
    "# ---- Diseño de figura: probabilidad (arriba) + banda delgada de régimen (medio) + cesta de acciones (abajo) ----\n",
    "fig, (ax_p, ax_band, ax_spy) = plt.subplots(\n",
    "    3, 1, figsize=(14, 8), sharex=True, gridspec_kw={\"height_ratios\": [2.2, 0.35, 2.8]}\n",
    ")\n",
    "\n",
    "# Panel superior: probabilidad de crisis\n",
    "ax_p.plot(regime.index, p_crisis_smooth, color=\"#c0392b\", linewidth=1.2, label=\"P(crisis) (EWMA)\")\n",
    "ax_p.axhline(ENTER_CRISIS, color=\"#2c3e50\", linestyle=\"--\", linewidth=1, alpha=0.7, label=\"umbral_entrada\")\n",
    "ax_p.axhline(EXIT_CRISIS, color=\"#2c3e50\", linestyle=\":\", linewidth=1, alpha=0.7, label=\"umbral_salida\")\n",
    "ax_p.set_ylim(-0.02, 1.02)\n",
    "ax_p.set_ylabel(\"Probabilidad\")\n",
    "ax_p.set_title(\"Regímenes de mercado (HMM 2 estados) — vista macro\")\n",
    "ax_p.legend(loc=\"upper right\", frameon=True)\n",
    "\n",
    "# Panel medio: banda de régimen (limpia, legible)\n",
    "x0 = mdates.date2num(is_crisis.index[0])\n",
    "x1 = mdates.date2num(is_crisis.index[-1])\n",
    "band = is_crisis.astype(int).to_numpy()[None, :]  # forma (1, T)\n",
    "cmap = ListedColormap([\"#ecf0f1\", \"#f5b7b1\"])  # normal, crisis\n",
    "ax_band.imshow(band, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, extent=[x0, x1, 0, 1])\n",
    "ax_band.set_yticks([])\n",
    "ax_band.set_ylabel(\"Régimen\")\n",
    "ax_band.set_ylim(0, 1)\n",
    "ax_band.grid(False)\n",
    "# Ocultar bordes para apariencia limpia\n",
    "for spine in ax_band.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Panel inferior: retorno logarítmico acumulado de la cesta de acciones\n",
    "basket_lr = returns_hmm[EQUITY_TICKERS_HMM].mean(axis=1).cumsum()\n",
    "ax_spy.plot(\n",
    "    basket_lr.index,\n",
    "    basket_lr.values,\n",
    "    color=\"#1f4e79\",\n",
    "    linewidth=1.2,\n",
    "    label=\"Cesta de acciones (ex GME) retorno logarítmico acumulado\",\n",
    ")\n",
    "ax_spy.set_ylabel(\"Retorno log-acumulado\")\n",
    "ax_spy.legend(loc=\"upper left\", frameon=True)\n",
    "\n",
    "# Sombreado ligero también en el panel inferior (refuerza interpretación)\n",
    "for a, b in _runs_to_segments(is_crisis):\n",
    "    ax_spy.axvspan(a, b, color=\"#f1948a\", alpha=0.15, linewidth=0)\n",
    "\n",
    "# Formato del eje X (único eje en la parte inferior)\n",
    "locator = mdates.AutoDateLocator(minticks=6, maxticks=12)\n",
    "ax_spy.xaxis.set_major_locator(locator)\n",
    "ax_spy.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator))\n",
    "# Ocultar etiquetas de fecha en paneles superiores\n",
    "ax_p.tick_params(labelbottom=False)\n",
    "ax_band.tick_params(labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Segmentos sombreados (crisis):\", len(_runs_to_segments(is_crisis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f3aca",
   "metadata": {},
   "source": [
    "### Fase 1 - S&P 500 por régimen detectado\n",
    "\n",
    "Visual de control del modelo de estados:\n",
    "\n",
    "- Blanco = Calma\n",
    "- Azul = Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5b396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:03.488614Z",
     "iopub.status.busy": "2026-02-11T18:19:03.487617Z",
     "iopub.status.idle": "2026-02-11T18:19:05.506443Z",
     "shell.execute_reply": "2026-02-11T18:19:05.506443Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 1: S&P 500 coloreado por regimen (Blanco=Calma, Azul=Crisis) ---\n",
    "\n",
    "spx = yf.download(\"^GSPC\", start=START, end=END, progress=False, auto_adjust=False)\n",
    "if isinstance(spx.columns, pd.MultiIndex):\n",
    "    if (\"Adj Close\", \"^GSPC\") in spx.columns:\n",
    "        spx_close = spx[(\"Adj Close\", \"^GSPC\")]\n",
    "    else:\n",
    "        spx_close = spx[(\"Close\", \"^GSPC\")]\n",
    "else:\n",
    "    spx_close = spx[\"Adj Close\"] if \"Adj Close\" in spx.columns else spx[\"Close\"]\n",
    "\n",
    "spx_close = spx_close.reindex(regime.index).ffill().dropna()\n",
    "reg_spx = is_crisis.reindex(spx_close.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "spx_calm = spx_close.where(~reg_spx)\n",
    "spx_crisis = spx_close.where(reg_spx)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.set_facecolor(\"#111111\")\n",
    "fig.patch.set_facecolor(\"#111111\")\n",
    "\n",
    "# Linea base tenue + tramos coloreados por regimen\n",
    "ax.plot(spx_close.index, spx_close.values, color=\"#666666\", linewidth=0.8, alpha=0.6)\n",
    "ax.plot(spx_calm.index, spx_calm.values, color=\"white\", linewidth=1.4, label=\"Calma (blanco)\")\n",
    "ax.plot(spx_crisis.index, spx_crisis.values, color=\"#1f77b4\", linewidth=1.6, label=\"Crisis (azul)\")\n",
    "\n",
    "ax.set_title(\"S&P 500 por regimen detectado (HMM)\", color=\"white\")\n",
    "ax.set_ylabel(\"Nivel indice\", color=\"white\")\n",
    "ax.grid(alpha=0.18)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#AAAAAA\")\n",
    "\n",
    "leg = ax.legend(loc=\"upper left\", frameon=True)\n",
    "leg.get_frame().set_facecolor(\"#222222\")\n",
    "leg.get_frame().set_edgecolor(\"#666666\")\n",
    "for txt in leg.get_texts():\n",
    "    txt.set_color(\"white\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912af08e",
   "metadata": {},
   "source": [
    "## Cómo leer la tabla de drivers por bloque de crisis\n",
    "\n",
    "Los drivers son señales estadísticas (no causalidad estricta) que indican qué variables se desviaron más frente al estado normal.\n",
    "\n",
    "Para cada bloque continuo `is_crisis=True`, se calcula z-score de cada feature respecto al período normal:\n",
    "\n",
    "- `z > 0`: feature por encima de lo normal\n",
    "- `z < 0`: feature por debajo de lo normal\n",
    "- `|z|` alto: variable candidata a explicar ese episodio\n",
    "\n",
    "Ejemplos económicos:\n",
    "\n",
    "- `dd_equity` muy negativo -> drawdown profundo de renta variable\n",
    "- `vol_equity_21` alto -> fuerte inestabilidad en equity\n",
    "- `vol_hy_21` alto -> tensión de crédito\n",
    "- `r_credit` muy negativo -> entorno risk-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc18f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:05.509210Z",
     "iopub.status.busy": "2026-02-11T18:19:05.509210Z",
     "iopub.status.idle": "2026-02-11T18:19:05.532544Z",
     "shell.execute_reply": "2026-02-11T18:19:05.532032Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Diagnósticos: por qué se marca cada 'bloque de crisis' (atribución de características) ---\n",
    "\n",
    "# Hacer que las columnas de texto largo sean legibles en las visualizaciones del notebook\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "def segments_from_flags(flags: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Convierte banderas de crisis en segmentos de tiempo continuos.\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana donde True indica crisis\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con segmentos de crisis (inicio, fin, duración)\n",
    "    \"\"\"\n",
    "    segs = _runs_to_segments(flags)\n",
    "    if not segs:\n",
    "        return pd.DataFrame(columns=[\"start\", \"end\", \"n_days\"])\n",
    "    out = []\n",
    "    for a, b in segs:\n",
    "        idx_seg = flags.loc[a:b].index\n",
    "        out.append({\"start\": a, \"end\": b, \"n_days\": int(len(idx_seg))})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Obtener segmentos de crisis a partir de las banderas\n",
    "seg_tbl = segments_from_flags(is_crisis)\n",
    "if seg_tbl.empty:\n",
    "    print(\"No se encontraron segmentos de crisis.\")\n",
    "else:\n",
    "    # Columnas de características a reportar (solo las principales para mantener interpretabilidad)\n",
    "    cols_preferred = [\n",
    "        \"vol_equity_21\",  # Volatilidad de acciones a 21 días\n",
    "        \"dd_equity\",      # Drawdown de acciones\n",
    "        \"r_credit\",       # Retorno de crédito\n",
    "        \"vol_hy_21\",      # Volatilidad de high yield a 21 días\n",
    "        \"r_equity\",       # Retorno de acciones\n",
    "        \"r_hy\",           # Retorno de high yield\n",
    "        \"r_10y\",          # Retorno de bonos a 10 años\n",
    "        \"r_2y\",           # Retorno de bonos a 2 años\n",
    "        \"r_gld\",          # Retorno de oro\n",
    "    ]\n",
    "    feat_cols = [c for c in cols_preferred if c in features.columns]\n",
    "    feat = features[feat_cols].copy()\n",
    "\n",
    "    # Línea base normal = todas las fechas que no están en crisis (para z-scores)\n",
    "    normal_mask = ~is_crisis.reindex(feat.index, fill_value=False)\n",
    "    mu0 = feat.loc[normal_mask].mean()  # Media en períodos normales\n",
    "    sd0 = feat.loc[normal_mask].std(ddof=1).replace(0, np.nan)  # Desviación estándar en períodos normales\n",
    "\n",
    "    rows = []\n",
    "    for _, row in seg_tbl.iterrows():\n",
    "        a, b = row[\"start\"], row[\"end\"]\n",
    "        seg = feat.loc[a:b]\n",
    "        m = seg.mean()  # Media del segmento de crisis\n",
    "        z = (m - mu0) / sd0  # Z-score respecto a períodos normales\n",
    "        z = z.replace([np.inf, -np.inf], np.nan)\n",
    "        top = z.abs().sort_values(ascending=False).head(4)  # Top 4 características más atípicas\n",
    "\n",
    "        drivers = []\n",
    "        for k in top.index:\n",
    "            if pd.notna(z[k]):\n",
    "                drivers.append((k, float(z[k])))\n",
    "        top_txt = \"; \".join([f\"{k}: z={val:+.2f}\" for k, val in drivers])\n",
    "\n",
    "        base = {\n",
    "            \"start\": a,\n",
    "            \"end\": b,\n",
    "            \"n_days\": int(row[\"n_days\"]),\n",
    "            \"p_crisis_mean\": float(p_crisis_smooth.loc[a:b].mean()),\n",
    "            \"p_crisis_max\": float(p_crisis_smooth.loc[a:b].max()),\n",
    "            \"top_drivers_vs_normal\": top_txt,\n",
    "        }\n",
    "        # Agregar los 4 principales conductores como columnas separadas\n",
    "        for i in range(4):\n",
    "            if i < len(drivers):\n",
    "                base[f\"driver_{i+1}\"] = drivers[i][0]\n",
    "                base[f\"z_{i+1}\"] = drivers[i][1]\n",
    "            else:\n",
    "                base[f\"driver_{i+1}\"] = \"\"\n",
    "                base[f\"z_{i+1}\"] = np.nan\n",
    "        rows.append(base)\n",
    "\n",
    "    diag = pd.DataFrame(rows)\n",
    "    diag[\"start\"] = diag[\"start\"].dt.date\n",
    "    diag[\"end\"] = diag[\"end\"].dt.date\n",
    "    diag = diag.sort_values(\"start\")\n",
    "\n",
    "    # NOTA: pandas .style requiere jinja2; evitamos esa dependencia renderizando HTML directamente.\n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "    css = \"\"\"\n",
    "<style>\n",
    "table.dataframe td { vertical-align: top; }\n",
    "table.dataframe td, table.dataframe th { white-space: pre-wrap; }\n",
    "</style>\n",
    "\"\"\"\n",
    "    display(HTML(css + diag.to_html(index=False)))\n",
    "\n",
    "    print(\"\\nCómo leer 'top_drivers_vs_normal':\")\n",
    "    print(\"- z > 0 significa más alto que lo normal; z < 0 significa más bajo que lo normal.\")\n",
    "    print(\"- |z| grande indica que esa característica es inusual en ese segmento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc71e3",
   "metadata": {},
   "source": [
    "## Fase 2 - Riesgo marginal por estado\n",
    "\n",
    "Objetivo: medir cómo cambian las distribuciones individuales en Normal vs Estrés.\n",
    "\n",
    "Se reporta por activo y estado:\n",
    "\n",
    "- media\n",
    "- volatilidad\n",
    "- skew\n",
    "- kurtosis\n",
    "- VaR y ES (1% y 5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddde1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:05.535676Z",
     "iopub.status.busy": "2026-02-11T18:19:05.534667Z",
     "iopub.status.idle": "2026-02-11T18:19:05.968740Z",
     "shell.execute_reply": "2026-02-11T18:19:05.967728Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 2: riesgo marginal por estado (Normal vs Estres) ---\n",
    "\n",
    "# Seguridad: alinear todo en el mismo índice\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "def var_es(x: pd.Series, alpha: float) -> tuple[float, float]:\n",
    "    \"\"\"Calcula Value at Risk (VaR) y Expected Shortfall (ES).\n",
    "    \n",
    "    Args:\n",
    "        x: Serie de retornos\n",
    "        alpha: Nivel de confianza (ej. 0.05 para 5%)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (VaR, ES)\n",
    "    \"\"\"\n",
    "    x = x.dropna()\n",
    "    if len(x) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    q = x.quantile(alpha)\n",
    "    tail = x[x <= q]\n",
    "    es = tail.mean() if len(tail) else np.nan\n",
    "    return (float(q), float(es))\n",
    "\n",
    "ALPHAS = [0.01, 0.05]  # Niveles de confianza: 1% y 5%\n",
    "states = {\"Normal\": ~flags, \"Estres\": flags}  # Estados: Normal vs Estres\n",
    "\n",
    "rows = []\n",
    "for state_name, mask in states.items():\n",
    "    sub = ret.loc[mask]  # Subset de retornos para este estado\n",
    "    for ticker in ret.columns:\n",
    "        x = sub[ticker].dropna()  # Retornos del ticker sin valores faltantes\n",
    "        r = {\n",
    "            \"state\": state_name,\n",
    "            \"ticker\": ticker,\n",
    "            \"n_days\": int(x.shape[0]),\n",
    "            \"mean\": float(x.mean()) if len(x) else np.nan,      # Media de retornos\n",
    "            \"vol\": float(x.std(ddof=1)) if len(x) > 1 else np.nan,  # Volatilidad\n",
    "            \"skew\": float(x.skew()) if len(x) > 2 else np.nan,    # Asimetría\n",
    "            \"kurt\": float(x.kurt()) if len(x) > 3 else np.nan,    # Curtosis\n",
    "        }\n",
    "        # Calcular VaR y ES para cada nivel de confianza\n",
    "        for a in ALPHAS:\n",
    "            v, e = var_es(x, a)\n",
    "            r[f\"VaR_{int(a*100)}\"] = v  # Value at Risk\n",
    "            r[f\"ES_{int(a*100)}\"] = e   # Expected Shortfall\n",
    "        rows.append(r)\n",
    "\n",
    "risk_by_state = pd.DataFrame(rows).sort_values([\"ticker\", \"state\"]).reset_index(drop=True)\n",
    "\n",
    "# Guardar resultados (útil para reporte/exportación)\n",
    "out_csv = OUT_DIR / \"phase2_risk_by_state.csv\"\n",
    "risk_by_state.to_csv(out_csv, index=False)\n",
    "print(\"Guardado:\", out_csv)\n",
    "\n",
    "# Mostrar vista compacta (tabla completa en CSV)\n",
    "cols_show = [\"ticker\", \"state\", \"n_days\", \"mean\", \"vol\", \"VaR_5\", \"ES_5\", \"VaR_1\", \"ES_1\", \"skew\", \"kurt\"]\n",
    "display(risk_by_state[cols_show].head(16))\n",
    "\n",
    "# Visualización rápida: comparación ES(5%) por activo (Normal vs Estres)\n",
    "pivot_es5 = risk_by_state.pivot(index=\"ticker\", columns=\"state\", values=\"ES_5\").reindex(ret.columns)\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "pivot_es5.plot(kind=\"bar\", ax=ax)\n",
    "ax.axhline(0, color=\"black\", linewidth=0.8)\n",
    "ax.set_title(\"Expected Shortfall (5%) por estado\")\n",
    "ax.set_ylabel(\"ES 5% (retorno logarítmico diario)\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificación opcional: cuántos días por estado\n",
    "print(\"Días en Normal:\", int((~flags).sum()))\n",
    "print(\"Días en Estres:\", int(flags.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedeac35",
   "metadata": {},
   "source": [
    "### Lectura ejecutiva de Fase 2\n",
    "\n",
    "Se resume de forma automática:\n",
    "\n",
    "1. Cuánto aumenta la volatilidad de HYG al pasar de Normal a Estrés.\n",
    "2. Si GLD mantiene comportamiento defensivo en períodos de estrés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5c67e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:05.971829Z",
     "iopub.status.busy": "2026-02-11T18:19:05.971829Z",
     "iopub.status.idle": "2026-02-11T18:19:05.990003Z",
     "shell.execute_reply": "2026-02-11T18:19:05.989492Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 2: respuestas automaticas (HYG y GLD) ---\n",
    "phase2_q = (\n",
    "    risk_by_state[risk_by_state[\"ticker\"].isin([\"HYG\", \"GLD\"])]\n",
    "    .set_index([\"ticker\", \"state\"])\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "display(\n",
    "    phase2_q[[\"n_days\", \"mean\", \"vol\", \"VaR_1\", \"ES_1\", \"VaR_5\", \"ES_5\", \"skew\", \"kurt\"]]\n",
    ")\n",
    "\n",
    "hyg_n = phase2_q.loc[(\"HYG\", \"Normal\")]\n",
    "hyg_s = phase2_q.loc[(\"HYG\", \"Estres\")]\n",
    "hyg_vol_inc_pct = (hyg_s[\"vol\"] / hyg_n[\"vol\"] - 1.0) * 100.0\n",
    "\n",
    "print(\n",
    "    f\"HYG: vol Normal={hyg_n['vol']:.6f} vs Estres={hyg_s['vol']:.6f} \"\n",
    "    f\"-> aumento = {hyg_vol_inc_pct:.2f}% (x{hyg_s['vol']/hyg_n['vol']:.2f}).\"\n",
    ")\n",
    "\n",
    "gld_n = phase2_q.loc[(\"GLD\", \"Normal\")]\n",
    "gld_s = phase2_q.loc[(\"GLD\", \"Estres\")]\n",
    "\n",
    "gld_checks = {\n",
    "    \"mean_positive_in_stress\": bool(gld_s[\"mean\"] > 0),\n",
    "    \"vol_not_higher_in_stress\": bool(gld_s[\"vol\"] <= gld_n[\"vol\"]),\n",
    "    \"ES5_not_worse_in_stress\": bool(abs(gld_s[\"ES_5\"]) <= abs(gld_n[\"ES_5\"])),\n",
    "    \"VaR1_not_worse_in_stress\": bool(abs(gld_s[\"VaR_1\"]) <= abs(gld_n[\"VaR_1\"])),\n",
    "}\n",
    "score = sum(gld_checks.values())\n",
    "\n",
    "if score >= 3:\n",
    "    gld_conclusion = \"Sí: GLD actúa como refugio (al menos parcial) en este corte.\"\n",
    "elif score == 2:\n",
    "    gld_conclusion = \"Mixto: GLD no empeora claramente, pero la señal de refugio no es concluyente.\"\n",
    "else:\n",
    "    gld_conclusion = \"No: GLD no se comporta como refugio en este corte.\"\n",
    "\n",
    "print(\"GLD checks:\", gld_checks)\n",
    "print(gld_conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5ddab",
   "metadata": {},
   "source": [
    "### Distribuciones Normal vs Estrés\n",
    "\n",
    "Visual comparativo de histogramas por activo para revisar cambios en forma, dispersión y cola izquierda.\n",
    "\n",
    "Para legibilidad, el eje X se acota a percentiles centrales, manteniendo referencias de VaR(5%) y VaR(1%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28432878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:05.992018Z",
     "iopub.status.busy": "2026-02-11T18:19:05.992018Z",
     "iopub.status.idle": "2026-02-11T18:19:07.477220Z",
     "shell.execute_reply": "2026-02-11T18:19:07.477220Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 2 (extra figure): return distributions by state ---\n",
    "\n",
    "# Reuse objects from Phase 2\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Pick a few representative assets (risk + defensive)\n",
    "if \"EQUITY_TICKERS\" in globals() and EQUITY_TICKERS:\n",
    "    equity_anchor = EQUITY_TICKERS[0]\n",
    "else:\n",
    "    equity_anchor = ret.columns[0] if len(ret.columns) else None\n",
    "\n",
    "hy_ticker = HY_TICKER if \"HY_TICKER\" in globals() else None\n",
    "bond10_ticker = BOND_10Y_TICKER if \"BOND_10Y_TICKER\" in globals() else None\n",
    "gold_ticker = GOLD_TICKER if \"GOLD_TICKER\" in globals() else None\n",
    "\n",
    "focus_candidates = [equity_anchor, hy_ticker, bond10_ticker, gold_ticker]\n",
    "tickers_focus = [t for t in focus_candidates if t is not None and t in ret.columns]\n",
    "if not tickers_focus:\n",
    "    tickers_focus = list(ret.columns[:4])\n",
    "\n",
    "def _safe_quantile(x, q, default=np.nan):\n",
    "    x = pd.Series(x).dropna()\n",
    "    return float(x.quantile(q)) if len(x) else default\n",
    "\n",
    "n = len(tickers_focus)\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 3.6 * nrows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "for ax, t in zip(axes, tickers_focus):\n",
    "    xN = ret.loc[~flags, t].dropna()\n",
    "    xS = ret.loc[flags, t].dropna()\n",
    "    x_all = pd.concat([xN, xS], axis=0)\n",
    "\n",
    "    # Central window for readability (still marking VaR lines)\n",
    "    q_lo = _safe_quantile(x_all, 0.005)\n",
    "    q_hi = _safe_quantile(x_all, 0.995)\n",
    "    if np.isfinite(q_lo) and np.isfinite(q_hi) and q_hi > q_lo:\n",
    "        bins = np.linspace(q_lo, q_hi, 70)\n",
    "        ax.set_xlim(q_lo, q_hi)\n",
    "    else:\n",
    "        bins = 70\n",
    "\n",
    "    ax.hist(xN, bins=bins, density=True, alpha=0.55, color=\"#4C78A8\", label=\"Normal\")\n",
    "    ax.hist(xS, bins=bins, density=True, alpha=0.45, color=\"#E45756\", label=\"Estres\")\n",
    "\n",
    "    # Mark VaR lines (5% dashed, 1% dotted)\n",
    "    v5N, _ = var_es(xN, 0.05)\n",
    "    v1N, _ = var_es(xN, 0.01)\n",
    "    v5S, _ = var_es(xS, 0.05)\n",
    "    v1S, _ = var_es(xS, 0.01)\n",
    "\n",
    "    if np.isfinite(v5N):\n",
    "        ax.axvline(v5N, color=\"#4C78A8\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1N):\n",
    "        ax.axvline(v1N, color=\"#4C78A8\", linestyle=\":\", linewidth=1)\n",
    "    if np.isfinite(v5S):\n",
    "        ax.axvline(v5S, color=\"#E45756\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1S):\n",
    "        ax.axvline(v1S, color=\"#E45756\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "    ax.set_title(f\"{t} — Normal vs Estres (hist densidad)\\nVaR5% (--) y VaR1% (:) por estado\")\n",
    "    ax.set_xlabel(\"Retorno log diario\")\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "\n",
    "# Hide unused axes\n",
    "for ax in axes[len(tickers_focus):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d09ed8",
   "metadata": {},
   "source": [
    "## Fase 3 - Dependencia entre activos\n",
    "\n",
    "Objetivo: analizar cómo se deteriora la diversificacion en estrés.\n",
    "\n",
    "Secuencia de trabajo:\n",
    "\n",
    "1. Correlaciones por estado (dependencia lineal).\n",
    "2. Dependencia en cola empírica (co-movimientos extremos).\n",
    "3. Ajuste de cópulas (gaussiana y t) por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94df08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:07.477220Z",
     "iopub.status.busy": "2026-02-11T18:19:07.477220Z",
     "iopub.status.idle": "2026-02-11T18:19:08.360486Z",
     "shell.execute_reply": "2026-02-11T18:19:08.359478Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 3a: correlation by state (Normal vs Stress) ---\n",
    "\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "retN = ret.loc[~flags].dropna(how=\"all\")\n",
    "retS = ret.loc[flags].dropna(how=\"all\")\n",
    "\n",
    "corrN = retN.corr()\n",
    "corrS = retS.corr()\n",
    "\n",
    "# Difference heatmap (Stress - Normal)\n",
    "corrD = corrS - corrN\n",
    "\n",
    "def _heatmap(ax, M, title, vmin=-1, vmax=1, cmap=\"coolwarm\"):\n",
    "    im = ax.imshow(M.values, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    ax.set_xticks(range(M.shape[1]))\n",
    "    ax.set_yticks(range(M.shape[0]))\n",
    "    ax.set_xticklabels(M.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(M.index)\n",
    "    ax.set_title(title)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    return im\n",
    "\n",
    "# Fixed delta scale (case-specific): vivid and comparable within this report\n",
    "max_abs_delta = float(np.nanmax(np.abs(corrD.values)))\n",
    "delta_lim = 0.15  # choose a \"reasonable\" fixed scale for this dataset/report\n",
    "if max_abs_delta > delta_lim:\n",
    "    print(f\"[Aviso] max |Δ| = {max_abs_delta:.3f} supera ±{delta_lim:.2f}; el panel Δ saturará colores.\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4), constrained_layout=True)\n",
    "im0 = _heatmap(axes[0], corrN, f\"Corr (Normal), n={len(retN)}\")\n",
    "im1 = _heatmap(axes[1], corrS, f\"Corr (Estres), n={len(retS)}\")\n",
    "im2 = _heatmap(\n",
    "    axes[2],\n",
    "    corrD,\n",
    "    f\"Δ Corr (Estres − Normal)\\n(max |Δ| = {max_abs_delta:.3f}, escala ±{delta_lim:.2f})\",\n",
    "    vmin=-delta_lim,\n",
    "    vmax=delta_lim,\n",
    ")\n",
    "\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "plt.show()\n",
    "\n",
    "# Show the largest absolute changes (upper triangle)\n",
    "pairs = []\n",
    "cols = list(corrD.columns)\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i + 1, len(cols)):\n",
    "        a, b = cols[i], cols[j]\n",
    "        v = float(corrD.loc[a, b])\n",
    "        pairs.append((abs(v), v, a, b))\n",
    "\n",
    "top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "display(pd.DataFrame(top, columns=[\"abs_delta\", \"delta\", \"asset_a\", \"asset_b\"]))\n",
    "\n",
    "display(corrD.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dd1ab",
   "metadata": {},
   "source": [
    "### 3b) Dependencia en cola (empírica)\n",
    "\n",
    "La correlación central no captura bien episodios extremos.\n",
    "\n",
    "Se estima para cuantiles bajos `q`:\n",
    "\n",
    "`lambda_L(q) = P(R_i <= Q_i(q), R_j <= Q_j(q)) / q`\n",
    "\n",
    "Interpretación: cuanto mas alto `lambda_L`, mayor probabilidad de caidas conjuntas en la cola izquierda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c938a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:08.362496Z",
     "iopub.status.busy": "2026-02-11T18:19:08.362496Z",
     "iopub.status.idle": "2026-02-11T18:19:11.198472Z",
     "shell.execute_reply": "2026-02-11T18:19:11.197965Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 3b: empirical lower-tail dependence by state ---\n",
    "\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "states = {\"Normal\": ~flags, \"Estres\": flags}\n",
    "qs = [0.05, 0.01]\n",
    "\n",
    "assets = list(ret.columns)\n",
    "\n",
    "def empirical_lambda_L(sub: pd.DataFrame, q: float) -> pd.DataFrame:\n",
    "    sub = sub.dropna(how=\"all\")\n",
    "    out = pd.DataFrame(index=assets, columns=assets, dtype=float)\n",
    "\n",
    "    # Diagonal: perfect self-dependence\n",
    "    for a in assets:\n",
    "        out.loc[a, a] = 1.0\n",
    "\n",
    "    # Off-diagonals\n",
    "    for i, a in enumerate(assets):\n",
    "        for j in range(i + 1, len(assets)):\n",
    "            b = assets[j]\n",
    "            x = sub[[a, b]].dropna()\n",
    "            if len(x) < 50:\n",
    "                out.loc[a, b] = np.nan\n",
    "                out.loc[b, a] = np.nan\n",
    "                continue\n",
    "\n",
    "            qa = float(x[a].quantile(q))\n",
    "            qb = float(x[b].quantile(q))\n",
    "            ind = (x[a].to_numpy() <= qa) & (x[b].to_numpy() <= qb)\n",
    "            p = float(ind.mean())\n",
    "            val = p / q if q > 0 else np.nan\n",
    "            out.loc[a, b] = val\n",
    "            out.loc[b, a] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "lam = {}  # lam[(state, q)] -> matrix\n",
    "for state_name, mask in states.items():\n",
    "    sub = ret.loc[mask]\n",
    "    for q in qs:\n",
    "        lam[(state_name, q)] = empirical_lambda_L(sub, q)\n",
    "\n",
    "# Plot heatmaps for q=5% and q=1% (Normal, Stress, Delta)\n",
    "for q in qs:\n",
    "    L_N = lam[(\"Normal\", q)]\n",
    "    L_S = lam[(\"Estres\", q)]\n",
    "    L_D = L_S - L_N\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4), constrained_layout=True)\n",
    "    vmin, vmax = 0.0, 1.0\n",
    "\n",
    "    im0 = axes[0].imshow(L_N.values, vmin=vmin, vmax=vmax, cmap=\"viridis\")\n",
    "    axes[0].set_title(f\"λ_L empírica (q={int(q*100)}%) — Normal\")\n",
    "\n",
    "    im1 = axes[1].imshow(L_S.values, vmin=vmin, vmax=vmax, cmap=\"viridis\")\n",
    "    axes[1].set_title(f\"λ_L empírica (q={int(q*100)}%) — Estres\")\n",
    "\n",
    "    im2 = axes[2].imshow(L_D.values, vmin=-0.5, vmax=0.5, cmap=\"coolwarm\")\n",
    "    axes[2].set_title(f\"Δ λ_L (Estres − Normal), q={int(q*100)}%\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(len(assets)))\n",
    "        ax.set_yticks(range(len(assets)))\n",
    "        ax.set_xticklabels(assets, rotation=45, ha=\"right\")\n",
    "        ax.set_yticklabels(assets)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    fig.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "    fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "    plt.show()\n",
    "\n",
    "# Export matrices (useful for report)\n",
    "for (state_name, q), M in lam.items():\n",
    "    out = OUT_DIR / f\"phase3_taildep_empirical_{state_name.lower()}_q{int(q*100)}.csv\"\n",
    "    M.to_csv(out)\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# Quick ranking: which pairs increase most in tail dependence at q=5%\n",
    "q = 0.05\n",
    "L_D = lam[(\"Estres\", q)] - lam[(\"Normal\", q)]\n",
    "pairs = []\n",
    "for i, a in enumerate(assets):\n",
    "    for j in range(i + 1, len(assets)):\n",
    "        b = assets[j]\n",
    "        v = L_D.loc[a, b]\n",
    "        if np.isfinite(v):\n",
    "            pairs.append((float(v), a, b))\n",
    "\n",
    "top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "display(pd.DataFrame(top, columns=[\"delta_lambda\", \"asset_a\", \"asset_b\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344d217",
   "metadata": {},
   "source": [
    "### 3c) Cópulas por estado (gaussiana vs t)\n",
    "\n",
    "Se separan marginales y dependencia para modelar estructura conjunta de retornos:\n",
    "\n",
    "- Cópula gaussiana: buena para dependencia media, débil en colas.\n",
    "- Cópula t: más realista para extremos conjuntos.\n",
    "\n",
    "Se exportan matrices de correlacion y (para cópula t) dependencia de cola implícita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa82ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:11.203482Z",
     "iopub.status.busy": "2026-02-11T18:19:11.202484Z",
     "iopub.status.idle": "2026-02-11T18:19:16.759081Z",
     "shell.execute_reply": "2026-02-11T18:19:16.758074Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 3c: Gaussian vs t copula by state (pseudo-likelihood) ---\n",
    "import json\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "states = {\"Normal\": ~flags, \"Estres\": flags}\n",
    "assets = list(ret.columns)\n",
    "\n",
    "# Fixed delta scales (case-specific): improve contrast in Δ panels\n",
    "DELTA_CORR_LIM = 0.15   # for Gauss copula Δ corr\n",
    "DELTA_LAML_LIM = 0.15   # for t-copula Δ tail-dependence (asymptotic)\n",
    "\n",
    "def _pseudo_obs(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Pseudo-observations in (0,1) via ranks columnwise.\"\"\"\n",
    "    x = df[assets].dropna()\n",
    "    n = len(x)\n",
    "    if n < 10:\n",
    "        return np.empty((0, len(assets)))\n",
    "    r = x.rank(method=\"average\")\n",
    "    u = (r / (n + 1.0)).to_numpy()  # avoid 0/1\n",
    "    u = np.clip(u, 1e-6, 1 - 1e-6)\n",
    "    return u\n",
    "\n",
    "def _nearest_psd_corr(C: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Project to PSD correlation matrix via eigenvalue clipping.\"\"\"\n",
    "    C = np.asarray(C, dtype=float)\n",
    "    C = 0.5 * (C + C.T)\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    w, V = np.linalg.eigh(C)\n",
    "    w = np.maximum(w, eps)\n",
    "    C2 = (V * w) @ V.T\n",
    "    d = np.sqrt(np.diag(C2))\n",
    "    C2 = C2 / (d[:, None] * d[None, :])\n",
    "    np.fill_diagonal(C2, 1.0)\n",
    "    return 0.5 * (C2 + C2.T)\n",
    "\n",
    "def fit_gaussian_copula(U: np.ndarray) -> np.ndarray:\n",
    "    Z = norm.ppf(U)\n",
    "    C = np.corrcoef(Z, rowvar=False)\n",
    "    return _nearest_psd_corr(C)\n",
    "\n",
    "def try_multivariate_t_logpdf(Z: np.ndarray, C: np.ndarray, df: float) -> np.ndarray:\n",
    "    \"\"\"Return log f_T(Z; C, df). Uses scipy if available.\"\"\"\n",
    "    try:\n",
    "        from scipy.stats import multivariate_t  # type: ignore\n",
    "\n",
    "        mv = multivariate_t(loc=np.zeros(Z.shape[1]), shape=C, df=df)\n",
    "        return mv.logpdf(Z)\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"multivariate_t not available\") from exc\n",
    "\n",
    "def fit_t_copula(U: np.ndarray, df_grid=(3, 4, 5, 7, 10, 15, 20, 30, 50), df_fallback=5):\n",
    "    \"\"\"Fit t-copula by grid-search over df using pseudo log-likelihood, if scipy supports multivariate_t.\"\"\"\n",
    "    best = None\n",
    "    best_ll = -np.inf\n",
    "\n",
    "    can_mv_t = True\n",
    "    # quick capability probe\n",
    "    try:\n",
    "        Z_probe = t.ppf(U[: min(len(U), 20)], df_fallback)\n",
    "        C_probe = _nearest_psd_corr(np.corrcoef(Z_probe, rowvar=False))\n",
    "        _ = try_multivariate_t_logpdf(Z_probe, C_probe, df_fallback)\n",
    "    except Exception:\n",
    "        can_mv_t = False\n",
    "\n",
    "    if not can_mv_t:\n",
    "        df = df_fallback\n",
    "        Z = t.ppf(U, df)\n",
    "        C = _nearest_psd_corr(np.corrcoef(Z, rowvar=False))\n",
    "        return df, C, False\n",
    "\n",
    "    for df in df_grid:\n",
    "        Z = t.ppf(U, df)\n",
    "        C = _nearest_psd_corr(np.corrcoef(Z, rowvar=False))\n",
    "        log_joint = try_multivariate_t_logpdf(Z, C, df)\n",
    "        log_marg = t.logpdf(Z, df=df).sum(axis=1)\n",
    "        log_cop = log_joint - log_marg\n",
    "        ll = float(np.nanmean(log_cop))\n",
    "        if ll > best_ll:\n",
    "            best_ll = ll\n",
    "            best = (df, C)\n",
    "\n",
    "    assert best is not None\n",
    "    return best[0], best[1], True\n",
    "\n",
    "def t_copula_taildep_lambda_L(C: np.ndarray, df: float) -> np.ndarray:\n",
    "    \"\"\"Asymptotic lower tail dependence for t-copula (pairwise), matrix form.\"\"\"\n",
    "    d = C.shape[0]\n",
    "    out = np.zeros((d, d), dtype=float)\n",
    "    np.fill_diagonal(out, 1.0)\n",
    "    for i in range(d):\n",
    "        for j in range(i + 1, d):\n",
    "            rho = float(C[i, j])\n",
    "            # guard numeric\n",
    "            rho = max(min(rho, 0.999), -0.999)\n",
    "            a = np.sqrt((df + 1.0) * (1.0 - rho) / (1.0 + rho))\n",
    "            lam = 2.0 * t.cdf(-a, df=df + 1.0)\n",
    "            out[i, j] = lam\n",
    "            out[j, i] = lam\n",
    "    return out\n",
    "\n",
    "fits = {}\n",
    "for state_name, mask in states.items():\n",
    "    U = _pseudo_obs(ret.loc[mask])\n",
    "    if len(U) == 0:\n",
    "        continue\n",
    "\n",
    "    Cg = fit_gaussian_copula(U)\n",
    "    df_t, Ct, used_mvt = fit_t_copula(U)\n",
    "    L_t = t_copula_taildep_lambda_L(Ct, df_t)\n",
    "\n",
    "    fits[state_name] = {\n",
    "        \"n\": int(U.shape[0]),\n",
    "        \"gaussian_corr\": Cg,\n",
    "        \"t_df\": float(df_t),\n",
    "        \"t_corr\": Ct,\n",
    "        \"t_taildep_lambdaL\": L_t,\n",
    "        \"used_multivariate_t\": bool(used_mvt),\n",
    "    }\n",
    "\n",
    "    # Exports\n",
    "    pd.DataFrame(Cg, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_gaussian_corr_{state_name.lower()}.csv\")\n",
    "    pd.DataFrame(Ct, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_t_corr_{state_name.lower()}.csv\")\n",
    "    pd.DataFrame(L_t, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_t_taildep_lambdaL_{state_name.lower()}.csv\")\n",
    "\n",
    "# Save summary JSON\n",
    "summary = {\n",
    "    s: {\"n\": fits[s][\"n\"], \"t_df\": fits[s][\"t_df\"], \"used_multivariate_t\": fits[s][\"used_multivariate_t\"]}\n",
    "    for s in fits\n",
    "}\n",
    "out_json = OUT_DIR / \"phase3_copula_summary.json\"\n",
    "out_json.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved:\", out_json)\n",
    "print(summary)\n",
    "\n",
    "# Plots: Gaussian corr and t-taildep side by side (Normal vs Stress)\n",
    "if \"Normal\" in fits and \"Estres\" in fits:\n",
    "    CgN = pd.DataFrame(fits[\"Normal\"][\"gaussian_corr\"], index=assets, columns=assets)\n",
    "    CgS = pd.DataFrame(fits[\"Estres\"][\"gaussian_corr\"], index=assets, columns=assets)\n",
    "    LtN = pd.DataFrame(fits[\"Normal\"][\"t_taildep_lambdaL\"], index=assets, columns=assets)\n",
    "    LtS = pd.DataFrame(fits[\"Estres\"][\"t_taildep_lambdaL\"], index=assets, columns=assets)\n",
    "\n",
    "    dCg = (CgS - CgN)\n",
    "    dLt = (LtS - LtN)\n",
    "    max_abs_dCg = float(np.nanmax(np.abs(dCg.values)))\n",
    "    max_abs_dLt = float(np.nanmax(np.abs(dLt.values)))\n",
    "    if max_abs_dCg > DELTA_CORR_LIM:\n",
    "        print(f\"[Aviso] max |Δ corr (Gauss)| = {max_abs_dCg:.3f} supera ±{DELTA_CORR_LIM:.2f}; el panel Δ corr saturará colores.\")\n",
    "    if max_abs_dLt > DELTA_LAML_LIM:\n",
    "        print(f\"[Aviso] max |Δ λ_L (t)| = {max_abs_dLt:.3f} supera ±{DELTA_LAML_LIM:.2f}; el panel Δ λ_L saturará colores.\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 9), constrained_layout=True)\n",
    "\n",
    "    def _hm(ax, M, title, vmin, vmax, cmap):\n",
    "        im = ax.imshow(M.values, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.set_xticks(range(len(assets)))\n",
    "        ax.set_yticks(range(len(assets)))\n",
    "        ax.set_xticklabels(assets, rotation=45, ha=\"right\")\n",
    "        ax.set_yticklabels(assets)\n",
    "        ax.set_title(title)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        return im\n",
    "\n",
    "    im = _hm(axes[0, 0], CgN, \"Gauss copula corr — Normal\", -1, 1, \"coolwarm\")\n",
    "    fig.colorbar(im, ax=axes[0, 0], fraction=0.046)\n",
    "    im = _hm(axes[0, 1], CgS, \"Gauss copula corr — Estres\", -1, 1, \"coolwarm\")\n",
    "    fig.colorbar(im, ax=axes[0, 1], fraction=0.046)\n",
    "    im = _hm(\n",
    "        axes[0, 2],\n",
    "        dCg,\n",
    "        f\"Δ corr (Estres − Normal)\\n(max |Δ| = {max_abs_dCg:.3f}, escala ±{DELTA_CORR_LIM:.2f})\",\n",
    "        -DELTA_CORR_LIM,\n",
    "        DELTA_CORR_LIM,\n",
    "        \"coolwarm\",\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "    im = _hm(axes[1, 0], LtN, \"t-copula λ_L (asint.) — Normal\", 0, 1, \"viridis\")\n",
    "    fig.colorbar(im, ax=axes[1, 0], fraction=0.046)\n",
    "    im = _hm(axes[1, 1], LtS, \"t-copula λ_L (asint.) — Estres\", 0, 1, \"viridis\")\n",
    "    fig.colorbar(im, ax=axes[1, 1], fraction=0.046)\n",
    "    im = _hm(\n",
    "        axes[1, 2],\n",
    "        dLt,\n",
    "        f\"Δ λ_L (Estres − Normal)\\n(max |Δ| = {max_abs_dLt:.3f}, escala ±{DELTA_LAML_LIM:.2f})\",\n",
    "        -DELTA_LAML_LIM,\n",
    "        DELTA_LAML_LIM,\n",
    "        \"coolwarm\",\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[1, 2], fraction=0.046)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Quick ranking of tail-dependence increases (asymptotic) under t-copula\n",
    "    D = dLt.values\n",
    "    pairs = []\n",
    "    for i, a in enumerate(assets):\n",
    "        for j in range(i + 1, len(assets)):\n",
    "            b = assets[j]\n",
    "            pairs.append((float(D[i, j]), a, b))\n",
    "    top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "    display(pd.DataFrame(top, columns=[\"delta_lambdaL_t\", \"asset_a\", \"asset_b\"]))\n",
    "\n",
    "else:\n",
    "    print(\"Copula fit missing a state; available:\", list(fits.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d8004",
   "metadata": {},
   "source": [
    "### Puente Fase 3 -> Fase 4\n",
    "\n",
    "La celda siguiente resume con evidencia cuantitativa las diferencias entre Normal y Estrés en:\n",
    "\n",
    "- riesgo marginal,\n",
    "- dependencia,\n",
    "- features del HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fb6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:16.762083Z",
     "iopub.status.busy": "2026-02-11T18:19:16.761082Z",
     "iopub.status.idle": "2026-02-11T18:19:16.799022Z",
     "shell.execute_reply": "2026-02-11T18:19:16.798011Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Transicion Fase 3->4: resumen cuantitativo de diferencias entre regímenes ---\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "summary = pd.DataFrame(index=[\"Normal\", \"Estres\"])\n",
    "summary[\"n_days\"] = [int((~flags).sum()), int(flags.sum())]\n",
    "summary[\"pct_days\"] = summary[\"n_days\"] / len(flags)\n",
    "\n",
    "# Riesgo marginal medio por estado (promediado sobre activos)\n",
    "by_state = risk_by_state.groupby(\"state\")\n",
    "summary[\"avg_asset_vol\"] = by_state[\"vol\"].mean().reindex(summary.index)\n",
    "summary[\"avg_abs_VaR1\"] = (-by_state[\"VaR_1\"].mean()).reindex(summary.index)\n",
    "summary[\"avg_abs_ES1\"] = (-by_state[\"ES_1\"].mean()).reindex(summary.index)\n",
    "\n",
    "# Dependencia lineal media\n",
    "corr_n = ret.loc[~flags].corr()\n",
    "corr_s = ret.loc[flags].corr()\n",
    "tri = np.triu_indices_from(corr_n, k=1)\n",
    "summary.loc[\"Normal\", \"avg_pair_corr\"] = float(np.nanmean(corr_n.values[tri]))\n",
    "summary.loc[\"Estres\", \"avg_pair_corr\"] = float(np.nanmean(corr_s.values[tri]))\n",
    "\n",
    "# Features medias del HMM por estado\n",
    "feat = features.copy()\n",
    "feat[\"state\"] = np.where(flags.reindex(feat.index).astype(\"boolean\").fillna(False).astype(bool), \"Estres\", \"Normal\")\n",
    "feat_means = feat.groupby(\"state\")[[\"vol_equity_21\", \"vol_hy_21\", \"dd_equity\", \"r_credit\", \"r_equity\"]].mean()\n",
    "for col in feat_means.columns:\n",
    "    summary[col] = feat_means[col].reindex(summary.index)\n",
    "\n",
    "# Dependencia en cola t-copula (si existe fit en ambos estados)\n",
    "if \"Normal\" in fits and \"Estres\" in fits:\n",
    "    LtN = np.asarray(fits[\"Normal\"][\"t_taildep_lambdaL\"], dtype=float)\n",
    "    LtS = np.asarray(fits[\"Estres\"][\"t_taildep_lambdaL\"], dtype=float)\n",
    "    summary.loc[\"Normal\", \"avg_tcopula_lambdaL\"] = float(np.nanmean(LtN[tri]))\n",
    "    summary.loc[\"Estres\", \"avg_tcopula_lambdaL\"] = float(np.nanmean(LtS[tri]))\n",
    "\n",
    "display(summary.round(4))\n",
    "\n",
    "delta_pct = (summary.loc[\"Estres\"] / summary.loc[\"Normal\"] - 1.0) * 100.0\n",
    "key_cols = [\"avg_asset_vol\", \"avg_abs_VaR1\", \"avg_pair_corr\", \"vol_equity_21\", \"vol_hy_21\"]\n",
    "print()\n",
    "print(\"Cambios relativos Estres vs Normal (%) en metricas clave:\")\n",
    "print(delta_pct[key_cols].round(2))\n",
    "\n",
    "print()\n",
    "print(\"Interpretacion economica (sintesis):\")\n",
    "print(\"- El estado de Estres concentra mayor volatilidad y peores colas de pérdida.\")\n",
    "print(\"- La co-movilidad media entre activos aumenta (diversificación menos efectiva).\")\n",
    "print(\"- Suben las volatilidades realizadas (equity/HY) y se profundiza el drawdown de equity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654344fe",
   "metadata": {},
   "source": [
    "## 4) Fase 4 - Motor de simulación (10.000 trayectorias, 6 meses)\n",
    "\n",
    "Se implementa Monte Carlo con cambio de regimen diario mediante la matriz de transición del HMM y dependencia por cópula condicionada al estado.\n",
    "\n",
    "Validaciones incluidas:\n",
    "\n",
    "- wealth real vs bandas p5-p50-p95 simuladas,\n",
    "- reproduccion de regímenes,\n",
    "- reproducción de riesgo y dependencia (real vs simulado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df748d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:16.802560Z",
     "iopub.status.busy": "2026-02-11T18:19:16.801549Z",
     "iopub.status.idle": "2026-02-11T18:19:31.195885Z",
     "shell.execute_reply": "2026-02-11T18:19:31.195885Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 4: regime-switching MC (10k paths, 6M horizon) ---\n",
    "\n",
    "N_PATHS=10_000\n",
    "HORIZON_DAYS=126\n",
    "SEED=42\n",
    "ALPHA_99=0.01\n",
    "EXPORT=True\n",
    "\n",
    "ret=returns.copy()\n",
    "flags=is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "assets=list(ret.columns)\n",
    "OUT_DIR=Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "OUT_DIR.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def _nearest_psd_corr(C,eps=1e-8):\n",
    "    C=np.asarray(C,float); C=0.5*(C+C.T); np.fill_diagonal(C,1.0)\n",
    "    w,V=np.linalg.eigh(C); w=np.maximum(w,eps); C2=(V*w)@V.T\n",
    "    d=np.sqrt(np.diag(C2)); C2=C2/(d[:,None]*d[None,:]); np.fill_diagonal(C2,1.0)\n",
    "    return 0.5*(C2+C2.T)\n",
    "\n",
    "def _sample_t_copula_u(n,C,df,rng):\n",
    "    d=C.shape[0]; C=_nearest_psd_corr(C); L=np.linalg.cholesky(C)\n",
    "    y=rng.standard_normal((n,d))@L.T; w=rng.chisquare(df,size=n)/df; z=y/np.sqrt(w)[:,None]\n",
    "    u=t.cdf(z,df=df); return np.clip(u,1e-6,1-1e-6)\n",
    "\n",
    "def _inv_empirical(u,sample):\n",
    "    x=np.asarray(sample,float); x=x[np.isfinite(x)]\n",
    "    if len(x)<20: return np.full_like(u,np.nan,float)\n",
    "    xs=np.sort(x); p=np.arange(1,len(xs)+1)/(len(xs)+1.0)\n",
    "    return np.interp(u,p,xs)\n",
    "\n",
    "def _var_es(x,a=0.01):\n",
    "    x=np.asarray(x,float); x=x[np.isfinite(x)]\n",
    "    if len(x)==0: return np.nan,np.nan\n",
    "    q=float(np.quantile(x,a)); tail=x[x<=q]\n",
    "    return q,float(np.mean(tail)) if len(tail) else np.nan\n",
    "\n",
    "def _mdd_logret(r):\n",
    "    w=np.exp(np.cumsum(np.asarray(r,float))); peak=np.maximum.accumulate(w)\n",
    "    return float(np.min(w/peak-1.0))\n",
    "\n",
    "def _load_params():\n",
    "    if 'fits' in globals() and isinstance(globals().get('fits'),dict) and len(fits)>0:\n",
    "        if 'Normal' in fits and 'Estres' in fits:\n",
    "            return {'Normal':{'df':float(fits['Normal']['t_df']),'Ct':np.asarray(fits['Normal']['t_corr'],float)},\n",
    "                    'Estres':{'df':float(fits['Estres']['t_df']),'Ct':np.asarray(fits['Estres']['t_corr'],float)}}\n",
    "    s=json.loads((OUT_DIR/'phase3_copula_summary.json').read_text(encoding='utf-8'))\n",
    "    out={}\n",
    "    for nm,slug in [('Normal','normal'),('Estres','estres')]:\n",
    "        out[nm]={'df':float(s[nm]['t_df']),'Ct':pd.read_csv(OUT_DIR/f'phase3_copula_t_corr_{slug}.csv',index_col=0).loc[assets,assets].to_numpy()}\n",
    "    return out\n",
    "\n",
    "def _sample_state(n,Ct,df,hist_df,rng):\n",
    "    if n<=0: return np.empty((0,len(assets)))\n",
    "    hist=hist_df[assets].dropna(how='any')\n",
    "    U=_sample_t_copula_u(n,Ct,df,rng); R=np.zeros((n,len(assets)))\n",
    "    for j,a in enumerate(assets): R[:,j]=_inv_empirical(U[:,j],hist[a].to_numpy())\n",
    "    return R\n",
    "\n",
    "def _state_stats(s01):\n",
    "    x=np.asarray(s01,np.int8)\n",
    "    if x.size==0: return dict(pct_normal=np.nan,pct_estres=np.nan,avg_dur_normal=np.nan,avg_dur_estres=np.nan,n_switches=np.nan)\n",
    "    runs=[]; cur=int(x[0]); ln=1\n",
    "    for v in x[1:]:\n",
    "        v=int(v)\n",
    "        if v==cur: ln+=1\n",
    "        else: runs.append((cur,ln)); cur=v; ln=1\n",
    "    runs.append((cur,ln)); d0=[l for s,l in runs if s==0]; d1=[l for s,l in runs if s==1]\n",
    "    return dict(pct_normal=float(np.mean(x==0)),pct_estres=float(np.mean(x==1)),avg_dur_normal=float(np.mean(d0)) if d0 else np.nan,avg_dur_estres=float(np.mean(d1)) if d1 else np.nan,n_switches=float(np.sum(x[1:]!=x[:-1])) if x.size>1 else 0.0)\n",
    "\n",
    "def _simulate_states(n_paths,horizon,P,pi,rng):\n",
    "    S=np.zeros((n_paths,horizon),dtype=np.int8); S[:,0]=(rng.random(n_paths)<float(pi[1])).astype(np.int8)\n",
    "    for t in range(1,horizon):\n",
    "        prev=S[:,t-1]; p1=np.where(prev==0,P[0,1],P[1,1]); S[:,t]=(rng.random(n_paths)<p1).astype(np.int8)\n",
    "    return S\n",
    "\n",
    "params_by_state=_load_params()\n",
    "hist_by_state={'Normal':ret.loc[~flags,assets].dropna(how='any'),'Estres':ret.loc[flags,assets].dropna(how='any')}\n",
    "if len(hist_by_state['Normal'])<100 or len(hist_by_state['Estres'])<100:\n",
    "    raise ValueError(f\"Insufficient history by state. Normal={len(hist_by_state['Normal'])}, Estres={len(hist_by_state['Estres'])}\")\n",
    "\n",
    "# Calibracion de transiciones para simulacion:\n",
    "# por defecto se usan transiciones empiricas de la serie historica etiquetada,\n",
    "# porque reproducen mejor frecuencias y duraciones de regimen.\n",
    "USE_EMPIRICAL_TRANSITIONS = True\n",
    "\n",
    "if USE_EMPIRICAL_TRANSITIONS:\n",
    "    s=flags.astype(int).to_numpy(); cnt=np.zeros((2,2),float)\n",
    "    for a,b in zip(s[:-1],s[1:]):\n",
    "        cnt[int(a),int(b)] += 1.0\n",
    "    # Suavizado de Laplace para evitar ceros estructurales\n",
    "    P=(cnt + 1.0) / (cnt + 1.0).sum(axis=1,keepdims=True)\n",
    "    Psrc='empirical_from_flags'\n",
    "elif 'hmm' in globals() and hasattr(hmm,'transmat_') and 'crisis_state' in globals():\n",
    "    T=np.asarray(hmm.transmat_,float); c=int(crisis_state); n=1-c\n",
    "    P=T[np.ix_([n,c],[n,c])]; Psrc='hmm.transmat_'\n",
    "else:\n",
    "    s=flags.astype(int).to_numpy(); cnt=np.zeros((2,2),float)\n",
    "    for a,b in zip(s[:-1],s[1:]):\n",
    "        cnt[int(a),int(b)] += 1.0\n",
    "    P=(cnt + 1.0) / (cnt + 1.0).sum(axis=1,keepdims=True)\n",
    "    Psrc='empirical_from_flags_fallback'\n",
    "\n",
    "P=np.asarray(P,float)\n",
    "P=np.maximum(P,0.0)\n",
    "P=P/np.where(P.sum(axis=1,keepdims=True)<=1e-12,1.0,P.sum(axis=1,keepdims=True))\n",
    "pi=np.array([float((~flags).mean()),float(flags.mean())],float)\n",
    "pi=pi/pi.sum()\n",
    "\n",
    "print('Phase4 setup:',{'paths':N_PATHS,'horizon_days':HORIZON_DAYS,'transition_source':Psrc,'pi':[float(pi[0]),float(pi[1])]})\n",
    "display(pd.DataFrame(P,index=['from_Normal','from_Estres'],columns=['to_Normal','to_Estres']))\n",
    "\n",
    "rng=np.random.default_rng(SEED)\n",
    "S_paths=_simulate_states(N_PATHS,HORIZON_DAYS,P,pi,rng)\n",
    "S_flat=S_paths.reshape(-1)\n",
    "idxN=np.flatnonzero(S_flat==0); idxS=np.flatnonzero(S_flat==1)\n",
    "R_flat=np.zeros((S_flat.size,len(assets)),float)\n",
    "R_flat[idxN]=_sample_state(len(idxN),params_by_state['Normal']['Ct'],params_by_state['Normal']['df'],hist_by_state['Normal'],rng)\n",
    "R_flat[idxS]=_sample_state(len(idxS),params_by_state['Estres']['Ct'],params_by_state['Estres']['df'],hist_by_state['Estres'],rng)\n",
    "R_paths=R_flat.reshape(N_PATHS,HORIZON_DAYS,len(assets))\n",
    "\n",
    "w=np.ones(len(assets),float)/len(assets)\n",
    "rp_paths=np.einsum('ntd,d->nt',R_paths,w)\n",
    "wealth_paths=np.exp(np.cumsum(rp_paths,axis=1))\n",
    "rp_real=(ret[assets]@w).dropna(); rp_real_last=rp_real.iloc[-HORIZON_DAYS:].to_numpy(); wealth_real_last=np.exp(np.cumsum(rp_real_last))\n",
    "real_6m=np.exp(rp_real.rolling(HORIZON_DAYS).sum().dropna().to_numpy())-1.0\n",
    "sim_6m=np.exp(rp_paths.sum(axis=1))-1.0\n",
    "\n",
    "bands=pd.DataFrame({'day':np.arange(1,HORIZON_DAYS+1),'p5':np.percentile(wealth_paths,5,axis=0),'p50':np.percentile(wealth_paths,50,axis=0),'p95':np.percentile(wealth_paths,95,axis=0),'real_last_window':wealth_real_last})\n",
    "fig,ax=plt.subplots(figsize=(11,5))\n",
    "ax.fill_between(bands['day'],bands['p5'],bands['p95'],alpha=0.22,color='#4C78A8',label='Sim p5-p95')\n",
    "ax.plot(bands['day'],bands['p50'],color='#1f4e79',lw=1.8,label='Sim p50')\n",
    "ax.plot(bands['day'],bands['real_last_window'],color='#E45756',lw=1.6,label='Real (ultimos 6 meses)')\n",
    "ax.set_title(f'Fase 4 - Wealth cartera (n={N_PATHS:,}, h={HORIZON_DAYS}d)'); ax.set_xlabel('Dia'); ax.set_ylabel('Wealth base=1'); ax.grid(alpha=0.25); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "rp_sim_daily=rp_paths.reshape(-1); rp_real_daily=rp_real.to_numpy()\n",
    "fig,ax=plt.subplots(figsize=(10,4))\n",
    "bins=np.linspace(np.nanquantile(np.concatenate([rp_real_daily,rp_sim_daily]),0.001),np.nanquantile(np.concatenate([rp_real_daily,rp_sim_daily]),0.999),100)\n",
    "ax.hist(rp_real_daily,bins=bins,density=True,alpha=0.45,label='Real daily',color='#72B7B2')\n",
    "ax.hist(rp_sim_daily,bins=bins,density=True,alpha=0.45,label='Sim daily',color='#F58518')\n",
    "ax.set_title('Fase 4 - Distribucion retornos diarios cartera (real vs sim)'); ax.set_xlabel('Retorno log diario'); ax.grid(alpha=0.25); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "reg_real=_state_stats(flags.astype(int).to_numpy())\n",
    "path_stats=[_state_stats(S_paths[i]) for i in range(S_paths.shape[0])]\n",
    "reg_sim={k:float(np.nanmean([ps[k] for ps in path_stats])) for k in path_stats[0].keys()}\n",
    "\n",
    "real_len=len(flags)\n",
    "sim_len=HORIZON_DAYS\n",
    "switch_rate_real=float(reg_real['n_switches']/max(real_len-1,1)*252.0)\n",
    "switch_rate_sim=float(np.nanmean([ps['n_switches']/max(sim_len-1,1)*252.0 for ps in path_stats]))\n",
    "\n",
    "regime_tbl=pd.DataFrame([\n",
    "    {'dataset':'Real',**reg_real,'switches_252d':switch_rate_real},\n",
    "    {'dataset':'Simulado (promedio paths)',**reg_sim,'switches_252d':switch_rate_sim}\n",
    "])\n",
    "display(regime_tbl)\n",
    "print(f\"Nota: n_switches depende del tamano de ventana. Para comparar de forma homogenea, usar switches_252d.\")\n",
    "\n",
    "mdd_sim=np.array([_mdd_logret(rp_paths[i]) for i in range(N_PATHS)],float)\n",
    "\n",
    "# MDD comparable a 6 meses: real en ventanas rolling de HORIZON_DAYS vs simulado por trayectoria\n",
    "if len(rp_real_daily) >= HORIZON_DAYS:\n",
    "    real_roll = np.lib.stride_tricks.sliding_window_view(rp_real_daily, HORIZON_DAYS)\n",
    "    mdd_real_6m = np.array([_mdd_logret(real_roll[i]) for i in range(real_roll.shape[0])], float)\n",
    "else:\n",
    "    mdd_real_6m = np.array([_mdd_logret(rp_real_daily)], float)\n",
    "\n",
    "var99_rd,es99_rd=_var_es(rp_real_daily,ALPHA_99); var99_sd,es99_sd=_var_es(rp_sim_daily,ALPHA_99)\n",
    "var99_r6,es99_r6=_var_es(real_6m,ALPHA_99); var99_s6,es99_s6=_var_es(sim_6m,ALPHA_99)\n",
    "risk_tbl=pd.DataFrame([\n",
    "{'dataset':'Real','vol_daily':float(np.std(rp_real_daily,ddof=1)),'vol_ann':float(np.std(rp_real_daily,ddof=1)*np.sqrt(252)),'max_drawdown_6m_mean':float(np.mean(mdd_real_6m)),'max_drawdown_6m_p95':float(np.quantile(mdd_real_6m,0.95)),'max_drawdown_fullsample':float(_mdd_logret(rp_real_daily)),'VaR99_daily':float(var99_rd),'ES99_daily':float(es99_rd),'VaR99_6m':float(var99_r6),'ES99_6m':float(es99_r6)},\n",
    "{'dataset':'Simulado','vol_daily':float(np.std(rp_sim_daily,ddof=1)),'vol_ann':float(np.std(rp_sim_daily,ddof=1)*np.sqrt(252)),'max_drawdown_6m_mean':float(np.mean(mdd_sim)),'max_drawdown_6m_p95':float(np.quantile(mdd_sim,0.95)),'max_drawdown_fullsample':np.nan,'VaR99_daily':float(var99_sd),'ES99_daily':float(es99_sd),'VaR99_6m':float(var99_s6),'ES99_6m':float(es99_s6)}\n",
    "]); display(risk_tbl)\n",
    "print('Nota: max_drawdown_6m_* es la comparacion homogena real vs simulado a 6 meses; max_drawdown_fullsample se reporta solo como referencia historica real.')\n",
    "\n",
    "simN=pd.DataFrame(R_flat[idxN],columns=assets); simS=pd.DataFrame(R_flat[idxS],columns=assets)\n",
    "def _avg_corr(df):\n",
    "    C=df.corr().to_numpy(float); iu=np.triu_indices(C.shape[0],k=1); v=C[iu]; v=v[np.isfinite(v)]; return float(np.mean(v)) if len(v) else np.nan\n",
    "\n",
    "dep_tbl=pd.DataFrame([\n",
    "{'state':'Normal','n_hist':int(len(hist_by_state['Normal'])),'n_sim':int(len(simN)),'avg_vol_hist':float(hist_by_state['Normal'].std(ddof=1).mean()),'avg_vol_sim':float(simN.std(ddof=1).mean()),'avg_corr_hist':float(_avg_corr(hist_by_state['Normal'])),'avg_corr_sim':float(_avg_corr(simN))},\n",
    "{'state':'Estres','n_hist':int(len(hist_by_state['Estres'])),'n_sim':int(len(simS)),'avg_vol_hist':float(hist_by_state['Estres'].std(ddof=1).mean()),'avg_vol_sim':float(simS.std(ddof=1).mean()),'avg_corr_hist':float(_avg_corr(hist_by_state['Estres'])),'avg_corr_sim':float(_avg_corr(simS))}\n",
    "]);\n",
    "\n",
    "dep_summary=pd.DataFrame([\n",
    "{'metric':'vol_ratio_estres_vs_normal','hist':float(dep_tbl.loc[dep_tbl['state']=='Estres','avg_vol_hist'].iloc[0]/dep_tbl.loc[dep_tbl['state']=='Normal','avg_vol_hist'].iloc[0]),'sim':float(dep_tbl.loc[dep_tbl['state']=='Estres','avg_vol_sim'].iloc[0]/dep_tbl.loc[dep_tbl['state']=='Normal','avg_vol_sim'].iloc[0])},\n",
    "{'metric':'delta_avg_corr_estres_minus_normal','hist':float(dep_tbl.loc[dep_tbl['state']=='Estres','avg_corr_hist'].iloc[0]-dep_tbl.loc[dep_tbl['state']=='Normal','avg_corr_hist'].iloc[0]),'sim':float(dep_tbl.loc[dep_tbl['state']=='Estres','avg_corr_sim'].iloc[0]-dep_tbl.loc[dep_tbl['state']=='Normal','avg_corr_sim'].iloc[0])}\n",
    "])\n",
    "\n",
    "display(dep_tbl); display(dep_summary)\n",
    "\n",
    "if EXPORT:\n",
    "    bands.to_csv(OUT_DIR/f'phase4_wealth_bands_6m_n{N_PATHS}.csv',index=False)\n",
    "    regime_tbl.to_csv(OUT_DIR/f'phase4_regime_reproduction_6m_n{N_PATHS}.csv',index=False)\n",
    "    risk_tbl.to_csv(OUT_DIR/f'phase4_portfolio_risk_validation_6m_n{N_PATHS}.csv',index=False)\n",
    "    dep_tbl.to_csv(OUT_DIR/f'phase4_dependence_validation_6m_n{N_PATHS}.csv',index=False)\n",
    "    dep_summary.to_csv(OUT_DIR/f'phase4_dependence_summary_6m_n{N_PATHS}.csv',index=False)\n",
    "    pd.DataFrame(P,index=['from_Normal','from_Estres'],columns=['to_Normal','to_Estres']).to_csv(OUT_DIR/'phase4_transition_matrix_normal_estres.csv')\n",
    "    print('Saved phase4 outputs in',OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5827993",
   "metadata": {},
   "source": [
    "## 5) Fase 5 - Escenarios de estrés (10.000 trayectorias, 6 meses)\n",
    "\n",
    "Escenarios ejecutados:\n",
    "\n",
    "- Escenario 1: Estanflación 2022\n",
    "- Escenario 2: Crisis de crédito 2008\n",
    "- Escenario 3: Alternativo de liquidez global\n",
    "\n",
    "Se reportan VaR 99% y ES/CVaR 99% para comparacion base vs escenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe34801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:19:31.199278Z",
     "iopub.status.busy": "2026-02-11T18:19:31.199278Z",
     "iopub.status.idle": "2026-02-11T18:20:22.766206Z",
     "shell.execute_reply": "2026-02-11T18:20:22.765685Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 5: stress scenarios (10k paths, 6-month horizon) ---\n",
    "\n",
    "N_STRESS=10_000\n",
    "HORIZON_STRESS_DAYS=126\n",
    "SEED_STRESS=123\n",
    "EXPORT_STRESS=True\n",
    "ALPHA_99=0.01\n",
    "\n",
    "required=['assets','w','P','pi','params_by_state','hist_by_state','_simulate_states','_sample_state','_var_es','_mdd_logret','_nearest_psd_corr']\n",
    "missing=[k for k in required if k not in globals()]\n",
    "if missing: raise RuntimeError(f'Missing prereqs for Phase 5: {missing}. Run Phase 4 first.')\n",
    "\n",
    "OUT_DIR=Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "OUT_DIR.mkdir(parents=True,exist_ok=True)\n",
    "ret=returns.copy(); assets=list(assets); w=np.asarray(w,float)\n",
    "\n",
    "base_df=float(params_by_state['Estres']['df'])\n",
    "base_Ct=np.asarray(params_by_state['Estres']['Ct'],float)\n",
    "histS=hist_by_state['Estres'].copy()\n",
    "\n",
    "def _sample_t_copula_u(n,C,df,rng):\n",
    "    d=C.shape[0]; C=_nearest_psd_corr(C); L=np.linalg.cholesky(C)\n",
    "    y=rng.standard_normal((n,d))@L.T; wchi=rng.chisquare(df,size=n)/df; z=y/np.sqrt(wchi)[:,None]\n",
    "    from scipy.stats import t as _t\n",
    "    u=_t.cdf(z,df=df); return np.clip(u,1e-6,1-1e-6)\n",
    "\n",
    "def _inv_empirical(u,sample):\n",
    "    x=np.asarray(sample,float); x=x[np.isfinite(x)]\n",
    "    if len(x)<20: return np.full_like(u,np.nan,float)\n",
    "    xs=np.sort(x); p=np.arange(1,len(xs)+1)/(len(xs)+1.0)\n",
    "    return np.interp(u,p,xs)\n",
    "\n",
    "def _simulate_state_from_copula(n,Ct,df,hist,rng):\n",
    "    hist=hist.loc[:,assets].dropna(how='any')\n",
    "    U=_sample_t_copula_u(n,Ct,df,rng); R=np.zeros((n,len(assets)),float)\n",
    "    for j,a in enumerate(assets): R[:,j]=_inv_empirical(U[:,j],hist[a].to_numpy())\n",
    "    return R\n",
    "\n",
    "def _shock_dependence_contagion(C,risky_idx,target_rho=0.85,alpha=0.60):\n",
    "    C2=np.array(C,float,copy=True)\n",
    "    for i in risky_idx:\n",
    "        for j in risky_idx:\n",
    "            if i!=j: C2[i,j]=(1-alpha)*C2[i,j]+alpha*target_rho\n",
    "    np.fill_diagonal(C2,1.0); return _nearest_psd_corr(C2)\n",
    "\n",
    "def _tweak_pairs(C,pairs,alpha=0.70):\n",
    "    C2=np.array(C,float,copy=True)\n",
    "    for a,b,target in pairs:\n",
    "        if a in assets and b in assets:\n",
    "            i,j=assets.index(a),assets.index(b)\n",
    "            C2[i,j]=(1-alpha)*C2[i,j]+alpha*float(target); C2[j,i]=C2[i,j]\n",
    "    np.fill_diagonal(C2,1.0); return _nearest_psd_corr(C2)\n",
    "\n",
    "def _shock_marginal_vol_mean(R,vol_mult=1.50,mean_shift=0.0,cols=None):\n",
    "    X=np.array(R,float,copy=True)\n",
    "    if cols is None: cols=list(range(X.shape[1]))\n",
    "    mu=np.nanmean(X[:,cols],axis=0); X[:,cols]=(mu+vol_mult*(X[:,cols]-mu))+float(mean_shift)\n",
    "    return X\n",
    "\n",
    "def _shock_tail_left(R,q=0.01,tail_mult=1.50,cols=None):\n",
    "    X=np.array(R,float,copy=True)\n",
    "    if cols is None: cols=list(range(X.shape[1]))\n",
    "    for j in cols:\n",
    "        qv=float(np.quantile(X[:,j],q)); m=X[:,j]<=qv; X[m,j]=qv+float(tail_mult)*(X[m,j]-qv)\n",
    "    return X\n",
    "\n",
    "def _portfolio_metrics(R,w):\n",
    "    rp=R@w; v5,e5=_var_es(rp,0.05); v1,e1=_var_es(rp,0.01)\n",
    "    return {'VaR5':float(v5),'ES5':float(e5),'VaR1':float(v1),'ES1':float(e1),'mean':float(np.mean(rp)),'vol':float(np.std(rp,ddof=1))}\n",
    "\n",
    "def _portfolio_metrics_paths(rp_paths):\n",
    "    daily=rp_paths.reshape(-1); sixm=np.exp(np.sum(rp_paths,axis=1))-1.0\n",
    "    mdd=np.array([_mdd_logret(rp_paths[i]) for i in range(rp_paths.shape[0])],float)\n",
    "    v99d,e99d=_var_es(daily,ALPHA_99); v99m,e99m=_var_es(sixm,ALPHA_99)\n",
    "    return {'vol_daily':float(np.std(daily,ddof=1)),'vol_ann':float(np.std(daily,ddof=1)*np.sqrt(252)),'max_drawdown_mean':float(np.mean(mdd)),'max_drawdown_p95':float(np.quantile(mdd,0.95)),'VaR99_daily':float(v99d),'ES99_daily':float(e99d),'VaR99_6m':float(v99m),'ES99_6m':float(e99m)}\n",
    "\n",
    "if 'EQUITY_TICKERS' in globals() and EQUITY_TICKERS: EQUITY_CORE=[t for t in EQUITY_TICKERS if t in assets]\n",
    "else: EQUITY_CORE=[t for t in assets if t not in ['GLD','IEF','SHY','HYG']]\n",
    "RISKY=[t for t in (EQUITY_CORE+['HYG']) if t in assets]; risky_idx=[assets.index(a) for a in RISKY]\n",
    "\n",
    "scenario_specs=[\n",
    "{'name':'S1_Estanflacion_2022','label':'Escenario 1 (Estanflacion 2022)','desc':'Inflacion persistente: equity y bonos corrigen juntos, volatilidad elevada.','df':max(3.0,base_df*0.90),'Ct':np.array(base_Ct,copy=True),'vol_mult':1.35,'tail_mult':1.25,'mean_shift':-0.0004,'apply_cols':list(range(len(assets)))},\n",
    "{'name':'S2_Crisis_Credito_2008','label':'Escenario 2 (Crisis de Credito 2008)','desc':'Ruptura de credito: HYG y equity altamente sincronizados, cola izquierda severa.','df':max(3.0,base_df*0.65),'Ct':np.array(base_Ct,copy=True),'vol_mult':1.55,'tail_mult':1.70,'mean_shift':-0.0008,'apply_cols':[assets.index('HYG')]+[assets.index(x) for x in EQUITY_CORE if x in assets] if 'HYG' in assets else list(range(len(assets)))},\n",
    "{'name':'S3_Alternativo_Liquidez_Global','label':'Escenario 3 (Alternativo: Contagio de liquidez)','desc':'Shock de liquidez transversal con aumento de correlaciones y colas en todos los bloques.','df':max(3.0,base_df*0.80),'Ct':np.array(base_Ct,copy=True),'vol_mult':1.45,'tail_mult':1.45,'mean_shift':-0.0006,'apply_cols':list(range(len(assets)))}]\n",
    "\n",
    "pairs_eq_bond=[]\n",
    "for eq in EQUITY_CORE:\n",
    "    if 'IEF' in assets: pairs_eq_bond.append((eq,'IEF',0.25))\n",
    "    if 'SHY' in assets: pairs_eq_bond.append((eq,'SHY',0.15))\n",
    "pairs_hyg_eq=[('HYG',eq,0.85) for eq in EQUITY_CORE] if 'HYG' in assets else []\n",
    "pairs_liq=[(a,'GLD',0.25) for a in RISKY if 'GLD' in assets]\n",
    "\n",
    "for spec in scenario_specs:\n",
    "    if spec['name']=='S1_Estanflacion_2022':\n",
    "        C1=_shock_dependence_contagion(base_Ct,risky_idx,target_rho=0.75,alpha=0.45); spec['Ct']=_tweak_pairs(C1,pairs_eq_bond,alpha=0.70)\n",
    "    elif spec['name']=='S2_Crisis_Credito_2008':\n",
    "        C2=_shock_dependence_contagion(base_Ct,risky_idx,target_rho=0.90,alpha=0.75); spec['Ct']=_tweak_pairs(C2,pairs_hyg_eq,alpha=0.80)\n",
    "    else:\n",
    "        C3=_shock_dependence_contagion(base_Ct,risky_idx,target_rho=0.82,alpha=0.60); spec['Ct']=_tweak_pairs(C3,pairs_liq,alpha=0.55)\n",
    "\n",
    "def _simulate_paths_under_spec(spec,n_paths,horizon,seed):\n",
    "    rng=np.random.default_rng(seed); S=_simulate_states(n_paths,horizon,P,pi,rng); sf=S.reshape(-1)\n",
    "    idxN=np.flatnonzero(sf==0); idxS=np.flatnonzero(sf==1); Rf=np.zeros((sf.size,len(assets)),float)\n",
    "    Rf[idxN]=_sample_state(len(idxN),params_by_state['Normal']['Ct'],params_by_state['Normal']['df'],hist_by_state['Normal'],rng)\n",
    "    Rs=_sample_state(len(idxS),spec['Ct'],spec['df'],hist_by_state['Estres'],rng)\n",
    "    Rs=_shock_marginal_vol_mean(Rs,vol_mult=float(spec['vol_mult']),mean_shift=float(spec.get('mean_shift',0.0)),cols=spec.get('apply_cols'))\n",
    "    Rs=_shock_tail_left(Rs,q=0.01,tail_mult=float(spec['tail_mult']),cols=spec.get('apply_cols'))\n",
    "    Rf[idxS]=Rs\n",
    "    Rp=Rf.reshape(n_paths,horizon,len(assets)); rpp=np.einsum('ntd,d->nt',Rp,w); W=np.exp(np.cumsum(rpp,axis=1))\n",
    "    return {'S_paths':S,'R_paths':Rp,'rp_paths':rpp,'wealth_paths':W,'R_flat':Rf}\n",
    "\n",
    "base_spec={'name':'BASE_HMM_COPULA','label':'Base (HMM + copulas por estado)','desc':'Simulacion base sin shock adicional.','df':base_df,'Ct':np.array(base_Ct,copy=True),'vol_mult':1.0,'tail_mult':1.0,'mean_shift':0.0,'apply_cols':None}\n",
    "all_specs=[base_spec]+scenario_specs\n",
    "scenario_results={}; rows=[]\n",
    "\n",
    "for i,spec in enumerate(all_specs):\n",
    "    out=_simulate_paths_under_spec(spec,N_STRESS,HORIZON_STRESS_DAYS,SEED_STRESS+i); scenario_results[spec['name']]=out\n",
    "    m=_portfolio_metrics_paths(out['rp_paths'])\n",
    "    reg=np.array([_state_stats(out['S_paths'][k])['pct_estres'] for k in range(out['S_paths'].shape[0])],float)\n",
    "    dur=np.array([_state_stats(out['S_paths'][k])['avg_dur_estres'] for k in range(out['S_paths'].shape[0])],float)\n",
    "    sw=np.array([_state_stats(out['S_paths'][k])['n_switches'] for k in range(out['S_paths'].shape[0])],float)\n",
    "    sf=out['S_paths'].reshape(-1); idxS=np.flatnonzero(sf==1); stress_df=pd.DataFrame(out['R_flat'][idxS],columns=assets)\n",
    "    C=stress_df.corr().to_numpy(float); iu=np.triu_indices(C.shape[0],k=1); v=C[iu]; v=v[np.isfinite(v)]; avg_corr=float(np.mean(v)) if len(v) else np.nan\n",
    "    rows.append({'scenario':spec['name'],'label':spec.get('label',spec['name']),'desc':spec['desc'],'n_paths':int(N_STRESS),'horizon_days':int(HORIZON_STRESS_DAYS),'df_used_stress':float(spec['df']),'vol_mult_stress':float(spec['vol_mult']),'tail_mult_stress':float(spec['tail_mult']),'mean_shift_stress':float(spec.get('mean_shift',0.0)),'pct_estres_sim':float(np.nanmean(reg)),'avg_dur_estres_sim':float(np.nanmean(dur)),'n_switches_sim':float(np.nanmean(sw)),'avg_corr_stress_sim':float(avg_corr),**m})\n",
    "\n",
    "stress_summary=pd.DataFrame(rows).sort_values('ES99_6m').reset_index(drop=True)\n",
    "display(stress_summary)\n",
    "\n",
    "rp_real=(ret[assets]@w).dropna(); rp_real_last=rp_real.iloc[-HORIZON_STRESS_DAYS:].to_numpy(); wealth_real_last=np.exp(np.cumsum(rp_real_last))\n",
    "days=np.arange(1,HORIZON_STRESS_DAYS+1)\n",
    "fig,axes=plt.subplots(1,2,figsize=(15,5))\n",
    "base_w=scenario_results['BASE_HMM_COPULA']['wealth_paths']; p5=np.percentile(base_w,5,axis=0); p50=np.percentile(base_w,50,axis=0); p95=np.percentile(base_w,95,axis=0)\n",
    "axes[0].fill_between(days,p5,p95,alpha=0.20,color='#4C78A8',label='Base p5-p95'); axes[0].plot(days,p50,color='#1f4e79',lw=2.0,label='Base p50'); axes[0].plot(days,wealth_real_last,color='#E45756',lw=1.6,label='Real (ultimos 6 meses)')\n",
    "for spec in scenario_specs:\n",
    "    med=np.percentile(scenario_results[spec['name']]['wealth_paths'],50,axis=0); axes[0].plot(days,med,lw=1.3,label=spec['name'])\n",
    "axes[0].set_title('Fase 5 - Wealth cartera (validacion 6M)'); axes[0].set_xlabel('Dia'); axes[0].set_ylabel('Wealth base=1'); axes[0].grid(alpha=0.25); axes[0].legend(fontsize=8)\n",
    "for spec in all_specs:\n",
    "    sixm=np.exp(np.sum(scenario_results[spec['name']]['rp_paths'],axis=1))-1.0; axes[1].hist(sixm,bins=80,density=True,alpha=0.35,label=spec['name'])\n",
    "axes[1].set_title('Fase 5 - Distribucion retorno 6M (base vs estres)'); axes[1].set_xlabel('Retorno simple 6M'); axes[1].grid(alpha=0.25); axes[1].legend(fontsize=8)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def _avg_block_corr(C,names):\n",
    "    idx=[assets.index(a) for a in names if a in assets]\n",
    "    if len(idx)<2: return np.nan\n",
    "    vals=[]\n",
    "    for i in range(len(idx)):\n",
    "        for j in range(i+1,len(idx)): vals.append(float(C[idx[i],idx[j]]))\n",
    "    return float(np.mean(vals)) if vals else np.nan\n",
    "\n",
    "def _avg_pair_corr(C,pairs):\n",
    "    vals=[]\n",
    "    for a,b in pairs:\n",
    "        if a in assets and b in assets:\n",
    "            i,j=assets.index(a),assets.index(b); vals.append(float(C[i,j]))\n",
    "    return float(np.mean(vals)) if vals else np.nan\n",
    "\n",
    "pairs_bonds=[]\n",
    "for eq in EQUITY_CORE:\n",
    "    for b in ['IEF','SHY']:\n",
    "        if b in assets: pairs_bonds.append((eq,b))\n",
    "pairs_hyg=[('HYG',eq) for eq in EQUITY_CORE] if 'HYG' in assets else []\n",
    "\n",
    "lever_rows=[]\n",
    "for spec in all_specs:\n",
    "    Ct=np.asarray(spec['Ct'],float); ac=spec.get('apply_cols')\n",
    "    apply_to=','.join([assets[i] for i in ac if 0<=i<len(assets)]) if isinstance(ac,list) else 'all'\n",
    "    lever_rows.append({'scenario':spec['name'],'label':spec.get('label',spec['name']),'dependence_shock':'No' if spec['name']=='BASE_HMM_COPULA' else 'Si','tail_shock':'Si' if float(spec.get('tail_mult',1.0))>1.0 or float(spec.get('df',base_df))<float(base_df) else 'No','vol_shock':'Si' if float(spec.get('vol_mult',1.0))>1.0 else 'No','drift_shock':'Si' if abs(float(spec.get('mean_shift',0.0)))>0 else 'No','df_used':float(spec['df']),'vol_mult':float(spec['vol_mult']),'tail_mult':float(spec['tail_mult']),'mean_shift':float(spec.get('mean_shift',0.0)),'apply_to':apply_to,'avg_corr_risky':_avg_block_corr(Ct,RISKY),'avg_corr_bonds_hedge':_avg_pair_corr(Ct,pairs_bonds),'avg_corr_hyg_equity':_avg_pair_corr(Ct,pairs_hyg),'desc':spec['desc']})\n",
    "\n",
    "lever_tbl=pd.DataFrame(lever_rows)\n",
    "base_row=lever_tbl[lever_tbl['scenario']=='BASE_HMM_COPULA'].iloc[0]\n",
    "lever_tbl['delta_avg_corr_risky']=lever_tbl['avg_corr_risky']-float(base_row['avg_corr_risky'])\n",
    "lever_tbl['delta_avg_corr_bonds_hedge']=lever_tbl['avg_corr_bonds_hedge']-float(base_row['avg_corr_bonds_hedge'])\n",
    "lever_tbl['delta_avg_corr_hyg_equity']=lever_tbl['avg_corr_hyg_equity']-float(base_row['avg_corr_hyg_equity'])\n",
    "display(lever_tbl)\n",
    "\n",
    "if EXPORT_STRESS:\n",
    "    stress_summary.to_csv(OUT_DIR/f'phase5_stress_summary_6m_n{N_STRESS}.csv',index=False)\n",
    "    lever_tbl.to_csv(OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv',index=False)\n",
    "    lever_tbl.to_csv(OUT_DIR/'phase5_stress_levers_summary.csv',index=False)\n",
    "    print('Saved phase5 outputs in',OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89719e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:20:22.769336Z",
     "iopub.status.busy": "2026-02-11T18:20:22.768291Z",
     "iopub.status.idle": "2026-02-11T18:20:22.788032Z",
     "shell.execute_reply": "2026-02-11T18:20:22.788032Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 5 (reporting): levers / palancas summary ---\n",
    "\n",
    "OUT_DIR=Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "if 'lever_tbl' not in globals() or lever_tbl is None:\n",
    "    p1=OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv'\n",
    "    p2=OUT_DIR/'phase5_stress_levers_summary.csv'\n",
    "    if p1.exists(): lever_tbl=pd.read_csv(p1)\n",
    "    elif p2.exists(): lever_tbl=pd.read_csv(p2)\n",
    "    else: raise RuntimeError('No encuentro tabla de palancas. Ejecuta primero la celda principal de Fase 5.')\n",
    "\n",
    "cols=['scenario','label','dependence_shock','tail_shock','vol_shock','drift_shock','df_used','vol_mult','tail_mult','mean_shift','apply_to','avg_corr_risky','avg_corr_bonds_hedge','avg_corr_hyg_equity','delta_avg_corr_risky','delta_avg_corr_bonds_hedge','delta_avg_corr_hyg_equity','desc']\n",
    "lever_tbl=lever_tbl[[c for c in cols if c in lever_tbl.columns]]\n",
    "display(lever_tbl)\n",
    "lever_tbl.to_csv(OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv',index=False)\n",
    "print('Saved:',OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f10ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:20:22.790685Z",
     "iopub.status.busy": "2026-02-11T18:20:22.790685Z",
     "iopub.status.idle": "2026-02-11T18:20:24.013811Z",
     "shell.execute_reply": "2026-02-11T18:20:24.013290Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 5 - Plots de palancas (comite-friendly) ---\n",
    "\n",
    "OUT_DIR=Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "_lever_tbl=globals().get('lever_tbl')\n",
    "if _lever_tbl is None:\n",
    "    p=OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv'\n",
    "    if not p.exists(): raise RuntimeError('No encuentro tabla de palancas. Ejecuta la celda anterior de Fase 5.')\n",
    "    _lever_tbl=pd.read_csv(p)\n",
    "\n",
    "if 'scenario' in _lever_tbl.columns:\n",
    "    order=['BASE_HMM_COPULA','S1_Estanflacion_2022','S2_Crisis_Credito_2008','S3_Alternativo_Liquidez_Global']\n",
    "    cats=[c for c in order if c in set(_lever_tbl['scenario'])]\n",
    "    _lever_tbl=_lever_tbl.copy(); _lever_tbl['scenario']=pd.Categorical(_lever_tbl['scenario'],categories=cats,ordered=True); _lever_tbl=_lever_tbl.sort_values('scenario')\n",
    "\n",
    "x=_lever_tbl['scenario'].astype(str).tolist() if 'scenario' in _lever_tbl.columns else [str(i) for i in range(len(_lever_tbl))]\n",
    "fig,axes=plt.subplots(2,3,figsize=(14,7),sharex=True)\n",
    "cm={'Delta corr (risky-risky)':'delta_avg_corr_risky','Delta corr (bonos-equity)':'delta_avg_corr_bonds_hedge','Delta corr (HYG-equity)':'delta_avg_corr_hyg_equity'}\n",
    "pm={'df (colas, ?)':'df_used','vol_mult':'vol_mult','tail_mult':'tail_mult'}\n",
    "for ax,(ttl,col) in zip(axes[0],cm.items()):\n",
    "    if col in _lever_tbl.columns:\n",
    "        y=pd.to_numeric(_lever_tbl[col],errors='coerce').to_numpy(); ax.bar(x,y,color=['#6c757d' if 'BASE' in s else '#0d6efd' for s in x]); lim=max(0.02,float(np.nanmax(np.abs(y)))*1.15) if np.isfinite(y).any() else 0.05; ax.axhline(0,color='black',lw=1); ax.set_ylim(-lim,lim)\n",
    "    else: ax.text(0.5,0.5,f'Falta columna: {col}',ha='center',va='center')\n",
    "    ax.set_title(ttl); ax.grid(True,axis='y',alpha=0.25)\n",
    "for ax,(ttl,col) in zip(axes[1],pm.items()):\n",
    "    if col in _lever_tbl.columns:\n",
    "        y=pd.to_numeric(_lever_tbl[col],errors='coerce').to_numpy(); ax.bar(x,y,color=['#6c757d' if 'BASE' in s else '#198754' for s in x]); ax.axhline(0,color='black',lw=1)\n",
    "    else: ax.text(0.5,0.5,f'Falta columna: {col}',ha='center',va='center')\n",
    "    ax.set_title(ttl); ax.grid(True,axis='y',alpha=0.25)\n",
    "for ax in axes[1]: ax.tick_params(axis='x',rotation=20)\n",
    "fig.suptitle('Fase 5 - Palancas por escenario (Delta dependencia y parametros)', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "if globals().get('EXPORT_STRESS',False):\n",
    "    out=OUT_DIR/f'phase5_levers_barplot_6m_n{N_STRESS}.png'; fig.savefig(out,dpi=160,bbox_inches='tight'); print('Saved:',out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9196b8",
   "metadata": {},
   "source": [
    "## 6) Fase 6 - Reverse stress testing\n",
    "\n",
    "Pregunta de gestion: cual es el shock mínimo para cruzar un umbral de pérdida inaceptable.\n",
    "\n",
    "Enfoque aplicado:\n",
    "\n",
    "- baseline en régimen Estrés,\n",
    "- interpolación de intensidad `lambda in [0,1]` sobre familias de shocks,\n",
    "- búsqueda del `lambda` mínimo que rompe el umbral objetivo.\n",
    "\n",
    "Outputs: grid completo, mínimo por familia, mejor global y análisis de sensibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd41fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:20:24.017954Z",
     "iopub.status.busy": "2026-02-11T18:20:24.017444Z",
     "iopub.status.idle": "2026-02-11T18:20:39.166824Z",
     "shell.execute_reply": "2026-02-11T18:20:39.165814Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 6: Reverse stress testing (minimum shock intensity λ) ---\n",
    "\n",
    "# Settings\n",
    "N_REV = 25_000\n",
    "SEED_REV = 777\n",
    "EXPORT_REV = True\n",
    "\n",
    "# Metric and threshold\n",
    "TARGET_METRIC = \"ES5\"   # {\"ES5\",\"VaR5\",\"ES1\",\"VaR1\"}\n",
    "TARGET_MULT = 1.60      # threshold = baseline_metric * TARGET_MULT (more negative)\n",
    "TARGET_ABS = None       # e.g. -0.04 (overrides TARGET_MULT if not None)\n",
    "\n",
    "# Sensitivity multipliers for justification (uses same grid)\n",
    "SENSITIVITY_MULTS = [1.4, 1.6, 1.8]\n",
    "\n",
    "# Preconditions\n",
    "required = [\"assets\", \"histS\", \"base_Ct\", \"base_df\", \"scenario_specs\", \"w\"]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing prereqs for Phase 6: {missing}. Run Phase 5 first.\")\n",
    "\n",
    "OUT_DIR = Path(OUT_DIR) if \"OUT_DIR\" in globals() else Path(\"outputs_taller\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse helpers from Phase 5, fallback if needed\n",
    "if \"_nearest_psd_corr\" not in globals():\n",
    "    raise RuntimeError(\"Missing _nearest_psd_corr. Run Phase 5 cell first.\")\n",
    "if \"_simulate_state_from_copula\" not in globals():\n",
    "    raise RuntimeError(\"Missing _simulate_state_from_copula. Run Phase 5 cell first.\")\n",
    "if \"_shock_marginal_vol_mean\" not in globals() or \"_shock_tail_left\" not in globals():\n",
    "    raise RuntimeError(\"Missing marginal shock helpers. Run Phase 5 cell first.\")\n",
    "if \"_portfolio_metrics\" not in globals():\n",
    "    raise RuntimeError(\"Missing _portfolio_metrics. Run Phase 5 cell first.\")\n",
    "\n",
    "\n",
    "def _blend_corr(C0: np.ndarray, C1: np.ndarray, lam: float) -> np.ndarray:\n",
    "    lam = float(np.clip(lam, 0.0, 1.0))\n",
    "    C = (1.0 - lam) * np.asarray(C0, dtype=float) + lam * np.asarray(C1, dtype=float)\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    return _nearest_psd_corr(C)\n",
    "\n",
    "\n",
    "def _lin(a: float, b: float, lam: float) -> float:\n",
    "    lam = float(np.clip(lam, 0.0, 1.0))\n",
    "    return float((1.0 - lam) * a + lam * b)\n",
    "\n",
    "\n",
    "def _eval_lambda(spec: dict, lam: float, n: int, seed: int) -> dict:\n",
    "    Ct_l = _blend_corr(base_Ct, np.asarray(spec[\"Ct\"], dtype=float), lam)\n",
    "    df_l = _lin(float(base_df), float(spec[\"df\"]), lam)\n",
    "    vol_mult_l = _lin(1.0, float(spec[\"vol_mult\"]), lam)\n",
    "    tail_mult_l = _lin(1.0, float(spec[\"tail_mult\"]), lam)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    R = _simulate_state_from_copula(n, Ct_l, df_l, histS, rng)\n",
    "\n",
    "    cols = spec.get(\"apply_cols\")\n",
    "    R = _shock_marginal_vol_mean(R, vol_mult=vol_mult_l, mean_shift=0.0, cols=cols)\n",
    "    R = _shock_tail_left(R, q=0.05, tail_mult=tail_mult_l, cols=cols)\n",
    "\n",
    "    m = _portfolio_metrics(R, w)\n",
    "\n",
    "    # Add levers for reporting\n",
    "    apply_cols = spec.get(\"apply_cols\")\n",
    "    if isinstance(apply_cols, list):\n",
    "        apply_names = [assets[i] for i in apply_cols if 0 <= i < len(assets)]\n",
    "        apply_to = \",\".join(apply_names) if apply_names else \"(subset)\"\n",
    "    else:\n",
    "        apply_to = \"all\"\n",
    "\n",
    "    return {\n",
    "        \"family\": spec[\"name\"],\n",
    "        \"lambda\": float(lam),\n",
    "        \"n\": int(n),\n",
    "        \"seed\": int(seed),\n",
    "        \"df_used\": float(df_l),\n",
    "        \"vol_mult\": float(vol_mult_l),\n",
    "        \"tail_mult\": float(tail_mult_l),\n",
    "        \"apply_to\": apply_to,\n",
    "        **m,\n",
    "    }\n",
    "\n",
    "\n",
    "# Baseline metric under Estres model (λ=0)\n",
    "if \"R_base\" in globals() and isinstance(globals().get(\"R_base\"), np.ndarray):\n",
    "    base_metrics = _portfolio_metrics(R_base, w)\n",
    "else:\n",
    "    rng0 = np.random.default_rng(SEED_REV)\n",
    "    R0 = _simulate_state_from_copula(N_REV, base_Ct, float(base_df), histS, rng0)\n",
    "    base_metrics = _portfolio_metrics(R0, w)\n",
    "\n",
    "baseline_value = float(base_metrics.get(TARGET_METRIC, np.nan))\n",
    "if not np.isfinite(baseline_value):\n",
    "    raise RuntimeError(f\"Baseline metric {TARGET_METRIC} is NaN; cannot reverse-stress.\")\n",
    "\n",
    "if TARGET_ABS is not None:\n",
    "    target_value = float(TARGET_ABS)\n",
    "else:\n",
    "    target_value = float(baseline_value * TARGET_MULT)\n",
    "\n",
    "print(f\"Baseline {TARGET_METRIC} (Estres model): {baseline_value:.4%}\")\n",
    "print(f\"Target   {TARGET_METRIC}: {target_value:.4%}  (TARGET_MULT={TARGET_MULT}, TARGET_ABS={TARGET_ABS})\")\n",
    "\n",
    "\n",
    "# Grid search over λ for each Phase-5 family\n",
    "lams = np.linspace(0.0, 1.0, 21)\n",
    "rows = []\n",
    "\n",
    "for spec in scenario_specs:\n",
    "    for lam in lams:\n",
    "        rows.append(_eval_lambda(spec, float(lam), n=N_REV, seed=SEED_REV))\n",
    "\n",
    "grid = pd.DataFrame(rows)\n",
    "\n",
    "# Find minimal λ that crosses the target (for loss metrics, more negative is \"worse\")\n",
    "# We assume VaR/ES are negative in returns space.\n",
    "def _crosses(val: float, target: float) -> bool:\n",
    "    return float(val) <= float(target)\n",
    "\n",
    "# Sensitivity on threshold (no extra simulations, reuses grid)\n",
    "if TARGET_ABS is None:\n",
    "    sens_rows = []\n",
    "    for mult in SENSITIVITY_MULTS:\n",
    "        target_val = float(baseline_value * mult)\n",
    "        for fam, g in grid.groupby(\"family\", sort=False):\n",
    "            g = g.sort_values(\"lambda\")\n",
    "            metric_series = g[TARGET_METRIC]\n",
    "            idx = None\n",
    "            for i in range(len(g)):\n",
    "                v = float(metric_series.iloc[i])\n",
    "                if np.isfinite(v) and _crosses(v, target_val):\n",
    "                    idx = i\n",
    "                    break\n",
    "            sens_rows.append({\n",
    "                \"target_mult\": mult,\n",
    "                \"target_value\": target_val,\n",
    "                \"family\": fam,\n",
    "                \"min_lambda\": float(g.iloc[idx][\"lambda\"]) if idx is not None else np.nan,\n",
    "                \"metric_at_min\": float(g.iloc[idx][TARGET_METRIC]) if idx is not None else np.nan,\n",
    "                \"status\": \"CROSS\" if idx is not None else \"NO_CROSS\",\n",
    "            })\n",
    "\n",
    "    sens_tbl = pd.DataFrame(sens_rows)\n",
    "    display(sens_tbl)\n",
    "    if EXPORT_REV:\n",
    "        out_sens = OUT_DIR / f\"phase6_reverse_stress_sensitivity_1d_n{N_REV}.csv\"\n",
    "        sens_tbl.to_csv(out_sens, index=False)\n",
    "        print(\"Saved:\", out_sens)\n",
    "else:\n",
    "    print(\"[Info] TARGET_ABS set; sensitivity table uses TARGET_MULTs only when TARGET_ABS is None.\")\n",
    "\n",
    "min_rows = []\n",
    "for fam, g in grid.groupby(\"family\", sort=False):\n",
    "    g = g.sort_values(\"lambda\")\n",
    "    metric_series = g[TARGET_METRIC]\n",
    "    idx = None\n",
    "    for i in range(len(g)):\n",
    "        v = float(metric_series.iloc[i])\n",
    "        if np.isfinite(v) and _crosses(v, target_value):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx is None:\n",
    "        min_rows.append({\"family\": fam, \"lambda_min\": np.nan, \"status\": \"NO_CROSS\"})\n",
    "        continue\n",
    "\n",
    "    lam_hit = float(g[\"lambda\"].iloc[idx])\n",
    "    min_rows.append({\n",
    "        \"family\": fam,\n",
    "        \"lambda_min\": lam_hit,\n",
    "        \"status\": \"CROSS\",\n",
    "        TARGET_METRIC: float(g[TARGET_METRIC].iloc[idx]),\n",
    "        \"VaR5\": float(g[\"VaR5\"].iloc[idx]),\n",
    "        \"ES5\": float(g[\"ES5\"].iloc[idx]),\n",
    "        \"VaR1\": float(g[\"VaR1\"].iloc[idx]),\n",
    "        \"ES1\": float(g[\"ES1\"].iloc[idx]),\n",
    "        \"df_used\": float(g[\"df_used\"].iloc[idx]),\n",
    "        \"vol_mult\": float(g[\"vol_mult\"].iloc[idx]),\n",
    "        \"tail_mult\": float(g[\"tail_mult\"].iloc[idx]),\n",
    "        \"apply_to\": str(g[\"apply_to\"].iloc[idx]),\n",
    "    })\n",
    "\n",
    "min_tbl = pd.DataFrame(min_rows)\n",
    "\n",
    "# Best (minimum λ across families)\n",
    "best = min_tbl[min_tbl[\"status\"] == \"CROSS\"].copy()\n",
    "if len(best) > 0:\n",
    "    best = best.sort_values([\"lambda_min\", TARGET_METRIC], ascending=[True, True]).head(1)\n",
    "    best_family = str(best[\"family\"].iloc[0])\n",
    "    best_lambda = float(best[\"lambda_min\"].iloc[0])\n",
    "    print(f\"Best reverse-stress (min λ): {best_family} @ λ={best_lambda:.2f}\")\n",
    "else:\n",
    "    best_family, best_lambda = None, np.nan\n",
    "    print(\"No family crossed the target on this λ-grid. Consider decreasing TARGET_MULT or setting TARGET_ABS closer to baseline.\")\n",
    "\n",
    "# Display\n",
    "show_cols = [\"family\", \"lambda_min\", \"status\"]\n",
    "extra = [c for c in [TARGET_METRIC, \"ES5\", \"VaR5\", \"ES1\", \"VaR1\", \"df_used\", \"vol_mult\", \"tail_mult\", \"apply_to\"] if c in min_tbl.columns]\n",
    "show_cols = show_cols + [c for c in extra if c not in show_cols]\n",
    "\n",
    "display(min_tbl[show_cols])\n",
    "\n",
    "\n",
    "# Plot metric vs λ\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for fam, g in grid.groupby(\"family\", sort=False):\n",
    "    g = g.sort_values(\"lambda\")\n",
    "    ax.plot(g[\"lambda\"], g[TARGET_METRIC], marker=\"o\", lw=1.6, label=fam)\n",
    "\n",
    "ax.axhline(target_value, color=\"black\", ls=\"--\", lw=1, label=f\"target {TARGET_METRIC}\")\n",
    "ax.set_title(f\"Phase 6 — Reverse stress: {TARGET_METRIC} vs λ (n={N_REV})\")\n",
    "ax.set_xlabel(\"Shock intensity λ (0=baseline Estres, 1=Phase5 stress)\")\n",
    "ax.set_ylabel(TARGET_METRIC)\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(ncol=2, fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Export\n",
    "if EXPORT_REV:\n",
    "    out_grid = OUT_DIR / f\"phase6_reverse_stress_grid_1d_n{N_REV}.csv\"\n",
    "    grid.to_csv(out_grid, index=False)\n",
    "    print(\"Saved:\", out_grid)\n",
    "\n",
    "    out_min = OUT_DIR / f\"phase6_reverse_stress_minimum_1d_n{N_REV}.csv\"\n",
    "    min_tbl.to_csv(out_min, index=False)\n",
    "    print(\"Saved:\", out_min)\n",
    "\n",
    "    if globals().get(\"EXPORT_STRESS\", False):\n",
    "        out_png = OUT_DIR / f\"phase6_reverse_stress_plot_1d_n{N_REV}.png\"\n",
    "        fig.savefig(out_png, dpi=160, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out_png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fed514",
   "metadata": {},
   "source": [
    "## 7) Conclusiones teécnicas\n",
    "\n",
    "- El esquema de paneles (CORE/FULL/diagnóstico 2006) permite incluir el episodio 2008 sin imputaciones y mantener robustez del universo completo.\n",
    "- La clasificación de regímenes separa períodos de calma y estrés con señales macro consistentes (volatilidad, drawdown y credito), incluyendo el bloque 2008-2009.\n",
    "- En riesgo marginal, HYG muestra un salto fuerte de volatilidad en estrés (x2.70 vs normal), mientras que GLD no mantiene un perfil de refugio robusto en este corte.\n",
    "- En dependencia, el estado de estrés presenta mas correlación media y mayor dependencia de cola que el estado normal, lo que confirma deterioro de diversificacion.\n",
    "- El motor Monte Carlo replica bien magnitudes de riesgo base (volatilidad y cola diaria) y reproduce de forma coherente el peso relativo de cada régimen.\n",
    "- En escenarios forzados, la Crisis de Crédito 2008 es el caso mas severo en VaR/ES 99% a 6 meses.\n",
    "- En reverse stress, el umbral crítico se alcanza antes con el escenario de crédito, lo que lo posiciona como principal vulnerabilidad de la cartera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85272f01",
   "metadata": {},
   "source": [
    "## Control de entregables\n",
    "\n",
    "La celda final verifica automaticamente que existan los outputs tecnicos clave generados por el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef0467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T18:20:39.168832Z",
     "iopub.status.busy": "2026-02-11T18:20:39.168832Z",
     "iopub.status.idle": "2026-02-11T18:20:39.180111Z",
     "shell.execute_reply": "2026-02-11T18:20:39.180111Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Checklist tecnico de entregables ---\n",
    "\n",
    "out = Path(OUT_DIR)\n",
    "checks = [\n",
    "    (\"Fase 2: tabla riesgo por estado\", out / \"phase2_risk_by_state.csv\"),\n",
    "    (\"Fase 3: copula summary\", out / \"phase3_copula_summary.json\"),\n",
    "    (\"Fase 4: wealth bands 6m 10k\", out / \"phase4_wealth_bands_6m_n10000.csv\"),\n",
    "    (\"Fase 4: reproduccion regimenes\", out / \"phase4_regime_reproduction_6m_n10000.csv\"),\n",
    "    (\"Fase 4: validacion riesgo cartera\", out / \"phase4_portfolio_risk_validation_6m_n10000.csv\"),\n",
    "    (\"Fase 4: validacion dependencia\", out / \"phase4_dependence_validation_6m_n10000.csv\"),\n",
    "    (\"Fase 5: resumen escenarios estres\", out / \"phase5_stress_summary_6m_n10000.csv\"),\n",
    "    (\"Fase 5: palancas escenarios\", out / \"phase5_stress_levers_summary_6m_n10000.csv\"),\n",
    "    (\"Fase 6: reverse stress minimo\", out / \"phase6_reverse_stress_minimum_1d_n25000.csv\"),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for item, path in checks:\n",
    "    rows.append({\"item\": item, \"status\": \"OK\" if path.exists() else \"FALTA\", \"path\": str(path)})\n",
    "\n",
    "check_df = pd.DataFrame(rows)\n",
    "display(check_df)\n",
    "\n",
    "n_missing = int((check_df[\"status\"] == \"FALTA\").sum())\n",
    "if n_missing == 0:\n",
    "    print(\"Checklist tecnico: completo.\")\n",
    "else:\n",
    "    print(f\"Checklist tecnico: faltan {n_missing} outputs.\")\n",
    "\n",
    "print(\"Control de entregables: se han verificado los archivos técnicos clave en outputs_taller/.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
