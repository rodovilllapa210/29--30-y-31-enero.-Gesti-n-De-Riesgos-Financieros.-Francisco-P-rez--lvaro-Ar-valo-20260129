{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8c7813",
   "metadata": {},
   "source": [
    "# Taller - HMM, Cópulas y Stress Testing (2006-hoy)\n",
    "\n",
    "Notebook técnico para modelar cambios de régimen, dependencia y riesgo de cola en una cartera multi-activo long-only.\n",
    "\n",
    "Universo de activos: AAPL, AMZN, BAC, BRK-B, CVX, ENPH, GME, GOOGL, JNJ, JPM, MSFT, NVDA, PG, XOM, GLD, IEF, SHY, HYG.\n",
    "\n",
    "Rango temporal objetivo: desde 2006-01-01 hasta fecha disponible.\n",
    "\n",
    "Ajustes metodologicos activos: ENPH puede reconstruirse con proxy (FSLR->TAN->ICLN) y GME se controla con politica configurable (`exclude` por defecto)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751edcc",
   "metadata": {},
   "source": [
    "## Ejecución y reproducibilidad\n",
    "\n",
    "Objetivo técnico: que el notebook corra de principio a fin en cualquier equipo sin rutas absolutas.\n",
    "\n",
    "- El directorio de proyecto se detecta automáticamente.\n",
    "- Los datos se descargan desde Yahoo Finance solo si no hay cache local.\n",
    "- Se guardan CSV en `outputs_taller/` para acelerar ejecuciones futuras.\n",
    "- Se fija semilla en simulaciones para reproducibilidad.\n",
    "\n",
    "Nota de proxies (bonos/crédito):\n",
    "\n",
    "- 10Y UST -> `IEF`\n",
    "- 2Y UST -> `SHY`\n",
    "- High Yield -> `HYG`\n",
    "\n",
    "Estos proxies se usan para mantener frecuencia diaria uniforme y evitar incompatibilidades de fuentes heterogéneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b6254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:28.057419Z",
     "iopub.status.busy": "2026-02-15T21:24:28.055939Z",
     "iopub.status.idle": "2026-02-15T21:24:33.393735Z",
     "shell.execute_reply": "2026-02-15T21:24:33.391475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importaciones necesarias para el análisis de riesgos financieros\n",
    "\n",
    "# Permite usar anotaciones de tipo futuras\n",
    "from __future__ import annotations  \n",
    "\n",
    "# Librerías estándar de Python\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Librerías científicas y de análisis de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Para descargar datos financieros de Yahoo Finance\n",
    "import yfinance as yf  \n",
    "\n",
    "# Control de advertencias durante la ejecución\n",
    "import warnings\n",
    "\n",
    "# Machine Learning - Modelos Hidden Markov\n",
    "from hmmlearn.hmm import GaussianHMM  # Modelo HMM con distribuciones gaussianas\n",
    "from sklearn.preprocessing import StandardScaler  # Para estandarizar características\n",
    "\n",
    "# Funciones matemáticas y estadísticas\n",
    "from scipy.special import logsumexp  # Para cálculo estable de log-sum-exp\n",
    "\n",
    "# Visualización de datos\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Manejo de datos JSON y distribuciones estadísticas\n",
    "import json\n",
    "from scipy.stats import norm, t  # Distribuciones normal y t-Student\n",
    "\n",
    "\n",
    "def find_project_root(\n",
    "    start_dir: Path,\n",
    "    markers: tuple[str, ...],\n",
    "    max_depth: int = 8,\n",
    " ) -> Path | None:\n",
    "    \"\"\"Encuentra el directorio raíz del proyecto de forma automática.\n",
    "\n",
    "    Evitamos codificar rutas absolutas. La raíz es el primer directorio que\n",
    "    contiene cualquiera de los archivos/marcadores especificados.\n",
    "    \"\"\"\n",
    "    current = start_dir.resolve()\n",
    "    for _ in range(max_depth + 1):\n",
    "        for marker in markers:\n",
    "            if (current / marker).exists():\n",
    "                return current\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- Configuración de rutas (portable) ---\n",
    "# Marcadores que identifican el directorio raíz del proyecto\n",
    "MARKERS = (\n",
    "    \"Taller_Riesgos_HMM_Copulasv2_main.ipynb\",\n",
    "    \"Taller_Riesgos_HMM_Copulasv2.pdf\",\n",
    "    \"Taller_Riesgos_HMM_Copulasv2_analisis.md\",\n",
    " )\n",
    "\n",
    "# Override opcional (útil si se lanza Jupyter desde una carpeta diferente)\n",
    "# Ejemplo: set TALLER_DIR=\"C:\\\\...\\\\tema 6 Gestion de riesgos\"\n",
    "override = os.environ.get(\"TALLER_DIR\")\n",
    "if override:\n",
    "    PROJECT_DIR = Path(override).expanduser().resolve()\n",
    "else:\n",
    "    PROJECT_DIR = find_project_root(Path.cwd(), MARKERS) or Path.cwd().resolve()\n",
    "\n",
    "# Directorio de salida para resultados y datos cacheados\n",
    "OUT_DIR = PROJECT_DIR / \"outputs_taller\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rutas de archivos CSV para datos cacheados\n",
    "PRICES_CSV = OUT_DIR / \"prices_adj_close.csv\"  # Precios ajustados\n",
    "RETURNS_CSV = OUT_DIR / \"returns_log.csv\"      # Retornos logarítmicos\n",
    "\n",
    "# --- Universo de activos solicitado (acciones + bonos + HY + oro) ---\n",
    "# Tickers de acciones estadounidenses\n",
    "EQUITY_TICKERS = [\n",
    "    \"AAPL\",   # Apple Inc.\n",
    "    \"AMZN\",   # Amazon.com Inc.\n",
    "    \"BAC\",    # Bank of America Corp.\n",
    "    \"BRK-B\",  # Berkshire Hathaway Inc. Class B\n",
    "    \"CVX\",    # Chevron Corp.\n",
    "    \"ENPH\",   # Enphase Energy Inc.\n",
    "    \"GME\",    # GameStop Corp.\n",
    "    \"GOOGL\",  # Alphabet Inc. Class A\n",
    "    \"JNJ\",    # Johnson & Johnson\n",
    "    \"JPM\",    # JPMorgan Chase & Co.\n",
    "    \"MSFT\",   # Microsoft Corp.\n",
    "    \"NVDA\",   # NVIDIA Corp.\n",
    "    \"PG\",     # Procter & Gamble Co.\n",
    "    \"XOM\",    # Exxon Mobil Corp.\n",
    "]\n",
    "\n",
    "# Tickers de activos defensivos y de deuda\n",
    "GOLD_TICKER = \"GLD\"        # SPDR Gold Shares (ETF de oro)\n",
    "BOND_10Y_TICKER = \"IEF\"    # iShares 7-10 Year Treasury Bond ETF (proxy bonos 7-10a)\n",
    "BOND_2Y_TICKER = \"SHY\"     # iShares 1-3 Year Treasury Bond ETF (proxy bonos 1-3a)\n",
    "HY_TICKER = \"HYG\"          # iShares iBoxx $ High Yield Corporate Bond ETF (high yield)\n",
    "\n",
    "# Lista completa de todos los tickers a analizar\n",
    "TICKERS = EQUITY_TICKERS + [GOLD_TICKER, BOND_10Y_TICKER, BOND_2Y_TICKER, HY_TICKER]\n",
    "\n",
    "# --- Ajustes metodolÃ³gicos solicitados (ENPH/GME) ---\n",
    "# ENPH: usar proxy para completar historia pre-2012 y mantenerlo en panel CORE\n",
    "USE_ENPH_PROXY = True\n",
    "ENPH_PROXY_TICKER = \"FSLR\"   # Proxy preferido\n",
    "ENPH_PROXY_CANDIDATES = [\"FSLR\", \"TAN\", \"ICLN\"]  # Fallback automÃ¡tico\n",
    "ENPH_PROXY_MODE = \"splice\"   # \"splice\": proxy antes del listing real, ENPH real despuÃ©s\n",
    "\n",
    "# GME: reducir sesgo por evento meme (ene-2021)\n",
    "# Opciones: \"exclude\", \"winsorize\", \"include\"\n",
    "GME_POLICY = \"exclude\"\n",
    "GME_WINSOR_Q = 0.005\n",
    "\n",
    "# El proxy se descarga puntualmente mÃ¡s adelante solo si hace falta.\n",
    "\n",
    "# Fechas de análisis\n",
    "START = \"2006-01-01\"  # Fecha de inicio del análisis\n",
    "END = None           # None => hasta la fecha actual (hoy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32721b7",
   "metadata": {},
   "source": [
    "## 0) Descarga y preparación de datos\n",
    "\n",
    "Trabajamos con `Adj Close` para construir retornos logarítmicos diarios.\n",
    "\n",
    "Se construyen tres paneles de trabajo (CORE, FULL y diagnostico 2006). El flujo principal usa CORE para preservar 2008; ENPH puede extenderse con proxy por splice y GME se trata con politica de control de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ed68b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:33.399048Z",
     "iopub.status.busy": "2026-02-15T21:24:33.399048Z",
     "iopub.status.idle": "2026-02-15T21:24:33.494379Z",
     "shell.execute_reply": "2026-02-15T21:24:33.493352Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_prices(tickers: list[str], start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"Descarga precios históricos desde Yahoo Finance.\n",
    "    \n",
    "    Args:\n",
    "        tickers: Lista de símbolos de activos a descargar\n",
    "        start: Fecha de inicio (formato YYYY-MM-DD)\n",
    "        end: Fecha de fin (formato YYYY-MM-DD) o None para hoy\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con precios de cierre ajustados por activo\n",
    "    \"\"\"\n",
    "    # Descarga datos usando yfinance\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        auto_adjust=False,      # Mantener columnas originales (Adj Close, Close, etc.)\n",
    "        progress=False,         # No mostrar barra de progreso\n",
    "        group_by=\"column\",       # Agrupar por ticker (más fácil de procesar)\n",
    "    )\n",
    "    \n",
    "    # yfinance devuelve columnas MultiIndex cuando hay varios tickers\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Preferimos precios de cierre ajustados (Adj Close) que incluyen dividendos y splits\n",
    "        if \"Adj Close\" in data.columns.get_level_values(0):\n",
    "            prices = data[\"Adj Close\"].copy()\n",
    "        else:\n",
    "            # Fallback a Close si no hay Adj Close disponible\n",
    "            prices = data[\"Close\"].copy()\n",
    "    else:\n",
    "        # Caso especial: solo un ticker (estructura diferente)\n",
    "        prices = data[[\"Adj Close\"]].rename(columns={\"Adj Close\": tickers[0]})\n",
    "\n",
    "    # Asegurar formato correcto de fechas y orden cronológico\n",
    "    prices.index = pd.to_datetime(prices.index)\n",
    "    prices = prices.sort_index()\n",
    "    return prices\n",
    "\n",
    "\n",
    "def _normalize_cached_prices(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normaliza DataFrame cargado desde cache CSV.\n",
    "    \n",
    "    Limpia y estandariza el formato de datos cargados desde archivo.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame cargado desde CSV\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame normalizado con fechas y columnas limpias\n",
    "    \"\"\"\n",
    "    # Algunos CSV pueden incluir columnas sin nombre; asegurar columnas/índice limpios\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)      # Convertir índice a datetime\n",
    "    df = df.sort_index()                     # Ordenar por fecha cronológica\n",
    "    df.columns = [str(c).strip() for c in df.columns]  # Limpiar nombres de columnas\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_or_download_prices(\n",
    "    tickers: list[str],\n",
    "    start: str,\n",
    "    end: str,\n",
    "    cache_path: Path,\n",
    " ) -> pd.DataFrame:\n",
    "    \"\"\"Carga precios desde cache o descarga si es necesario.\n",
    "    \n",
    "    Implementa sistema de cache para evitar descargas repetidas\n",
    "    y acelerar ejecuciones posteriores.\n",
    "    \n",
    "    Args:\n",
    "        tickers: Lista de símbolos de activos\n",
    "        start: Fecha de inicio\n",
    "        end: Fecha de fin\n",
    "        cache_path: Ruta del archivo cache\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con precios de los tickers solicitados\n",
    "    \"\"\"\n",
    "    tickers_set = set(tickers)\n",
    "    \n",
    "    # Verificar si existe cache válido\n",
    "    if cache_path.exists():\n",
    "        # Cargar datos cacheados y normalizar formato\n",
    "        cached = _normalize_cached_prices(pd.read_csv(cache_path, index_col=0, parse_dates=True))\n",
    "        cached_cols = set(cached.columns)\n",
    "        \n",
    "        # Si el cache contiene al menos los tickers necesarios, reutilizarlo (subset)\n",
    "        if tickers_set.issubset(cached_cols):\n",
    "            print(f\"✓ Cargando {len(tickers)} tickers desde cache: {cache_path.name}\")\n",
    "            return cached.loc[:, tickers]\n",
    "        # Si no, refrescar cache para el universo exacto solicitado\n",
    "        \n",
    "    # Descargar datos frescos si no hay cache o está incompleto\n",
    "    print(f\"⬇ Descargando {len(tickers)} tickers desde Yahoo Finance...\")\n",
    "    prices = download_prices(tickers, start, end)\n",
    "    \n",
    "    # Guardar en cache para futuras ejecuciones\n",
    "    prices.to_csv(cache_path, encoding=\"utf-8\")\n",
    "    print(f\" Cache guardado en: {cache_path}\")\n",
    "    \n",
    "    return prices\n",
    "\n",
    "\n",
    "# Ejecutar carga/descarga de precios para el universo completo\n",
    "prices = load_or_download_prices(TICKERS, START, END, PRICES_CSV)\n",
    "\n",
    "# Mostrar últimos precios y dimensiones del dataset\n",
    "print(f\"\\n Dataset de precios:\")\n",
    "print(f\"   - Período: {prices.index.min().date()} → {prices.index.max().date()}\")\n",
    "print(f\"   - Activos: {prices.shape[1]}\")\n",
    "print(f\"   - Días de trading: {prices.shape[0]:,}\")\n",
    "\n",
    "# Mostrar últimas filas y forma del DataFrame\n",
    "prices.tail(), prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967b8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:33.501134Z",
     "iopub.status.busy": "2026-02-15T21:24:33.500099Z",
     "iopub.status.idle": "2026-02-15T21:24:41.904515Z",
     "shell.execute_reply": "2026-02-15T21:24:41.902969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construcción de paneles temporales\n",
    "# ---------------------------------\n",
    "# Panel CORE:\n",
    "#   - Incluye solo tickers con histórico previo al shock de 2008.\n",
    "#   - Se usa para HMM, copulas, simulación y escenarios.\n",
    "# Panel FULL:\n",
    "#   - Mantiene el universo completo para chequeos de robustez.\n",
    "# Panel 2006:\n",
    "#   - Sirve como diagnóstico de cobertura desde 2006.\n",
    "#\n",
    "# En el resto del notebook, `common`, `prices` y `returns` se refieren al panel CORE.\n",
    "\n",
    "# Guardar copia del universo completo descargado\n",
    "prices_full = prices.copy()  # descargado (universo completo)\n",
    "\n",
    "# Determinar primera fecha disponible por ticker (para decidir composición de paneles)\n",
    "# Resolver proxy de ENPH con fallback automÃ¡tico (FSLR -> TAN -> ICLN)\n",
    "proxy_candidates = [ENPH_PROXY_TICKER] + [p for p in ENPH_PROXY_CANDIDATES if p != ENPH_PROXY_TICKER]\n",
    "proxy_candidates = list(dict.fromkeys(proxy_candidates))\n",
    "ENPH_PROXY_TICKER_USED = None\n",
    "\n",
    "if USE_ENPH_PROXY:\n",
    "    # 1) Intentar con columnas ya presentes en cache\n",
    "    for pxy in proxy_candidates:\n",
    "        if pxy in prices_full.columns and prices_full[pxy].notna().sum() > 100:\n",
    "            ENPH_PROXY_TICKER_USED = pxy\n",
    "            break\n",
    "\n",
    "    # 2) Si no hay proxy en cache, descargar puntualmente por candidato\n",
    "    if ENPH_PROXY_TICKER_USED is None:\n",
    "        for pxy in proxy_candidates:\n",
    "            try:\n",
    "                proxy_px = download_prices([pxy], START, END)\n",
    "                if pxy in proxy_px.columns and proxy_px[pxy].notna().sum() > 100:\n",
    "                    prices_full = prices_full.join(proxy_px[[pxy]], how=\"outer\")\n",
    "                    ENPH_PROXY_TICKER_USED = pxy\n",
    "                    print(f\" Proxy {pxy} descargado para construir ENPH.\")\n",
    "                    break\n",
    "            except Exception as exc:\n",
    "                print(f\" No se pudo descargar proxy {pxy}: {exc}\")\n",
    "\n",
    "# ENPH proxy (opcional): usar proxy antes del listing real para evitar truncar CORE\n",
    "if USE_ENPH_PROXY and (\"ENPH\" in prices_full.columns) and (ENPH_PROXY_TICKER_USED is not None):\n",
    "    enph_real = prices_full[\"ENPH\"].copy()\n",
    "    enph_proxy = prices_full[ENPH_PROXY_TICKER_USED].copy()\n",
    "    first_real = enph_real.first_valid_index()\n",
    "    first_proxy = enph_proxy.first_valid_index()\n",
    "\n",
    "    if ENPH_PROXY_MODE == \"splice\" and first_proxy is not None and (first_real is None or first_proxy < first_real):\n",
    "        overlap = enph_real.dropna().index.intersection(enph_proxy.dropna().index)\n",
    "        if len(overlap) > 0 and pd.notna(enph_proxy.loc[overlap[0]]) and float(enph_proxy.loc[overlap[0]]) != 0.0:\n",
    "            anchor = overlap[0]\n",
    "            scale = float(enph_real.loc[anchor] / enph_proxy.loc[anchor])\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        proxy_scaled = enph_proxy * scale\n",
    "        enph_blended = proxy_scaled.copy()\n",
    "        if first_real is not None:\n",
    "            enph_blended.loc[first_real:] = enph_real.loc[first_real:]\n",
    "\n",
    "        prices_full[\"ENPH\"] = enph_blended\n",
    "        print(f\" ENPH proxy activo ({ENPH_PROXY_TICKER_USED}) | first_proxy={first_proxy.date()} | first_real={(first_real.date() if first_real is not None else 'N/A')}\")\n",
    "    else:\n",
    "        print(\" ENPH proxy no aplicado (proxy no mejora cobertura temporal)\")\n",
    "elif USE_ENPH_PROXY:\n",
    "    print(f\" ENPH proxy no aplicado (sin proxies disponibles: {proxy_candidates})\")\n",
    "\n",
    "# El proxy se usa solo para construir ENPH y no entra como activo adicional\n",
    "proxy_cols_drop = [pxy for pxy in proxy_candidates if pxy in prices_full.columns and pxy not in TICKERS]\n",
    "if proxy_cols_drop:\n",
    "    prices_full = prices_full.drop(columns=proxy_cols_drop)\n",
    "\n",
    "first_date = prices_full.apply(lambda s: s.first_valid_index()).dropna()\n",
    "first_date = first_date.sort_values()\n",
    "\n",
    "# Reintento puntual de descarga si faltan tickers del universo declarado\n",
    "missing_initial = sorted(set(TICKERS) - set(first_date.index))\n",
    "if missing_initial:\n",
    "    print(\" Reintentando descarga para tickers faltantes:\", missing_initial)\n",
    "    try:\n",
    "        retry_px = download_prices(missing_initial, START, END)\n",
    "        recovered = []\n",
    "        for t in missing_initial:\n",
    "            if t in retry_px.columns and retry_px[t].notna().sum() > 100:\n",
    "                prices_full[t] = retry_px[t]\n",
    "                recovered.append(t)\n",
    "        if recovered:\n",
    "            print(\" Recuperados en reintento:\", recovered)\n",
    "            first_date = prices_full.apply(lambda s: s.first_valid_index()).dropna()\n",
    "            first_date = first_date.sort_values()\n",
    "        else:\n",
    "            print(\" No se recuperaron tickers adicionales en el reintento.\")\n",
    "    except Exception as exc:\n",
    "        print(f\" Reintento de descarga fall?: {exc}\")\n",
    "\n",
    "# Preservar listas originales del universo declarado\n",
    "TICKERS_FULL = list(TICKERS)\n",
    "EQUITY_TICKERS_FULL = list(EQUITY_TICKERS)\n",
    "\n",
    "# Tickers realmente utilizables (con primera fecha v?lida en el dataset descargado)\n",
    "TICKERS_FULL_AVAILABLE = [t for t in TICKERS_FULL if t in first_date.index]\n",
    "TICKERS_WITHOUT_HISTORY = sorted(set(TICKERS_FULL) - set(TICKERS_FULL_AVAILABLE))\n",
    "if TICKERS_WITHOUT_HISTORY:\n",
    "    print(\" Tickers sin hist?rico usable en la descarga actual (se omiten en paneles):\")\n",
    "    print(TICKERS_WITHOUT_HISTORY)\n",
    "\n",
    "# --- Definición de criterios temporales para paneles ---\n",
    "CORE_CUTOFF = pd.Timestamp(\"2008-09-01\")      # Antes del shock de Lehman (aprox.)\n",
    "LONG2006_CUTOFF = pd.Timestamp(\"2006-01-03\")  # Primeros días de trading de 2006\n",
    "\n",
    "# Panel CORE: tickers con datos disponibles antes de la crisis de 2008\n",
    "# Incluye HYG porque empez? en 2007, antes del colapso de Lehman\n",
    "TICKERS_CORE = [t for t in TICKERS_FULL_AVAILABLE if first_date.loc[t] <= CORE_CUTOFF]\n",
    "\n",
    "# Panel 2006: tickers con datos desde principios de 2006\n",
    "# No necesariamente incluye HYG (que empez? m?s tarde)\n",
    "TICKERS_2006 = [t for t in TICKERS_FULL_AVAILABLE if first_date.loc[t] <= LONG2006_CUTOFF]\n",
    "\n",
    "\n",
    "def build_panel(px: pd.DataFrame, tickers: list[str], start: str | None = None, end: str | None = None):\n",
    "    \"\"\"Construye panel temporal con precios y retornos logar?tmicos.\"\"\"\n",
    "    tickers_eff = [t for t in tickers if t in px.columns]\n",
    "    if len(tickers_eff) == 0:\n",
    "        return pd.DataFrame(index=px.index), pd.DataFrame()\n",
    "\n",
    "    p = px.loc[:, tickers_eff].copy()\n",
    "    p = p.dropna(axis=1, how=\"all\")\n",
    "    if p.shape[1] == 0:\n",
    "        return pd.DataFrame(index=px.index), pd.DataFrame()\n",
    "\n",
    "    if start is not None:\n",
    "        p = p.loc[pd.to_datetime(start):]\n",
    "    if end is not None:\n",
    "        p = p.loc[:pd.to_datetime(end)]\n",
    "\n",
    "    common = p.dropna(how=\"any\")\n",
    "    if common.empty:\n",
    "        return common, pd.DataFrame(index=common.index)\n",
    "\n",
    "    returns = np.log(common / common.shift(1)).dropna()\n",
    "    return common, returns\n",
    "\n",
    "\n",
    "# Construir los tres paneles temporales\n",
    "print(\" Construyendo paneles temporales...\")\n",
    "common_core, returns_core = build_panel(prices_full, TICKERS_CORE, start=START, end=END)\n",
    "common_full, returns_full = build_panel(prices_full, TICKERS_FULL, start=START, end=END)\n",
    "common_2006, returns_2006 = build_panel(prices_full, TICKERS_2006, start=START, end=END)\n",
    "\n",
    "\n",
    "def _panel_summary(name: str, tickers: list[str], common: pd.DataFrame, returns: pd.DataFrame):\n",
    "    \"\"\"Muestra resumen estad?stico de un panel temporal.\"\"\"\n",
    "    if len(tickers) == 0:\n",
    "        print(f\"\\n[{name}] (vac?o)\")\n",
    "        return\n",
    "\n",
    "    tickers_hist = [t for t in tickers if t in first_date.index]\n",
    "    missing_hist = sorted(set(tickers) - set(tickers_hist))\n",
    "    if missing_hist:\n",
    "        print(f\"\\n[{name}] tickers sin hist?rico v?lido: {missing_hist}\")\n",
    "\n",
    "    if len(tickers_hist) == 0:\n",
    "        print(f\"[{name}] sin tickers con cobertura ?til\")\n",
    "        return\n",
    "\n",
    "    limiter = first_date.loc[tickers_hist].idxmax()\n",
    "    limiter_date = first_date.loc[tickers_hist].max()\n",
    "\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\" tickers solicitados: {len(tickers)} | utilizables: {len(tickers_hist)}\")\n",
    "    if common.empty:\n",
    "        print(\" range (prices):  N/A -> N/A  rows: 0\")\n",
    "        print(\" range (returns): N/A -> N/A  rows: 0\")\n",
    "    else:\n",
    "        print(f\" range (prices):  {common.index.min().date()} -> {common.index.max().date()}  rows: {len(common):,}\")\n",
    "        if returns.empty:\n",
    "            print(\" range (returns): N/A -> N/A  rows: 0\")\n",
    "        else:\n",
    "            print(f\" range (returns): {returns.index.min().date()} -> {returns.index.max().date()}  rows: {len(returns):,}\")\n",
    "    print(f\" limiting ticker (?ltimo en arrancar): {limiter}  first_date={limiter_date.date()}\")\n",
    "\n",
    "# Mostrar resúmenes de los tres paneles\n",
    "_panel_summary(\"CORE (<=2008)\", TICKERS_CORE, common_core, returns_core)\n",
    "_panel_summary(\"FULL (universo completo)\", TICKERS_FULL, common_full, returns_full)\n",
    "_panel_summary(\"2006 (diagnóstico)\", TICKERS_2006, common_2006, returns_2006)\n",
    "\n",
    "# Identificar y mostrar tickers excluidos del panel CORE\n",
    "excluded = sorted(set(TICKERS_FULL_AVAILABLE) - set(TICKERS_CORE))\n",
    "if excluded:\n",
    "    print(\"\\n Tickers EXCLUIDOS del CORE (no exist?an antes de 2008 o no tienen hist?rico suficiente):\")\n",
    "    print(excluded)\n",
    "if TICKERS_WITHOUT_HISTORY:\n",
    "    print(\"\\n Tickers FUERA de paneles por falta de hist?rico descargado:\")\n",
    "    print(TICKERS_WITHOUT_HISTORY)\n",
    "\n",
    "# --- Selección del panel principal para el resto del análisis (CORE) ---\n",
    "# A partir de aquí, todas las variables principales se refieren al panel CORE\n",
    "common = common_core\n",
    "prices = common_core\n",
    "returns = returns_core\n",
    "\n",
    "# Tratamiento de GME para reducir sesgo en HMM/cÃ³pulas/escenarios\n",
    "if \"GME\" in returns.columns:\n",
    "    if GME_POLICY == \"exclude\":\n",
    "        common = common.drop(columns=[\"GME\"])\n",
    "        prices = prices.drop(columns=[\"GME\"])\n",
    "        returns = returns.drop(columns=[\"GME\"])\n",
    "        print(\" GME excluido del panel CORE por evento idiosincrÃ¡tico extremo (meme squeeze)\")\n",
    "    elif GME_POLICY == \"winsorize\":\n",
    "        q = float(np.clip(GME_WINSOR_Q, 1e-4, 0.10))\n",
    "        lo = float(returns[\"GME\"].quantile(q))\n",
    "        hi = float(returns[\"GME\"].quantile(1.0 - q))\n",
    "        returns.loc[:, \"GME\"] = returns[\"GME\"].clip(lower=lo, upper=hi)\n",
    "        print(f\" GME winsorizado en q={q:.4f} (clip [{lo:.2%}, {hi:.2%}])\")\n",
    "    else:\n",
    "        print(\" GME incluido sin ajuste (GME_POLICY='include').\")\n",
    "\n",
    "# Actualizar listas de tickers para evitar errores en celdas posteriores\n",
    "TICKERS = list(common.columns)\n",
    "EQUITY_TICKERS = [t for t in EQUITY_TICKERS_FULL if t in TICKERS]\n",
    "\n",
    "print(f\"\\n Panel principal seleccionado: CORE con {len(TICKERS)} activos\")\n",
    "print(f\" Período de análisis: {returns.index.min().date()} → {returns.index.max().date()}\")\n",
    "\n",
    "# Sistema de cache para retornos del panel CORE\n",
    "rebuild_returns_cache = True\n",
    "if RETURNS_CSV.exists():\n",
    "    try:\n",
    "        cached_r = pd.read_csv(RETURNS_CSV, index_col=0, parse_dates=True)\n",
    "        cached_cols = [str(c).strip() for c in cached_r.columns]\n",
    "        rebuild_returns_cache = set(cached_cols) != set(returns.columns)\n",
    "    except Exception:\n",
    "        rebuild_returns_cache = True\n",
    "\n",
    "if rebuild_returns_cache:\n",
    "    print(f\" Guardando cache de retornos CORE en: {RETURNS_CSV.name}\")\n",
    "    returns.to_csv(RETURNS_CSV, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(f\"✓ Reutilizando cache de retornos CORE: {RETURNS_CSV.name}\")\n",
    "\n",
    "# Cache opcional del universo completo para análisis de robustez\n",
    "RETURNS_FULL_CSV = OUT_DIR / \"returns_log_full_universe.csv\"\n",
    "if not RETURNS_FULL_CSV.exists():\n",
    "    print(f\" Guardando cache de retornos FULL en: {RETURNS_FULL_CSV.name}\")\n",
    "    returns_full.to_csv(RETURNS_FULL_CSV, encoding=\"utf-8\")\n",
    "\n",
    "# Estadísticas descriptivas de los retornos del panel principal\n",
    "print(f\"\\n Estadísticas descriptivas - Retornos diarios del panel CORE:\")\n",
    "returns.describe().T[[\"mean\", \"std\", \"min\", \"max\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7618d",
   "metadata": {},
   "source": [
    "### Paneles de trabajo: CORE vs FULL\n",
    "\n",
    "Para cubrir bien los bloques de crisis y mantener robustez del universo:\n",
    "\n",
    "- `prices/common/returns` = panel **CORE** (activos con histórico util para crisis 2008). Este panel alimenta HMM, copulas, Monte Carlo y stress.\n",
    "- `prices_full/common_full/returns_full` = universo completo (robustez/apendice).\n",
    "- `common_2006/returns_2006` = diagnóstico de cobertura temporal desde 2006.\n",
    "\n",
    "Criterio metodológico: no se fuerzan series inexistentes; unica excepcion controlada es ENPH via splice con proxy sectorial para preservar cobertura temporal del panel CORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f12cb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:41.910552Z",
     "iopub.status.busy": "2026-02-15T21:24:41.909566Z",
     "iopub.status.idle": "2026-02-15T21:24:42.045147Z",
     "shell.execute_reply": "2026-02-15T21:24:42.043354Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Validación de datos y chequeos de integridad (versión rápida) ---\n",
    "print(\"\\n=== Validación de datos (precios/retornos) ===\")\n",
    "\n",
    "# 1) Dimensiones básicas y rango temporal\n",
    "print(f\"Precios:  {prices.shape[0]:,} filas x {prices.shape[1]} activos\")\n",
    "print(f\"Retornos: {returns.shape[0]:,} filas x {returns.shape[1]} activos\")\n",
    "print(f\"Rango de retornos: {returns.index.min().date()} -> {returns.index.max().date()}\")\n",
    "\n",
    "# 2) Integridad del índice temporal\n",
    "if not returns.index.is_monotonic_increasing:\n",
    "    raise ValueError(\"El índice de retornos no es creciente (monotónico)\")\n",
    "if returns.index.has_duplicates:\n",
    "    dup = returns.index[returns.index.duplicated()].unique()[:10]\n",
    "    raise ValueError(f\"El índice de retornos tiene duplicados (ej. {list(dup)})\")\n",
    "\n",
    "# 3) Verificación de valores faltantes e infinitos\n",
    "nan_prices = int(prices.isna().sum().sum())\n",
    "nan_returns = int(returns.isna().sum().sum())\n",
    "inf_returns = int(np.isinf(returns.to_numpy()).sum())\n",
    "print(f\"NaNs en precios:  {nan_prices}\")\n",
    "print(f\"NaNs en retornos: {nan_returns}\")\n",
    "print(f\"Infinitos en retornos:  {inf_returns}\")\n",
    "if nan_returns > 0 or inf_returns > 0:\n",
    "    raise ValueError(\"Los retornos contienen NaN/Inf después de la limpieza. Verificar paso de descarga/alineación.\")\n",
    "\n",
    "# 4) Verificación de precios no positivos (no debería ocurrir para ETFs)\n",
    "nonpos = (prices <= 0).sum().sum()\n",
    "print(f\"Precios no positivos: {int(nonpos)}\")\n",
    "if nonpos > 0:\n",
    "    raise ValueError(\"Se encontraron precios no positivos. Los datos están corruptos o la serie ajustada es inconsistente.\")\n",
    "\n",
    "# 5) Diagnóstico de cobertura: ¿qué tickers limitan la fecha de inicio común?\n",
    "coverage = pd.DataFrame({\n",
    "    \"first_date\": prices.apply(lambda s: s.first_valid_index()),\n",
    "    \"last_date\": prices.apply(lambda s: s.last_valid_index()),\n",
    "})\n",
    "coverage[\"n_obs\"] = prices.notna().sum(axis=0)\n",
    "coverage = coverage.sort_values(\"first_date\")\n",
    "display(coverage)\n",
    "print(\"\\nActivos con fecha de inicio más temprana (estos tienden a reducir el rango común porque empezaron más tarde):\")\n",
    "display(coverage.head(5))\n",
    "\n",
    "# 6) Análisis de valores extremos en retornos diarios (informativo)\n",
    "abs_r = returns.abs()\n",
    "max_abs = abs_r.max().sort_values(ascending=False)\n",
    "display(max_abs.to_frame(\"max_abs_daily_return\"))\n",
    "\n",
    "# Identificar movimientos diarios inusualmente grandes por activo\n",
    "# (movimientos >20% diarios son raros en ETFs, aunque algunos productos pueden superar este umbral)\n",
    "threshold = 0.20\n",
    "flagged = (abs_r > threshold).sum().sort_values(ascending=False)\n",
    "flagged = flagged[flagged > 0]\n",
    "print(f\"\\nConteo(|r|>{threshold:.0%}) por activo (solo >0 mostrados):\")\n",
    "display(flagged.to_frame(\"n_days\"))\n",
    "\n",
    "# Mostrar los 10 días con mayores movimientos absolutos en todo el universo\n",
    "top_moves = (abs_r.stack().sort_values(ascending=False).head(10)).reset_index()\n",
    "top_moves.columns = [\"date\", \"ticker\", \"abs_return\"]\n",
    "top_moves[\"return\"] = returns.stack().reindex(pd.MultiIndex.from_frame(top_moves[[\"date\",\"ticker\"]])).to_numpy()\n",
    "display(top_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec2e52",
   "metadata": {},
   "source": [
    "## 1. Dimensiones y Cobertura Temporal\n",
    "\n",
    "### **Dataset Robusto**\n",
    "- **Precios**: 4,743 filas × 17 activos\n",
    "- **Retornos**: 4,742 filas × 17 activos  \n",
    "- **Periodo**: 2007-04-12 → 2026-02-13 (~19 años de datos)\n",
    "\n",
    "### **Universo de Activos**\n",
    "El panel CORE incluye 17 activos con cobertura completa desde antes de la crisis de 2008, proporcionando:\n",
    "- **Episodio de crisis 2008** completo para análisis de estrés\n",
    "- **Diversificación sectorial** adecuada (equity, bonos, oro, high yield)\n",
    "- **Frecuencia diaria** consistente sin gaps temporales\n",
    "\n",
    "## 2. Integridad Estructural\n",
    "\n",
    "- **Cronología creciente**: Sin fechas desordenadas\n",
    "- **Sin duplicados**: Cada fecha única garantizada\n",
    "- **Continuidad**: Serie temporal completa sin saltos\n",
    "\n",
    "### ** Calidad de Datos**\n",
    "- **Cero valores faltantes**: Dataset completo sin NaNs\n",
    "- **Sin valores infinitos**: Retornos finitos y estables\n",
    "- **Precios positivos**: Todos los precios > 0 (consistente con ETFs)\n",
    "\n",
    "## 3. Análisis de Cobertura por Activo\n",
    "\n",
    "### **Observaciones de Cobertura**\n",
    "- **ENPH excluido**: No cumple criterio CORE (post-2008)\n",
    "- **Panel optimizado**: 17/18 activos originales mantienen cobertura pre-crisis\n",
    "\n",
    "## 4. Análisis de Valores Extremos\n",
    "\n",
    "### **Volatilidad por Activo (Top 5)**\n",
    "1. **GME**: 91.63% (máximo histórico - meme stock phenomenon)\n",
    "2. **NVDA**: 36.71% (high volatility tech)\n",
    "3. **BAC**: 34.21% (financial crisis sensitivity)\n",
    "4. **AMZN**: 23.86% (tech volatility)\n",
    "5. **JPM**: 23.23% (banking sector)\n",
    "\n",
    "### **Eventos Extremos (>20% diario)**\n",
    "- **GME**: 42 días (fenómeno meme stocks 2021)\n",
    "- **BAC**: 18 días (crisis financiera 2008)\n",
    "- **NVDA**: 5 días (volatilidad tech)\n",
    "- **JPM**: 4 días (sector bancario)\n",
    "\n",
    "### **Interpretación de Extremos**\n",
    "- **GME outlier**: Caso especial (meme stocks) que no representa riesgo sistémico\n",
    "- **BAC/NVDA**: Volatilidad esperada para sectores de alto riesgo\n",
    "- **Diversificación funcional**: Extremos concentrados en activos específicos\n",
    "\n",
    "## 5. Eventos Más Significativos (Top 10)\n",
    "\n",
    "### **Crisis de Meme Stocks (2021)**\n",
    "- **2021-02-02**: GME -91.63% (colapso post-squeeze)\n",
    "- **2021-01-27**: GME +85.37% (peak squeeze)\n",
    "- **2021-02-24**: GME +71.26% (recuperación parcial)\n",
    "\n",
    "### **Crisis Financiera 2008**\n",
    "- **2008-10-13**: HYG +11.57% (recuperación high yield)\n",
    "- **2008-09-29**: HYG -8.44% (peak crisis)\n",
    "\n",
    "### **Patrones Observados**\n",
    "- **Concentración temporal**: Eventos extremos en períodos de crisis\n",
    "- **Recuperación rápida**: Muchos extremos seguidos de reversión\n",
    "- **Sector específico**: Cada crisis afecta diferentes segmentos\n",
    "\n",
    "## 6. Calidad para Modelado Avanzado\n",
    "\n",
    "### ** Adecuado para HMM**\n",
    "- **Regímenes claros**: Episodios de crisis/normal bien definidos\n",
    "- **Transiciones suaves**: Datos suficientes para modelar cambios de estado\n",
    "- **Longitud temporal**: 19 años proporcionan estadísticas robustas\n",
    "\n",
    "### ** Adecuado para Cópulas**\n",
    "- **Dependencia temporal**: Suficientes datos para estimar correlaciones\n",
    "- **Eventos extremos**: Colas bien representadas para modelado tail risk\n",
    "- **Multi-activo**: 17 activos permiten matriz de dependencia estable\n",
    "\n",
    "### ** Adecuado para Stress Testing**\n",
    "- **Crisis históricas**: 2008, 2020, 2021 como referencia\n",
    "- **Volatilidad realista**: Rango completo desde normal hasta extremo\n",
    "- **Recuperaciones**: Datos completos de ciclos de mercado\n",
    "\n",
    "## 7. Recomendaciones para Análisis\n",
    "\n",
    "### **Tratamiento de Outliers**\n",
    "- **GME**: Considerar análisis separado o exclusión para riesgo sistémico\n",
    "- **BAC/NVDA**: Mantener como representativos de riesgo sectorial\n",
    "- **Threshold 20%**: Adecuado para identificar eventos significativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a677945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:42.052587Z",
     "iopub.status.busy": "2026-02-15T21:24:42.051340Z",
     "iopub.status.idle": "2026-02-15T21:24:42.388738Z",
     "shell.execute_reply": "2026-02-15T21:24:42.386721Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Validación de datos y chequeos de integridad (versión avanzada, rápida y compacta) ---\n",
    "\n",
    "print(\"\\n=== Validación de datos (avanzada) ===\")\n",
    "\n",
    "# A) Análisis de gaps calendario (informativo: los festivos son esperados)\n",
    "idx = returns.index\n",
    "bday_index = pd.bdate_range(idx.min(), idx.max())\n",
    "missing_bdays = bday_index.difference(idx)\n",
    "print(f\"Días laborables faltantes en el índice de retornos (informativo): {len(missing_bdays)}\")\n",
    "if len(missing_bdays) > 0:\n",
    "    print(\"Primeros días laborables faltantes (generalmente festivos):\", [d.date().isoformat() for d in missing_bdays[:10]])\n",
    "\n",
    "# Identificar gaps grandes (>=7 días calendario) que podrían indicar problemas\n",
    "gap_days = idx.to_series().diff().dt.days.dropna()\n",
    "large_gaps = gap_days[gap_days >= 7]\n",
    "print(f\"Gaps >=7 días calendario en el índice de retornos: {len(large_gaps)}\")\n",
    "if len(large_gaps) > 0:\n",
    "    print(\"Primeros gaps grandes:\")\n",
    "    display(large_gaps.head(10).to_frame(\"gap_days\"))\n",
    "\n",
    "# B) Consistencia: recalcular retornos desde precios alineados y comparar\n",
    "common_prices = prices.loc[idx.min() : idx.max()].copy()\n",
    "common_prices = common_prices.reindex(idx)\n",
    "recalc = np.log(common_prices / common_prices.shift(1)).dropna()\n",
    "recalc = recalc.reindex_like(returns)\n",
    "diff = (returns - recalc).abs()\n",
    "max_diff = float(np.nanmax(diff.to_numpy()))\n",
    "print(f\"Máxima diferencia |retornos - recalculados|: {max_diff:.3e}\")\n",
    "if not np.isfinite(max_diff):\n",
    "    raise ValueError(\"Diferencia no finita al recalcular retornos\")\n",
    "if max_diff > 1e-10:\n",
    "    warnings.warn(\"Los retornos difieren de la recomputación más que la tolerancia. Verificar alineación/series de precios.\")\n",
    "\n",
    "# C) Precios estancados/planos: contar cierres repetidos (redondeados para evitar ruido de punto flotante)\n",
    "flat_counts = {}\n",
    "for col in prices.columns:\n",
    "    s = prices[col].dropna().round(6)\n",
    "    flat_counts[col] = int(s.diff().eq(0).sum()) if len(s) else 0\n",
    "flat = pd.Series(flat_counts, name=\"n_flat_days\").sort_values(ascending=False)\n",
    "print(\"\\nDías con precios planos (top 10):\")\n",
    "display(flat.head(10).to_frame())\n",
    "\n",
    "# D) Detección robusta de outliers via z-score MAD (vectorizado)\n",
    "# MAD = Median Absolute Deviation, más robusto que desviación estándar\n",
    "med = returns.median(axis=0)\n",
    "mad = (returns.sub(med, axis=1)).abs().median(axis=0)\n",
    "scale = 1.4826 * mad  # Factor para consistencia con desviación estándar en distribución normal\n",
    "scale = scale.replace(0, np.nan)\n",
    "z = returns.sub(med, axis=1).div(scale, axis=1)\n",
    "z_abs = z.abs()\n",
    "z_max = z_abs.max(axis=0).sort_values(ascending=False)\n",
    "print(\"\\nZ-score robusto máximo por activo:\")\n",
    "display(z_max.to_frame(\"robust_z_max\"))\n",
    "\n",
    "# Identificar eventos extremos (z-score > 8)\n",
    "z_thr = 8.0\n",
    "z_hits = (z_abs > z_thr).sum(axis=0).sort_values(ascending=False)\n",
    "z_hits = z_hits[z_hits > 0]\n",
    "print(f\"Conteo(|z_robusto|>{z_thr}) por activo (solo >0 mostrados):\")\n",
    "display(z_hits.to_frame(\"n_days\"))\n",
    "\n",
    "if len(z_hits) > 0:\n",
    "    top_events = z_abs.stack().sort_values(ascending=False).head(10).reset_index()\n",
    "    top_events.columns = [\"date\", \"ticker\", \"abs_robust_z\"]\n",
    "    top_events[\"return\"] = returns.stack().reindex(pd.MultiIndex.from_frame(top_events[[\"date\", \"ticker\"]])).to_numpy()\n",
    "    print(\"Top eventos outliers robustos:\")\n",
    "    display(top_events)\n",
    "\n",
    "# E) Resumen estadístico de distribuciones\n",
    "stats = pd.DataFrame({\n",
    "    \"mean_daily\": returns.mean(),\n",
    "    \"vol_daily\": returns.std(ddof=1),\n",
    "    \"skew\": returns.skew(),\n",
    "    \"kurtosis\": returns.kurtosis(),\n",
    "    \"p01\": returns.quantile(0.01),\n",
    "    \"p99\": returns.quantile(0.99),\n",
    "})\n",
    "stats[\"vol_ann\"] = stats[\"vol_daily\"] * np.sqrt(252)  # Annualización (252 días laborables)\n",
    "print(\"\\nEstadísticas resumen (ordenadas por volatilidad anualizada):\")\n",
    "display(stats.sort_values(\"vol_ann\", ascending=False))\n",
    "\n",
    "# Identificar activos con volatilidades inusualmente bajas o altas\n",
    "low_vol = stats[stats[\"vol_ann\"] < 0.03]\n",
    "high_vol = stats[stats[\"vol_ann\"] > 1.50]\n",
    "if len(low_vol) > 0:\n",
    "    warnings.warn(f\"Activos con volatilidad anual muy baja (<3%): {list(low_vol.index)}\")\n",
    "if len(high_vol) > 0:\n",
    "    warnings.warn(f\"Activos con volatilidad anual muy alta (>150%): {list(high_vol.index)}\")\n",
    "\n",
    "# F) Verificaciones PSD para correlación/covarianza (crucial para modelos de riesgo)\n",
    "# PSD = Positive Semi-Definite, requisito matemático para matrices de correlación/covarianza\n",
    "corr = returns.corr()\n",
    "eig_corr = np.linalg.eigvalsh(corr.to_numpy())\n",
    "min_eig_corr = float(eig_corr.min())\n",
    "print(f\"\\nValor propio mínimo (correlación): {min_eig_corr:.3e}\")\n",
    "if min_eig_corr < -1e-8:\n",
    "    warnings.warn(\"La matriz de correlación no es PSD. Considerar shrinkage/proyección PSD.\")\n",
    "\n",
    "cov = returns.cov()\n",
    "eig_cov = np.linalg.eigvalsh(cov.to_numpy())\n",
    "min_eig_cov = float(eig_cov.min())\n",
    "print(f\"Valor propio mínimo (covarianza):  {min_eig_cov:.3e}\")\n",
    "if min_eig_cov < -1e-12:\n",
    "    warnings.warn(\"La matriz de covarianza tiene valores propios negativos. Considerar shrinkage/proyección PSD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e3286",
   "metadata": {},
   "source": [
    "# Análisis Avanzado de Calidad de Datos y Validación Estadística\n",
    "\n",
    "## 1. Análisis de Integridad Calendario\n",
    "\n",
    "### * Gaps Esperados y Normales**\n",
    "- **Días laborables faltantes**: 175 días (esperados: festivos y fines de semana)\n",
    "- **Gaps ≥7 días calendario**: 0 gaps grandes (sin anomalías temporales)\n",
    "- **Primeros gaps**: Corresponden a festivos estadounidenses (Memorial Day, Independence Day, etc.)\n",
    "- **Sin anomalías**: No hay gaps inesperados que indiquen problemas de datos\n",
    "- **Cobertura completa**: Serie temporal continua sin interrupciones problemáticas\n",
    "\n",
    "## 2. Consistencia Matemática de Retornos\n",
    "\n",
    "### **Validación de Integridad**\n",
    "- **Precios alineados**: Todos los precios corresponden exactamente a retornos\n",
    "- **Cálculos consistentes**: Sin errores de redondeo o sincronización\n",
    "- **Base matemática sólida**: Datos listos para cálculos cuantitativos precisos\n",
    "\n",
    "## 3. Análisis de Precios Planos\n",
    "\n",
    "### **Activos con Mayor Estabilidad (Top 5)**\n",
    "1. **SHY**: 418 días planos (bonos cortos - baja volatilidad esperada)\n",
    "2. **HYG**: 72 días planos (high yield - moderada estabilidad)\n",
    "3. **BAC**: 68 días planos (banco - sorprendentemente estable algunos días)\n",
    "4. **GME**: 55 días planos (incluyendo períodos de baja actividad)\n",
    "5. **IEF**: 39 días planos (bonos intermedios - comportamiento esperado)\n",
    "\n",
    "### **Interpretación de Estabilidad**\n",
    "- **SHY líder**: 418/4742 días (8.8%) - normal para activo de bajo riesgo\n",
    "- **Activos de renta fija**: SHY, IEF, HYG muestran estabilidad esperada\n",
    "- **Equity variable**: GME, BAC con días planos consistentes con su naturaleza\n",
    "\n",
    "## 4. Detección Robusta de Outliers (MAD Z-Score)\n",
    "\n",
    "### **Activos con Mayor Z-Score Robusto (Top 5)**\n",
    "1. **GME**: 36.96 (extremo - meme stock phenomenon)\n",
    "2. **HYG**: 35.51 (crisis de crédito 2008)\n",
    "3. **BAC**: 21.37 (crisis financiera)\n",
    "4. **CVX**: 20.50 (energía - shocks sectoriales)\n",
    "5. **BRK-B**: 18.81 (conglomerado - eventos de mercado)\n",
    "\n",
    "### **Eventos Extremos (Z > 8.0)**\n",
    "- **HYG**: 50 eventos (high yield - crisis de crédito)\n",
    "- **BAC**: 45 eventos (sector bancario volátil)\n",
    "- **GME**: 43 eventos (meme stocks)\n",
    "- **JPM**: 35 eventos (banco investment)\n",
    "- **SHY**: 23 eventos (sorprendente para bonos cortos)\n",
    "\n",
    "### **Top 10 Eventos Outliers Robustos**\n",
    "1. **2021-02-02**: GME -91.63% (z=36.96) - colapso meme stock\n",
    "2. **2008-10-13**: HYG +11.57% (z=35.51) - recuperación crisis crédito\n",
    "3. **2021-01-27**: GME +85.37% (z=34.44) - peak meme stock\n",
    "4. **2021-02-24**: GME +71.26% (z=28.75) - recuperación parcial\n",
    "5. **2008-09-29**: HYG -8.44% (z=26.08) - peak crisis financiera\n",
    "\n",
    "### **Análisis de Outliers**\n",
    "- **GME dominante**: 4/10 eventos extremos (fenómeno único)\n",
    "- **Crisis 2008**: HYG y BAC representan crisis financiera\n",
    "- **Diversidad temporal**: Eventos concentrados en períodos de crisis específicos\n",
    "\n",
    "## 5. Estadísticas Descriptivas por Volatilidad\n",
    "\n",
    "### **Ranking por Volatilidad Anualizada**\n",
    "\n",
    "#### **Alta Volatilidad (>30%)**\n",
    "1. **GME**: 84.03% (extremo - meme stocks)\n",
    "2. **NVDA**: 49.29% (tech high growth)\n",
    "3. **BAC**: 47.90% (financial sector)\n",
    "\n",
    "#### **Volatilidad Media (15-30%)**\n",
    "4. **AMZN**: 37.45% (tech e-commerce)\n",
    "5. **JPM**: 37.34% (banking)\n",
    "6. **AAPL**: 31.38% (tech blue chip)\n",
    "7. **GOOGL**: 29.68% (tech giant)\n",
    "8. **CVX**: 29.00% (energy)\n",
    "9. **MSFT**: 27.94% (tech software)\n",
    "\n",
    "#### **Baja Volatilidad (<20%)**\n",
    "10. **XOM**: 27.05% (energy integrated)\n",
    "11. **BRK-B**: 21.79% (conglomerate)\n",
    "12. **PG**: 18.51% (consumer staples)\n",
    "13. **GLD**: 17.86% (gold)\n",
    "14. **JNJ**: 17.69% (healthcare)\n",
    "15. **HYG**: 10.95% (high yield bonds)\n",
    "16. **IEF**: 6.99% (intermediate bonds)\n",
    "17. **SHY**: 1.52% (short bonds -  muy baja)\n",
    "\n",
    "### **Análisis de Kurtosis (Colas Gruesas)**\n",
    "- **GME**: 66.20 (extremo - eventos raros frecuentes)\n",
    "- **BAC**: 24.58 (crisis financieras)\n",
    "- **HYG**: 39.19 (crisis de crédito)\n",
    "- **NVDA**: 9.23 (tech volatility)\n",
    "- **IEF**: 2.56 (normal para bonos)\n",
    "\n",
    "### **Sesgo (Skewness)**\n",
    "- **GME**: +0.88 (sesgo positivo - ganancias extremas)\n",
    "- **AMZN**: +0.67 (sesgo positivo)\n",
    "- **BAC**: -0.32 (sesgo negativo - pérdidas extremas)\n",
    "- **NVDA**: -0.33 (sesgo negativo)\n",
    "\n",
    "## 6. Validación Matemática de Matrices\n",
    "\n",
    "### ** Propiedades PSD Confirmadas**\n",
    "- **Correlación**: Valor propio mínimo = 1.384e-01 (> 0)\n",
    "- **Covarianza**: Valor propio mínimo = 3.684e-07 (> 0)\n",
    "\n",
    "## 7. Alertas y Observaciones Especiales\n",
    "\n",
    "### ** Volatilidad Anormalmente Baja**\n",
    "- **SHY**: 1.52% anualizada (<3% umbral)\n",
    "- **Justificación**: Activo de renta fija ultra-corto plazo\n",
    "- **Impacto**: No problemático para diversificación\n",
    "\n",
    "### ** Sin Problemas Detectados**\n",
    "- **Sin warnings graves**: Todas las validaciones pasan\n",
    "- **Consistencia numérica**: Cero diferencias en recomputación\n",
    "- **Integridad matemática**: Matrices PSD confirmadas\n",
    "\n",
    "## 8. Recomendaciones para Modelado\n",
    "\n",
    "### **Tratamiento de Activos Especiales**\n",
    "- **GME**: Considerar análisis separado o tratamiento especial\n",
    "- **SHY**: Mantener como activo de baja volatilidad (no es problema)\n",
    "- **HYG**: Incluir como representante de riesgo de crédito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1dc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:24:42.393808Z",
     "iopub.status.busy": "2026-02-15T21:24:42.392793Z",
     "iopub.status.idle": "2026-02-15T21:25:07.756450Z",
     "shell.execute_reply": "2026-02-15T21:25:07.754887Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- HMM: Entrenamiento y probabilidades de régimen (2 estados) ---\n",
    "\n",
    "# 1) Construir características aptas para regímenes (observables en tiempo real)\n",
    "#    Usando solo información del universo: cesta equity + tipos + HY + oro\n",
    "#    (Core-only: conjunto de features parsimonioso y estable.)\n",
    "\n",
    "idx = returns.index\n",
    "px = common.reindex(idx)  # precios alineados (rango común)\n",
    "\n",
    "# --- Señales principales (nuevo universo) ---\n",
    "# --- Proxy Equity SOLO para HMM: excluye GME (riesgo meme/outlier) ---\n",
    "EQUITY_TICKERS_HMM = [t for t in EQUITY_TICKERS if t != \"GME\" and t in returns.columns]\n",
    "\n",
    "if len(EQUITY_TICKERS_HMM) < 5:\n",
    "    raise ValueError(f\"EQUITY_TICKERS_HMM demasiado pequeño: {EQUITY_TICKERS_HMM}\")\n",
    "\n",
    "# Retorno diario de la cesta equity (promedio simple)\n",
    "r_equity = returns[EQUITY_TICKERS_HMM].mean(axis=1)\n",
    "\n",
    "# Retornos individuales de activos clave (con fallback robusto)\n",
    "def _pick_available(preferred: str, alternatives: list[str], available_cols: list[str], label: str) -> str:\n",
    "    candidates = [preferred] + [c for c in alternatives if c != preferred]\n",
    "    for c in candidates:\n",
    "        if c in available_cols:\n",
    "            return c\n",
    "    raise KeyError(f\"No hay ticker disponible para {label}. Preferidos: {candidates}. Disponibles: {available_cols}\")\n",
    "\n",
    "cols_avail = list(returns.columns)\n",
    "HY_TICKER_USED = _pick_available(HY_TICKER, [\"JNK\", \"LQD\", \"USHY\"], cols_avail, \"High Yield\")\n",
    "BOND_10Y_TICKER_USED = _pick_available(BOND_10Y_TICKER, [\"TLT\", \"VGIT\"], cols_avail, \"Bonos 7-10a\")\n",
    "BOND_2Y_TICKER_USED = _pick_available(BOND_2Y_TICKER, [\"BIL\", \"SGOV\", \"VGSH\"], cols_avail, \"Bonos 1-3a\")\n",
    "GOLD_TICKER_USED = _pick_available(GOLD_TICKER, [\"IAU\", \"SGOL\"], cols_avail, \"Oro\")\n",
    "\n",
    "print(\n",
    "    f\" Tickers HMM usados | HY={HY_TICKER_USED}, 10Y={BOND_10Y_TICKER_USED}, \"\n",
    "    f\"2Y={BOND_2Y_TICKER_USED}, GOLD={GOLD_TICKER_USED}\"\n",
    ")\n",
    "\n",
    "r_hy = returns[HY_TICKER_USED]         # High Yield (cr?dito de alto riesgo)\n",
    "r_10y = returns[BOND_10Y_TICKER_USED]  # Bonos 7-10a (tipos medios)\n",
    "r_2y = returns[BOND_2Y_TICKER_USED]    # Bonos 1-3a (tipos cortos)\n",
    "r_gld = returns[GOLD_TICKER_USED]      # Oro (refugio)\n",
    "\n",
    "# Volatilidad realizada (21 días hábiles ~ 1 mes)\n",
    "vol_equity_21 = r_equity.rolling(21).std(ddof=1)\n",
    "vol_hy_21 = r_hy.rolling(21).std(ddof=1)\n",
    "\n",
    "# Drawdown rolling para cesta equity (máximo rolling 1año)\n",
    "px_equity = px[EQUITY_TICKERS_HMM].mean(axis=1)  # consistencia: mismo universo ex GME para todo el HMM\n",
    "log_equity = np.log(px_equity)\n",
    "roll_max_1y = log_equity.rolling(252, min_periods=60).max()\n",
    "dd_equity = log_equity - roll_max_1y  # <= 0, más negativo = drawdown más profundo\n",
    "\n",
    "# Proxy de riesgo de crédito: High Yield vs Bonos 10a\n",
    "r_credit = r_hy - r_10y\n",
    "\n",
    "# Construir DataFrame de features para el modelo HMM\n",
    "features = pd.DataFrame(\n",
    "    {\n",
    "        \"r_equity\": r_equity,        # Retorno cesta equity\n",
    "        \"r_hy\": r_hy,                # Retorno High Yield\n",
    "        \"r_10y\": r_10y,              # Retorno Bonos 10a\n",
    "        \"r_2y\": r_2y,                # Retorno Bonos 2a\n",
    "        \"r_gld\": r_gld,              # Retorno Oro\n",
    "        \"r_credit\": r_credit,        # Spread crédito (HY - 10a)\n",
    "        \"vol_equity_21\": vol_equity_21,  # Volatilidad equity 21d\n",
    "        \"vol_hy_21\": vol_hy_21,      # Volatilidad HY 21d\n",
    "        \"dd_equity\": dd_equity,      # Drawdown equity\n",
    "    },\n",
    "    index=idx,\n",
    ")\n",
    "\n",
    "# Eliminar filas con datos faltantes (requerido para HMM)\n",
    "features = features.dropna()\n",
    "\n",
    "# Mantener slice de retornos alineado con features para resúmenes posteriores\n",
    "returns_hmm = returns.loc[features.index]\n",
    "\n",
    "# Convertir features a numpy array para el modelo\n",
    "X = features.to_numpy(dtype=float)\n",
    "\n",
    "# ---- Configuración de modo tiempo real / sin look-ahead ----\n",
    "REALTIME_MODE = True   # False para ajuste full-sample (más rápido, pero usa look-ahead)\n",
    "MIN_TRAIN = 252        # Ventana 1año: permite detectar 2008 con panel CORE (arranca en 2007-04)\n",
    "REFIT_EVERY = 63       # Frecuencia de reajuste (días, ~3 meses)\n",
    "\n",
    "\n",
    "def _make_hmm() -> GaussianHMM:\n",
    "    \"\"\"Crea instancia de HMM con configuración estándar.\n",
    "    \n",
    "    Returns:\n",
    "        GaussianHMM configurado con 2 estados y priors de persistencia\n",
    "    \"\"\"\n",
    "    # Prior de transición fuerte en diagonal (persistencia de regímenes)\n",
    "    trans_prior = np.array([[200.0, 1.0], [1.0, 200.0]])\n",
    "    return GaussianHMM(\n",
    "        n_components=2,              # 2 estados: Normal vs Crisis\n",
    "        covariance_type=\"diag\",      # Covarianza diagonal (más robusto)\n",
    "        n_iter=500,                  # Máximo iteraciones EM\n",
    "        tol=1e-4,                    # Tolerancia de convergencia\n",
    "        random_state=42,             # Semilla para reproducibilidad\n",
    "        transmat_prior=trans_prior,  # Prior de matriz de transición\n",
    "    )\n",
    "\n",
    "\n",
    "def _forward_filter_probs(model: GaussianHMM, X_scaled: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calcula probabilidades forward-filtered (sin información futura).\n",
    "    \n",
    "    Implementa el algoritmo forward para obtener P(S_t | X_{1:t})\n",
    "    sin usar información futura (crucial para tiempo real).\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo HMM entrenado\n",
    "        X_scaled: Features estandarizadas\n",
    "        \n",
    "    Returns:\n",
    "        Array de probabilidades de estado (T, K)\n",
    "    \"\"\"\n",
    "    # Calcular log-verosimilitud de observaciones\n",
    "    logB = model._compute_log_likelihood(X_scaled)\n",
    "    log_start = np.log(model.startprob_)\n",
    "    log_trans = np.log(model.transmat_)\n",
    "    T, K = logB.shape\n",
    "    \n",
    "    # Inicializar forward probabilities\n",
    "    log_alpha = np.zeros((T, K))\n",
    "    log_alpha[0] = log_start + logB[0]\n",
    "    log_alpha[0] -= logsumexp(log_alpha[0])\n",
    "    \n",
    "    # Recursión forward\n",
    "    for t in range(1, T):\n",
    "        log_alpha[t] = logB[t] + logsumexp(log_alpha[t - 1][:, None] + log_trans, axis=0)\n",
    "        log_alpha[t] -= logsumexp(log_alpha[t])\n",
    "    \n",
    "    return np.exp(log_alpha)\n",
    "\n",
    "\n",
    "if not REALTIME_MODE:\n",
    "    # --- MODO FULL-SAMPLE (más rápido, pero usa look-ahead) ---\n",
    "    print(\" Modo FULL-SAMPLE: ajuste con toda la historia disponible\")\n",
    "    \n",
    "    # Estandarizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Ajustar modelo HMM\n",
    "    hmm = _make_hmm()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        hmm.fit(X_scaled)\n",
    "\n",
    "    # Decodificar estados y calcular probabilidades posteriores\n",
    "    state_seq_raw = hmm.predict(X_scaled)\n",
    "    state_prob_raw = hmm.predict_proba(X_scaled)  # shape: (T, n_states)\n",
    "\n",
    "    # Determinar qué estado es \"crisis\" (heurística): mayor vol + peor retorno equity\n",
    "    tmp = pd.DataFrame({\n",
    "        \"equity_mean\": r_equity.loc[features.index].groupby(state_seq_raw).mean(),\n",
    "        \"equity_vol\": r_equity.loc[features.index].groupby(state_seq_raw).std(ddof=1),\n",
    "        \"vol_equity_21\": features[\"vol_equity_21\"].groupby(state_seq_raw).mean(),\n",
    "        \"dd_equity\": features[\"dd_equity\"].groupby(state_seq_raw).mean(),\n",
    "        \"credit_mean\": features[\"r_credit\"].groupby(state_seq_raw).mean(),\n",
    "    })\n",
    "    display(tmp)\n",
    "\n",
    "    # Criterio: Crisis = menor equity_mean; desempates: mayor vol_equity_21, dd_equity más negativo, peor credit_mean\n",
    "    crisis_state = tmp.sort_values([\"equity_mean\", \"vol_equity_21\", \"dd_equity\", \"credit_mean\"], ascending=[True, False, True, True]).index[0]\n",
    "    normal_state = [s for s in range(hmm.n_components) if s != crisis_state][0]\n",
    "    print(f\"Estado crisis = {crisis_state}; Estado normal = {normal_state}\")\n",
    "\n",
    "    # Construir DataFrame de régimen con nomenclatura consistente\n",
    "    regime = pd.DataFrame({\n",
    "        \"state_raw\": state_seq_raw,\n",
    "        \"p_state0\": state_prob_raw[:, 0],\n",
    "        \"p_state1\": state_prob_raw[:, 1],\n",
    "    }, index=features.index)\n",
    "    regime[\"p_crisis\"] = regime[\"p_state0\"] if crisis_state == 0 else regime[\"p_state1\"]\n",
    "    regime[\"state\"] = np.where(regime[\"state_raw\"] == crisis_state, \"crisis\", \"normal\")\n",
    "\n",
    "else:\n",
    "    # --- MODO TIEMPO REAL (walk-forward, sin look-ahead) ---\n",
    "    print(\" Modo TIEMPO REAL: ajuste walk-forward sin información futura\")\n",
    "    \n",
    "    T = len(X)\n",
    "    if T < MIN_TRAIN:\n",
    "        raise ValueError(f\"Datos insuficientes para REALTIME_MODE: se necesitan >= {MIN_TRAIN}, hay {T}\")\n",
    "\n",
    "    # Inicializar arrays para almacenar resultados\n",
    "    state_prob_raw = np.full((T, 2), np.nan)\n",
    "    state_seq_raw = np.full(T, np.nan)\n",
    "    p_crisis = np.full(T, np.nan)\n",
    "    state_label = np.full(T, None, dtype=object)\n",
    "\n",
    "    block_rows = []\n",
    "    hmm_last = None\n",
    "    crisis_state_last = None\n",
    "    tmp_last = None\n",
    "\n",
    "    # Loop walk-forward: ajustar cada REFIT_EVERY días\n",
    "    for t0 in range(MIN_TRAIN, T, REFIT_EVERY):\n",
    "        t1 = min(t0 + REFIT_EVERY, T)\n",
    "\n",
    "        # Ajustar scaler con datos hasta t0 (sin look-ahead)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X[:t0])\n",
    "\n",
    "        # Ajustar HMM con datos históricos hasta t0\n",
    "        hmm = _make_hmm()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            hmm.fit(X_train)\n",
    "\n",
    "        # Determinar estado crisis usando solo datos pasados (hasta t0)\n",
    "        state_seq_train = hmm.predict(X_train)\n",
    "        tmp = pd.DataFrame({\n",
    "            \"equity_mean\": r_equity.loc[features.index].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"equity_vol\": r_equity.loc[features.index].iloc[:t0].groupby(state_seq_train).std(ddof=1),\n",
    "            \"vol_equity_21\": features[\"vol_equity_21\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"dd_equity\": features[\"dd_equity\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "            \"credit_mean\": features[\"r_credit\"].iloc[:t0].groupby(state_seq_train).mean(),\n",
    "        })\n",
    "        crisis_state = tmp.sort_values([\"equity_mean\", \"vol_equity_21\", \"dd_equity\", \"credit_mean\"], ascending=[True, False, True, True]).index[0]\n",
    "\n",
    "        # Filtrar probabilidades forward en el siguiente bloque (sin info futura)\n",
    "        X_block = scaler.transform(X[t0:t1])\n",
    "        prob_block = _forward_filter_probs(hmm, X_block)\n",
    "\n",
    "        # Almacenar resultados del bloque\n",
    "        state_prob_raw[t0:t1] = prob_block\n",
    "        state_seq_raw[t0:t1] = prob_block.argmax(axis=1)\n",
    "        p_crisis[t0:t1] = prob_block[:, crisis_state]\n",
    "        state_label[t0:t1] = np.where(prob_block.argmax(axis=1) == crisis_state, \"crisis\", \"normal\")\n",
    "\n",
    "        # Guardar información del bloque para diagnóstico\n",
    "        block_rows.append({\n",
    "            \"start\": features.index[t0],\n",
    "            \"end\": features.index[t1 - 1],\n",
    "            \"train_end\": features.index[t0 - 1],\n",
    "            \"crisis_state\": int(crisis_state),\n",
    "            \"n_train\": int(t0),\n",
    "            \"n_block\": int(t1 - t0),\n",
    "        })\n",
    "\n",
    "        hmm_last = hmm\n",
    "        crisis_state_last = crisis_state\n",
    "        tmp_last = tmp\n",
    "\n",
    "    # Construir DataFrame final de régimen\n",
    "    regime = pd.DataFrame({\n",
    "        \"state_raw\": state_seq_raw,\n",
    "        \"p_state0\": state_prob_raw[:, 0],\n",
    "        \"p_state1\": state_prob_raw[:, 1],\n",
    "        \"p_crisis\": p_crisis,\n",
    "        \"state\": state_label,\n",
    "    }, index=features.index)\n",
    "\n",
    "    # Mostrar información de bloques para diagnóstico\n",
    "    block_info = pd.DataFrame(block_rows)\n",
    "    display(block_info.tail())\n",
    "\n",
    "    if hmm_last is None:\n",
    "        raise RuntimeError(\"REALTIME_MODE falló al ajustar cualquier bloque.\")\n",
    "\n",
    "    # Usar último modelo ajustado como referencia\n",
    "    hmm = hmm_last\n",
    "    crisis_state = crisis_state_last\n",
    "    normal_state = [s for s in range(hmm.n_components) if s != crisis_state][0]\n",
    "    print(f\"(Tiempo real) Último ajuste: estado crisis = {crisis_state}; Estado normal = {normal_state}\")\n",
    "    display(tmp_last)\n",
    "\n",
    "# Mostrar matriz de transición del último ajuste\n",
    "trans = pd.DataFrame(hmm.transmat_, columns=[\"to_state0\", \"to_state1\"], index=[\"from_state0\", \"from_state1\"])\n",
    "print(\"\\n Matriz de transición (estados crudos):\")\n",
    "display(trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e9f81",
   "metadata": {},
   "source": [
    "# Análisis de Entrenamiento HMM y Detección de Regímenes de Mercado\n",
    "\n",
    "### **Advertencias de Convergencia**\n",
    "- **Frecuencia**: ~30 advertencias de no convergencia\n",
    "- **Causa**: Normal en walk-forward con datos cambiantes\n",
    "- **Impacto**: Mínimo - modelo funcional con convergencia parcial\n",
    "- **Solución**: Modelo robusto despite warnings\n",
    "\n",
    "## Caracterización de Regímenes\n",
    "\n",
    "### **Regímenes Identificados (Último Ajuste)**\n",
    "\n",
    "#### **Estado 0: Normal**\n",
    "- **Retorno equity**: +0.1095% diario (positivo)\n",
    "- **Volatilidad equity**: 0.7444% diaria (baja)\n",
    "- **Volatilidad 21d**: 0.7742% (controlada)\n",
    "- **Drawdown**: -1.53% (superficial)\n",
    "- **Spread crédito**: +0.0259% (favorable)\n",
    "\n",
    "#### **Estado 1: Crisis**\n",
    "- **Retorno equity**: -0.0443% diario (negativo)\n",
    "- **Volatilidad equity**: 2.0808% diaria (alta)\n",
    "- **Volatilidad 21d**: 1.8102% (elevada)\n",
    "- **Drawdown**: -12.55% (profundo)\n",
    "- **Spread crédito**: -0.0295% (adverso)\n",
    "\n",
    "### **Interpretación de Regímenes**\n",
    "- **Normal**: Crecimiento positivo, volatilidad controlada, crédito favorable\n",
    "- **Crisis**: Pérdidas, volatilidad elevada, stress de crédito\n",
    "\n",
    "## Evolución Temporal de Regímenes\n",
    "\n",
    "### **Información de Bloques Finales**\n",
    "\n",
    "| Bloque | Período | Fin Entrenamiento | Estado Crisis | Datos Entrenamiento | Días Bloque |\n",
    "|--------|---------|-------------------|---------------|---------------------|-------------|\n",
    "| 66 | 2025-01-14 → 2025-04-14 | 2025-01-13 | 1 | 4410 | 63 |\n",
    "| 67 | 2025-04-15 → 2025-07-16 | 2025-04-14 | 0 | 4473 | 63 |\n",
    "| 68 | 2025-07-17 → 2025-10-14 | 2025-07-16 | 1 | 4536 | 63 |\n",
    "| 69 | 2025-10-15 → 2026-01-14 | 2025-10-14 | 1 | 4599 | 63 |\n",
    "| 70 | 2026-01-15 → 2026-02-13 | 2026-01-14 | 1 | 4662 | 21 |\n",
    "\n",
    "## Validación del Modelo\n",
    "\n",
    "### ** Consistencia Económica**\n",
    "- **Normal**: Retornos positivos, volatilidad baja\n",
    "- **Crisis**: Retornos negativos, volatilidad alta\n",
    "- **Crédito**: Spread favorable en normal, adverso en crisis\n",
    "- **Drawdown**: Superficial en normal, profundo en crisis\n",
    "\n",
    "### ** Consideraciones Técnicas**\n",
    "- **Convergencia parcial**: Warnings esperados en walk-forward\n",
    "- **Estabilidad**: Modelo mantiene consistencia temporal\n",
    "- **Reproducibilidad**: Semilla fija garantiza resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7e21d",
   "metadata": {},
   "source": [
    "# Decisión de modelado (HMM)\n",
    "\n",
    "- Modelo parsimonioso: se evita sobrecargar el HMM con demasiadas features.\n",
    "- La politica sobre GME es configurable (`exclude`/`winsorize`/`include`); por defecto se excluye del panel CORE para reducir sesgo idiosincratico extremo.\n",
    "- En modo real-time se usa `MIN_TRAIN=252` para cubrir 2008 con el panel CORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d2a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:07.762709Z",
     "iopub.status.busy": "2026-02-15T21:25:07.761394Z",
     "iopub.status.idle": "2026-02-15T21:25:07.832479Z",
     "shell.execute_reply": "2026-02-15T21:25:07.830983Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Diagnósticos: cómo se ven los regímenes (medias/volatilidades) ---\n",
    "\n",
    "def summarize_by_state(returns_df: pd.DataFrame, state_labels: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Resume estadísticas de retornos por estado del HMM.\n",
    "    \n",
    "    Calcula métricas clave (media, volatilidad, percentiles) para cada activo\n",
    "    separando por régimen (normal vs crisis).\n",
    "    \n",
    "    Args:\n",
    "        returns_df: DataFrame de retornos\n",
    "        state_labels: Serie con etiquetas de estado por fecha\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame multi-índice con estadísticas por (estado, ticker)\n",
    "    \"\"\"\n",
    "    # Alinear etiquetas de estado con índice de retornos\n",
    "    state_labels = pd.Series(state_labels, index=returns_df.index).reindex(returns_df.index)\n",
    "    out = []\n",
    "    \n",
    "    # Calcular estadísticas para cada estado\n",
    "    for st in [\"normal\", \"crisis\"]:\n",
    "        mask = state_labels == st\n",
    "        chunk = returns_df.loc[mask]\n",
    "        \n",
    "        # Estadísticas descriptivas para el estado actual\n",
    "        stats = pd.DataFrame(\n",
    "            {\n",
    "                \"mean_daily\": chunk.mean(),                    # Media diaria\n",
    "                \"vol_daily\": chunk.std(ddof=1),               # Volatilidad diaria\n",
    "                \"mean_ann\": chunk.mean() * 252,               # Media anualizada (252 días)\n",
    "                \"vol_ann\": chunk.std(ddof=1) * np.sqrt(252),  # Volatilidad anualizada\n",
    "                \"p01\": chunk.quantile(0.01),                  # Percentil 1% (cola izquierda)\n",
    "                \"p99\": chunk.quantile(0.99),                  # Percentil 99% (cola derecha)\n",
    "            }\n",
    "        )\n",
    "        stats[\"state\"] = st\n",
    "        stats = stats.rename_axis(\"ticker\").reset_index()\n",
    "        out.append(stats)\n",
    "    \n",
    "    # Combinar resultados y estructurar como multi-índice\n",
    "    res = pd.concat(out, ignore_index=True)\n",
    "    return res.set_index([\"state\", \"ticker\"]).sort_index()\n",
    "\n",
    "\n",
    "# Calcular resumen estadístico por régimen para todos los activos\n",
    "print(\" Estadísticas de retornos por régimen:\")\n",
    "summary = summarize_by_state(returns_hmm, regime[\"state\"])\n",
    "display(summary)\n",
    "\n",
    "# Tabla comparativa: diferencias crisis vs normal (medias y volatilidades anualizadas)\n",
    "print(\"\\n Diferencias Crisis - Normal (impacto del régimen):\")\n",
    "cmp = (summary.xs(\"crisis\")[[\"mean_ann\", \"vol_ann\"]] - summary.xs(\"normal\")[[\"mean_ann\", \"vol_ann\"]])\n",
    "cmp = cmp.rename(columns={\"mean_ann\": \"delta_mean_ann\", \"vol_ann\": \"delta_vol_ann\"}).sort_values(\"delta_vol_ann\", ascending=False)\n",
    "display(cmp)\n",
    "\n",
    "# Interpretación de resultados clave\n",
    "print(\"\\n Interpretación de diferencias:\")\n",
    "print(\"• delta_mean_ann: Cambio en retorno esperado anualizado (Crisis - Normal)\")\n",
    "print(\"• delta_vol_ann: Cambio en volatilidad anualizada (Crisis - Normal)\")\n",
    "print(\"• Valores positivos en delta_vol_ann indican mayor riesgo en crisis\")\n",
    "print(\"• Valores negativos en delta_mean_ann indican peores retornos en crisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a1804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:07.838092Z",
     "iopub.status.busy": "2026-02-15T21:25:07.837109Z",
     "iopub.status.idle": "2026-02-15T21:25:09.126140Z",
     "shell.execute_reply": "2026-02-15T21:25:09.124601Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Gráficos (estilo informe): probabilidad + banda de régimen + cesta de acciones ---\n",
    "\n",
    "# Configurar tema visual para los gráficos\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ---- Controles de presentación (macro + cobertura 2008) ----\n",
    "# Se prioriza una señal macro estable que conserve el bloque 2008-2009 en el panel CORE.\n",
    "# Se prueba una cuadrícula de sensibilidad y se elige la primera configuración que detecte crisis en 2008.\n",
    "PARAM_CANDIDATES = [\n",
    "    {\"name\": \"macro_strict\", \"smooth_span\": 60, \"enter\": 0.70, \"exit\": 0.30, \"min_days\": 60, \"max_gap\": 10},\n",
    "    {\"name\": \"balanced\", \"smooth_span\": 30, \"enter\": 0.60, \"exit\": 0.40, \"min_days\": 30, \"max_gap\": 8},\n",
    "    {\"name\": \"sensitive_2008\", \"smooth_span\": 20, \"enter\": 0.55, \"exit\": 0.35, \"min_days\": 20, \"max_gap\": 6},\n",
    "    {\"name\": \"very_sensitive\", \"smooth_span\": 15, \"enter\": 0.50, \"exit\": 0.35, \"min_days\": 15, \"max_gap\": 5},\n",
    "]\n",
    "# Fechas objetivo para capturar la crisis de 2008\n",
    "TARGET_2008_START = pd.Timestamp(\"2008-09-01\")\n",
    "TARGET_2008_END = pd.Timestamp(\"2009-06-30\")\n",
    "\n",
    "def _runs_to_segments(flags: pd.Series) -> list[tuple[pd.Timestamp, pd.Timestamp]]:\n",
    "    \"\"\"Convierte series de banderas booleanas en segmentos de tiempo continuos.\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana que indica períodos de crisis\n",
    "        \n",
    "    Returns:\n",
    "        Lista de tuplas (inicio, fin) para cada segmento continuo de True\n",
    "    \"\"\"\n",
    "    flags = flags.astype(bool)\n",
    "    if len(flags) == 0:\n",
    "        return []\n",
    "    x = flags.to_numpy(copy=True)\n",
    "    idx = flags.index\n",
    "    # Detectar cambios de estado\n",
    "    chg = np.flatnonzero(x[1:] != x[:-1]) + 1\n",
    "    starts = np.r_[0, chg]\n",
    "    ends = np.r_[chg, len(x)]\n",
    "    segs = []\n",
    "    # Extraer solo segmentos donde el estado es True (crisis)\n",
    "    for s, e in zip(starts, ends):\n",
    "        if x[s]:\n",
    "            segs.append((idx[s], idx[e - 1]))\n",
    "    return segs\n",
    "\n",
    "def clean_regime_flags(flags: pd.Series, *, min_true: int, max_false_gap: int) -> pd.Series:\n",
    "    \"\"\"Post-procesa regímenes booleanos para evitar gráficos tipo 'código de barras'.\n",
    "    \n",
    "    Elimina ruido en las señales de crisis mediante dos reglas:\n",
    "    - Rellena gaps cortos de False dentro de corridas de True (<= max_false_gap)\n",
    "    - Elimina corridas cortas de True (< min_true)\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana original de regímenes\n",
    "        min_true: Duración mínima en días para considerar una crisis válida\n",
    "        max_false_gap: Máximo número de días falsos permitidos dentro de una crisis\n",
    "        \n",
    "    Returns:\n",
    "        Serie booleana limpia y estabilizada\n",
    "    \"\"\"\n",
    "    flags = flags.astype(bool).copy()\n",
    "    x = flags.to_numpy(copy=True)\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return flags\n",
    "\n",
    "    def _rle(arr: np.ndarray):\n",
    "        \"\"\"Run-length encoding: codifica secuencias consecutivas iguales.\"\"\"\n",
    "        chg = np.flatnonzero(arr[1:] != arr[:-1]) + 1\n",
    "        starts = np.r_[0, chg]\n",
    "        ends = np.r_[chg, n]\n",
    "        vals = arr[starts]\n",
    "        lens = ends - starts\n",
    "        return starts, ends, vals, lens\n",
    "\n",
    "    # 1) Rellenar gaps cortos de False rodeados de True\n",
    "    starts, ends, vals, lens = _rle(x)\n",
    "    for s, e, v, L in zip(starts, ends, vals, lens):\n",
    "        if (not v) and (L <= max_false_gap):\n",
    "            left_true = (s > 0) and x[s - 1]\n",
    "            right_true = (e < n) and x[e]\n",
    "            if left_true and right_true:\n",
    "                x[s:e] = True\n",
    "\n",
    "    # 2) Eliminar corridas cortas de True\n",
    "    starts, ends, vals, lens = _rle(x)\n",
    "    for s, e, v, L in zip(starts, ends, vals, lens):\n",
    "        if v and (L < min_true):\n",
    "            x[s:e] = False\n",
    "\n",
    "    return pd.Series(x, index=flags.index, name=flags.name)\n",
    "\n",
    "def hysteresis_flags(p: pd.Series, *, enter: float, exit: float) -> pd.Series:\n",
    "    \"\"\"Convierte una serie de probabilidades en un régimen booleano estable usando histéresis.\n",
    "    \n",
    "    La histéresis evita cambios frecuentes de estado usando umbrales diferentes\n",
    "    para entrar y salir de crisis.\n",
    "    \n",
    "    Args:\n",
    "        p: Serie de probabilidades de crisis\n",
    "        enter: Umbral para entrar en estado de crisis\n",
    "        exit: Umbral para salir de estado de crisis (debe ser < enter)\n",
    "        \n",
    "    Returns:\n",
    "        Serie booleana indicando períodos de crisis\n",
    "    \"\"\"\n",
    "    p = p.astype(float)\n",
    "    out = np.zeros(len(p), dtype=bool)\n",
    "    in_crisis = False\n",
    "    for i, val in enumerate(p.to_numpy()):\n",
    "        if not np.isfinite(val):\n",
    "            out[i] = in_crisis\n",
    "            continue\n",
    "        # Lógica de histéresis: requiere umbral más alto para entrar, más bajo para salir\n",
    "        if (not in_crisis) and (val >= enter):\n",
    "            in_crisis = True\n",
    "        elif in_crisis and (val <= exit):\n",
    "            in_crisis = False\n",
    "        out[i] = in_crisis\n",
    "    return pd.Series(out, index=p.index, name=\"is_crisis_hysteresis\")\n",
    "\n",
    "def _build_crisis_flags(\n",
    "    p: pd.Series,\n",
    "    *,\n",
    "    smooth_span: int,\n",
    "    enter: float,\n",
    "    exit: float,\n",
    "    min_days: int,\n",
    "    max_gap: int,\n",
    ") -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Construye banderas de crisis aplicando suavizado, histéresis y limpieza.\n",
    "    \n",
    "    Pipeline completo para generar señales de crisis robustas:\n",
    "    1. Suavizado exponencial ponderado (EWMA)\n",
    "    2. Histéresis para evitar cambios frecuentes\n",
    "    3. Limpieza de ruido\n",
    "    \n",
    "    Args:\n",
    "        p: Serie de probabilidades crudas del HMM\n",
    "        smooth_span: Período de suavizado EWMA\n",
    "        enter: Umbral de entrada a crisis\n",
    "        exit: Umbral de salida de crisis\n",
    "        min_days: Duración mínima de crisis\n",
    "        max_gap: Gap máximo permitido dentro de crisis\n",
    "        \n",
    "    Returns:\n",
    "        Tupla: (probabilidades_suavizadas, banderas_crudas, banderas_limpias)\n",
    "    \"\"\"\n",
    "    # Suavizar probabilidades con media móvil exponencial ponderada\n",
    "    p_smooth = p.ewm(span=smooth_span, adjust=False).mean()\n",
    "    # Aplicar histéresis para obtener banderas iniciales\n",
    "    raw = hysteresis_flags(p_smooth, enter=enter, exit=exit)\n",
    "    # Limpiar ruido y estabilizar señales\n",
    "    clean = clean_regime_flags(raw, min_true=min_days, max_false_gap=max_gap)\n",
    "    return p_smooth, raw, clean\n",
    "\n",
    "# Búsqueda automática de configuración que capture la crisis de 2008\n",
    "selected_cfg = None\n",
    "selected_pack = None\n",
    "for cfg in PARAM_CANDIDATES:\n",
    "    p_smooth_i, raw_i, clean_i = _build_crisis_flags(\n",
    "        regime[\"p_crisis\"],\n",
    "        smooth_span=cfg[\"smooth_span\"],\n",
    "        enter=cfg[\"enter\"],\n",
    "        exit=cfg[\"exit\"],\n",
    "        min_days=cfg[\"min_days\"],\n",
    "        max_gap=cfg[\"max_gap\"],\n",
    "    )\n",
    "    # Verificar si la configuración captura el período de crisis 2008\n",
    "    window_i = clean_i.loc[(clean_i.index >= TARGET_2008_START) & (clean_i.index <= TARGET_2008_END)]\n",
    "    captures_2008 = bool(window_i.any()) if len(window_i) else False\n",
    "    if captures_2008:\n",
    "        selected_cfg = cfg\n",
    "        selected_pack = (p_smooth_i, raw_i, clean_i)\n",
    "        break\n",
    "\n",
    "# Plan B: usar la configuración más sensible si ninguna detecta 2008\n",
    "if selected_cfg is None:\n",
    "    selected_cfg = PARAM_CANDIDATES[-1]\n",
    "    selected_pack = _build_crisis_flags(\n",
    "        regime[\"p_crisis\"],\n",
    "        smooth_span=selected_cfg[\"smooth_span\"],\n",
    "        enter=selected_cfg[\"enter\"],\n",
    "        exit=selected_cfg[\"exit\"],\n",
    "        min_days=selected_cfg[\"min_days\"],\n",
    "        max_gap=selected_cfg[\"max_gap\"],\n",
    "    )\n",
    "\n",
    "# Extraer resultados de la configuración seleccionada\n",
    "p_crisis_smooth, is_crisis_raw, is_crisis = selected_pack\n",
    "\n",
    "# Exponer parámetros finales (se usan también en el gráfico)\n",
    "SMOOTH_SPAN = selected_cfg[\"smooth_span\"]\n",
    "ENTER_CRISIS = selected_cfg[\"enter\"]\n",
    "EXIT_CRISIS = selected_cfg[\"exit\"]\n",
    "MIN_CRISIS_DAYS = selected_cfg[\"min_days\"]\n",
    "MAX_FALSE_GAP = selected_cfg[\"max_gap\"]\n",
    "\n",
    "# Validación y diagnóstico de la configuración seleccionada\n",
    "window = is_crisis.loc[(is_crisis.index >= TARGET_2008_START) & (is_crisis.index <= TARGET_2008_END)]\n",
    "print(\n",
    "    f\"Configuración de sombreado de crisis: {selected_cfg['name']} \"\n",
    "    f\"(span={SMOOTH_SPAN}, enter={ENTER_CRISIS:.2f}, exit={EXIT_CRISIS:.2f}, \"\n",
    "    f\"min_days={MIN_CRISIS_DAYS}, max_gap={MAX_FALSE_GAP})\"\n",
    ")\n",
    "if len(window):\n",
    "    print(f\"Días marcados como crisis en ventana 2008: {int(window.sum())} / {len(window)}\")\n",
    "\n",
    "# ---- Diseño de figura: probabilidad (arriba) + banda delgada de régimen (medio) + cesta de acciones (abajo) ----\n",
    "fig, (ax_p, ax_band, ax_spy) = plt.subplots(\n",
    "    3, 1, figsize=(14, 8), sharex=True, gridspec_kw={\"height_ratios\": [2.2, 0.35, 2.8]}\n",
    ")\n",
    "\n",
    "# Panel superior: probabilidad de crisis\n",
    "ax_p.plot(regime.index, p_crisis_smooth, color=\"#c0392b\", linewidth=1.2, label=\"P(crisis) (EWMA)\")\n",
    "ax_p.axhline(ENTER_CRISIS, color=\"#2c3e50\", linestyle=\"--\", linewidth=1, alpha=0.7, label=\"umbral_entrada\")\n",
    "ax_p.axhline(EXIT_CRISIS, color=\"#2c3e50\", linestyle=\":\", linewidth=1, alpha=0.7, label=\"umbral_salida\")\n",
    "ax_p.set_ylim(-0.02, 1.02)\n",
    "ax_p.set_ylabel(\"Probabilidad\")\n",
    "ax_p.set_title(\"Regímenes de mercado (HMM 2 estados) — vista macro\")\n",
    "ax_p.legend(loc=\"upper right\", frameon=True)\n",
    "\n",
    "# Panel medio: banda de régimen (limpia, legible)\n",
    "x0 = mdates.date2num(is_crisis.index[0])\n",
    "x1 = mdates.date2num(is_crisis.index[-1])\n",
    "band = is_crisis.astype(int).to_numpy()[None, :]  # forma (1, T)\n",
    "cmap = ListedColormap([\"#ecf0f1\", \"#f5b7b1\"])  # normal, crisis\n",
    "ax_band.imshow(band, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, extent=[x0, x1, 0, 1])\n",
    "ax_band.set_yticks([])\n",
    "ax_band.set_ylabel(\"Régimen\")\n",
    "ax_band.set_ylim(0, 1)\n",
    "ax_band.grid(False)\n",
    "# Ocultar bordes para apariencia limpia\n",
    "for spine in ax_band.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Panel inferior: retorno logarítmico acumulado de la cesta de acciones\n",
    "basket_lr = returns_hmm[EQUITY_TICKERS_HMM].mean(axis=1).cumsum()\n",
    "ax_spy.plot(\n",
    "    basket_lr.index,\n",
    "    basket_lr.values,\n",
    "    color=\"#1f4e79\",\n",
    "    linewidth=1.2,\n",
    "    label=\"Cesta de acciones (ex GME) retorno logarítmico acumulado\",\n",
    ")\n",
    "ax_spy.set_ylabel(\"Retorno log-acumulado\")\n",
    "ax_spy.legend(loc=\"upper left\", frameon=True)\n",
    "\n",
    "# Sombreado ligero también en el panel inferior (refuerza interpretación)\n",
    "for a, b in _runs_to_segments(is_crisis):\n",
    "    ax_spy.axvspan(a, b, color=\"#f1948a\", alpha=0.15, linewidth=0)\n",
    "\n",
    "# Formato del eje X (único eje en la parte inferior)\n",
    "locator = mdates.AutoDateLocator(minticks=6, maxticks=12)\n",
    "ax_spy.xaxis.set_major_locator(locator)\n",
    "ax_spy.xaxis.set_major_formatter(mdates.ConciseDateFormatter(locator))\n",
    "# Ocultar etiquetas de fecha en paneles superiores\n",
    "ax_p.tick_params(labelbottom=False)\n",
    "ax_band.tick_params(labelbottom=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Segmentos sombreados (crisis):\", len(_runs_to_segments(is_crisis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f3aca",
   "metadata": {},
   "source": [
    "### Fase 1 - S&P 500 por régimen detectado\n",
    "\n",
    "Visual de control del modelo de estados:\n",
    "\n",
    "- Blanco = Calma\n",
    "- Azul = Crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5b396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:09.130985Z",
     "iopub.status.busy": "2026-02-15T21:25:09.130985Z",
     "iopub.status.idle": "2026-02-15T21:25:14.409293Z",
     "shell.execute_reply": "2026-02-15T21:25:14.408274Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 1: S&P 500 coloreado por regimen (Blanco=Calma, Azul=Crisis) ---\n",
    "\n",
    "spx = yf.download(\"^GSPC\", start=START, end=END, progress=False, auto_adjust=False)\n",
    "if isinstance(spx.columns, pd.MultiIndex):\n",
    "    if (\"Adj Close\", \"^GSPC\") in spx.columns:\n",
    "        spx_close = spx[(\"Adj Close\", \"^GSPC\")]\n",
    "    else:\n",
    "        spx_close = spx[(\"Close\", \"^GSPC\")]\n",
    "else:\n",
    "    spx_close = spx[\"Adj Close\"] if \"Adj Close\" in spx.columns else spx[\"Close\"]\n",
    "\n",
    "spx_close = spx_close.reindex(regime.index).ffill().dropna()\n",
    "reg_spx = is_crisis.reindex(spx_close.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "spx_calm = spx_close.where(~reg_spx)\n",
    "spx_crisis = spx_close.where(reg_spx)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "ax.set_facecolor(\"#111111\")\n",
    "fig.patch.set_facecolor(\"#111111\")\n",
    "\n",
    "# Linea base tenue + tramos coloreados por regimen\n",
    "ax.plot(spx_close.index, spx_close.values, color=\"#666666\", linewidth=0.8, alpha=0.6)\n",
    "ax.plot(spx_calm.index, spx_calm.values, color=\"white\", linewidth=1.4, label=\"Calma (blanco)\")\n",
    "ax.plot(spx_crisis.index, spx_crisis.values, color=\"#1f77b4\", linewidth=1.6, label=\"Crisis (azul)\")\n",
    "\n",
    "ax.set_title(\"S&P 500 por regimen detectado (HMM)\", color=\"white\")\n",
    "ax.set_ylabel(\"Nivel indice\", color=\"white\")\n",
    "ax.grid(alpha=0.18)\n",
    "ax.tick_params(colors=\"white\")\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_color(\"#AAAAAA\")\n",
    "\n",
    "leg = ax.legend(loc=\"upper left\", frameon=True)\n",
    "leg.get_frame().set_facecolor(\"#222222\")\n",
    "leg.get_frame().set_edgecolor(\"#666666\")\n",
    "for txt in leg.get_texts():\n",
    "    txt.set_color(\"white\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912af08e",
   "metadata": {},
   "source": [
    "## Cómo leer la tabla de drivers por bloque de crisis\n",
    "\n",
    "Los drivers son señales estadísticas (no causalidad estricta) que indican qué variables se desviaron más frente al estado normal.\n",
    "\n",
    "Para cada bloque continuo `is_crisis=True`, se calcula z-score de cada feature respecto al período normal:\n",
    "\n",
    "- `z > 0`: feature por encima de lo normal\n",
    "- `z < 0`: feature por debajo de lo normal\n",
    "- `|z|` alto: variable candidata a explicar ese episodio\n",
    "\n",
    "Ejemplos económicos:\n",
    "\n",
    "- `dd_equity` muy negativo -> drawdown profundo de renta variable\n",
    "- `vol_equity_21` alto -> fuerte inestabilidad en equity\n",
    "- `vol_hy_21` alto -> tensión de crédito\n",
    "- `r_credit` muy negativo -> entorno risk-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc18f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:14.413992Z",
     "iopub.status.busy": "2026-02-15T21:25:14.412990Z",
     "iopub.status.idle": "2026-02-15T21:25:14.601602Z",
     "shell.execute_reply": "2026-02-15T21:25:14.600055Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Diagnósticos: por qué se marca cada 'bloque de crisis' (atribución de características) ---\n",
    "\n",
    "# Hacer que las columnas de texto largo sean legibles en las visualizaciones del notebook\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "def segments_from_flags(flags: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Convierte banderas de crisis en segmentos de tiempo continuos.\n",
    "    \n",
    "    Args:\n",
    "        flags: Serie booleana donde True indica crisis\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con segmentos de crisis (inicio, fin, duración)\n",
    "    \"\"\"\n",
    "    segs = _runs_to_segments(flags)\n",
    "    if not segs:\n",
    "        return pd.DataFrame(columns=[\"start\", \"end\", \"n_days\"])\n",
    "    out = []\n",
    "    for a, b in segs:\n",
    "        idx_seg = flags.loc[a:b].index\n",
    "        out.append({\"start\": a, \"end\": b, \"n_days\": int(len(idx_seg))})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# Obtener segmentos de crisis a partir de las banderas\n",
    "seg_tbl = segments_from_flags(is_crisis)\n",
    "if seg_tbl.empty:\n",
    "    print(\"No se encontraron segmentos de crisis.\")\n",
    "else:\n",
    "    # Columnas de características a reportar (solo las principales para mantener interpretabilidad)\n",
    "    cols_preferred = [\n",
    "        \"vol_equity_21\",  # Volatilidad de acciones a 21 días\n",
    "        \"dd_equity\",      # Drawdown de acciones\n",
    "        \"r_credit\",       # Retorno de crédito\n",
    "        \"vol_hy_21\",      # Volatilidad de high yield a 21 días\n",
    "        \"r_equity\",       # Retorno de acciones\n",
    "        \"r_hy\",           # Retorno de high yield\n",
    "        \"r_10y\",          # Retorno de bonos a 10 años\n",
    "        \"r_2y\",           # Retorno de bonos a 2 años\n",
    "        \"r_gld\",          # Retorno de oro\n",
    "    ]\n",
    "    feat_cols = [c for c in cols_preferred if c in features.columns]\n",
    "    feat = features[feat_cols].copy()\n",
    "\n",
    "    # Línea base normal = todas las fechas que no están en crisis (para z-scores)\n",
    "    normal_mask = ~is_crisis.reindex(feat.index, fill_value=False)\n",
    "    mu0 = feat.loc[normal_mask].mean()  # Media en períodos normales\n",
    "    sd0 = feat.loc[normal_mask].std(ddof=1).replace(0, np.nan)  # Desviación estándar en períodos normales\n",
    "\n",
    "    rows = []\n",
    "    for _, row in seg_tbl.iterrows():\n",
    "        a, b = row[\"start\"], row[\"end\"]\n",
    "        seg = feat.loc[a:b]\n",
    "        m = seg.mean()  # Media del segmento de crisis\n",
    "        z = (m - mu0) / sd0  # Z-score respecto a períodos normales\n",
    "        z = z.replace([np.inf, -np.inf], np.nan)\n",
    "        top = z.abs().sort_values(ascending=False).head(4)  # Top 4 características más atípicas\n",
    "\n",
    "        drivers = []\n",
    "        for k in top.index:\n",
    "            if pd.notna(z[k]):\n",
    "                drivers.append((k, float(z[k])))\n",
    "        top_txt = \"; \".join([f\"{k}: z={val:+.2f}\" for k, val in drivers])\n",
    "\n",
    "        base = {\n",
    "            \"start\": a,\n",
    "            \"end\": b,\n",
    "            \"n_days\": int(row[\"n_days\"]),\n",
    "            \"p_crisis_mean\": float(p_crisis_smooth.loc[a:b].mean()),\n",
    "            \"p_crisis_max\": float(p_crisis_smooth.loc[a:b].max()),\n",
    "            \"top_drivers_vs_normal\": top_txt,\n",
    "        }\n",
    "        # Agregar los 4 principales conductores como columnas separadas\n",
    "        for i in range(4):\n",
    "            if i < len(drivers):\n",
    "                base[f\"driver_{i+1}\"] = drivers[i][0]\n",
    "                base[f\"z_{i+1}\"] = drivers[i][1]\n",
    "            else:\n",
    "                base[f\"driver_{i+1}\"] = \"\"\n",
    "                base[f\"z_{i+1}\"] = np.nan\n",
    "        rows.append(base)\n",
    "\n",
    "    diag = pd.DataFrame(rows)\n",
    "    diag[\"start\"] = diag[\"start\"].dt.date\n",
    "    diag[\"end\"] = diag[\"end\"].dt.date\n",
    "    diag = diag.sort_values(\"start\")\n",
    "\n",
    "    # NOTA: pandas .style requiere jinja2; evitamos esa dependencia renderizando HTML directamente.\n",
    "    from IPython.display import HTML, display\n",
    "\n",
    "    css = \"\"\"\n",
    "<style>\n",
    "table.dataframe td { vertical-align: top; }\n",
    "table.dataframe td, table.dataframe th { white-space: pre-wrap; }\n",
    "</style>\n",
    "\"\"\"\n",
    "    display(HTML(css + diag.to_html(index=False)))\n",
    "\n",
    "    print(\"\\nCómo leer 'top_drivers_vs_normal':\")\n",
    "    print(\"- z > 0 significa más alto que lo normal; z < 0 significa más bajo que lo normal.\")\n",
    "    print(\"- |z| grande indica que esa característica es inusual en ese segmento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc71e3",
   "metadata": {},
   "source": [
    "## Fase 2 - Riesgo marginal por estado\n",
    "\n",
    "Objetivo: medir cómo cambian las distribuciones individuales en Normal vs Estrés.\n",
    "\n",
    "Se reporta por activo y estado:\n",
    "\n",
    "- media\n",
    "- volatilidad\n",
    "- skew\n",
    "- kurtosis\n",
    "- VaR y ES (1% y 5%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddde1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:14.606347Z",
     "iopub.status.busy": "2026-02-15T21:25:14.606347Z",
     "iopub.status.idle": "2026-02-15T21:25:17.178169Z",
     "shell.execute_reply": "2026-02-15T21:25:17.177656Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 2 (figura adicional): distribuciones de retornos por estado ---\n",
    "\n",
    "# Reutilizar objetos de la Fase 2\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Función para calcular VaR (Value at Risk)\n",
    "def var_es(returns, alpha):\n",
    "    \"\"\"Calcular Value at Risk (VaR) para un conjunto de retornos.\"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return np.nan, np.nan\n",
    "    sorted_returns = np.sort(returns)\n",
    "    index = int(alpha * len(sorted_returns))\n",
    "    if index == 0:\n",
    "        return sorted_returns[0], np.nan\n",
    "    return sorted_returns[index-1], np.nan\n",
    "\n",
    "# Seleccionar algunos activos representativos (de riesgo + defensivos)\n",
    "if \"EQUITY_TICKERS\" in globals() and EQUITY_TICKERS:\n",
    "    equity_anchor = EQUITY_TICKERS[0]  # Primera acción como ancla de riesgo\n",
    "else:\n",
    "    equity_anchor = ret.columns[0] if len(ret.columns) else None\n",
    "\n",
    "hy_ticker = HY_TICKER if \"HY_TICKER\" in globals() else None      # High Yield (bonos de alto rendimiento)\n",
    "bond10_ticker = BOND_10Y_TICKER if \"BOND_10Y_TICKER\" in globals() else None  # Bonos a 10 años\n",
    "gold_ticker = GOLD_TICKER if \"GOLD_TICKER\" in globals() else None  # Oro como activo defensivo\n",
    "\n",
    "# Crear lista de candidatos para análisis\n",
    "focus_candidates = [equity_anchor, hy_ticker, bond10_ticker, gold_ticker]\n",
    "tickers_focus = [t for t in focus_candidates if t is not None and t in ret.columns]\n",
    "if not tickers_focus:\n",
    "    tickers_focus = list(ret.columns[:4])  # Si no hay candidatos, usar primeros 4 activos\n",
    "\n",
    "def _safe_quantile(x, q, default=np.nan):\n",
    "    \"\"\"Función auxiliar para calcular cuantiles de forma segura.\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    return float(x.quantile(q)) if len(x) else default\n",
    "\n",
    "# Configurar la figura con subplots\n",
    "n = len(tickers_focus)\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 3.6 * nrows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "# Generar histogramas para cada activo seleccionado\n",
    "for ax, t in zip(axes, tickers_focus):\n",
    "    # Separar retornos por estado: Normal vs Estrés\n",
    "    xN = ret.loc[~flags, t].dropna()  # Retornos en estado Normal\n",
    "    xS = ret.loc[flags, t].dropna()   # Retornos en estado Estrés\n",
    "    x_all = pd.concat([xN, xS], axis=0)\n",
    "\n",
    "    # Ventana central para legibilidad (marcando líneas VaR)\n",
    "    q_lo = _safe_quantile(x_all, 0.005)  # Percentil 0.5%\n",
    "    q_hi = _safe_quantile(x_all, 0.995)  # Percentil 99.5%\n",
    "    if np.isfinite(q_lo) and np.isfinite(q_hi) and q_hi > q_lo:\n",
    "        bins = np.linspace(q_lo, q_hi, 70)\n",
    "        ax.set_xlim(q_lo, q_hi)\n",
    "    else:\n",
    "        bins = 70\n",
    "\n",
    "    # Crear histogramas de densidad para ambos estados\n",
    "    ax.hist(xN, bins=bins, density=True, alpha=0.55, color=\"#4C78A8\", label=\"Normal\")\n",
    "    ax.hist(xS, bins=bins, density=True, alpha=0.45, color=\"#E45756\", label=\"Estrés\")\n",
    "\n",
    "    # Marcar líneas de VaR (5% discontinua, 1% punteada)\n",
    "    v5N, _ = var_es(xN, 0.05)  # VaR 5% estado Normal\n",
    "    v1N, _ = var_es(xN, 0.01)  # VaR 1% estado Normal\n",
    "    v5S, _ = var_es(xS, 0.05)  # VaR 5% estado Estrés\n",
    "    v1S, _ = var_es(xS, 0.01)  # VaR 1% estado Estrés\n",
    "\n",
    "    # Dibujar líneas verticales para VaR Normal\n",
    "    if np.isfinite(v5N):\n",
    "        ax.axvline(v5N, color=\"#4C78A8\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1N):\n",
    "        ax.axvline(v1N, color=\"#4C78A8\", linestyle=\":\", linewidth=1)\n",
    "    \n",
    "    # Dibujar líneas verticales para VaR Estrés\n",
    "    if np.isfinite(v5S):\n",
    "        ax.axvline(v5S, color=\"#E45756\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1S):\n",
    "        ax.axvline(v1S, color=\"#E45756\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "    # Configurar título y etiquetas en español\n",
    "    ax.set_title(f\"{t} — Normal vs Estrés (histograma densidad)\\nVaR5% (--) y VaR1% (:) por estado\")\n",
    "    ax.set_xlabel(\"Retorno logarítmico diario\")\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "\n",
    "# Ocultar ejes no utilizados\n",
    "for ax in axes[len(tickers_focus):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificación opcional: cuántos días por estado\n",
    "print(\"Días en Normal:\", int((~flags).sum()))\n",
    "print(\"Días en Estrés:\", int(flags.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedeac35",
   "metadata": {},
   "source": [
    "### Lectura ejecutiva de Fase 2\n",
    "\n",
    "Se resume de forma automática:\n",
    "\n",
    "1. Cuánto aumenta la volatilidad de HYG al pasar de Normal a Estrés.\n",
    "2. Si GLD mantiene comportamiento defensivo en períodos de estrés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5c67e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:17.186709Z",
     "iopub.status.busy": "2026-02-15T21:25:17.186198Z",
     "iopub.status.idle": "2026-02-15T21:25:17.229896Z",
     "shell.execute_reply": "2026-02-15T21:25:17.228357Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 2: respuestas automáticas (HYG y GLD) ---\n",
    "\n",
    "# Definir risk_by_state a partir del summary calculado anteriormente\n",
    "risk_by_state = summary.reset_index()\n",
    "\n",
    "# Filtrar datos para los activos HYG (bonos high yield) y GLD (oro)\n",
    "phase2_q = (\n",
    "    risk_by_state[risk_by_state[\"ticker\"].isin([\"HYG\", \"GLD\"])]\n",
    "    .set_index([\"ticker\", \"state\"])\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Mostrar tabla con estadísticas principales por estado\n",
    "# Usar solo las columnas que existen en el DataFrame\n",
    "available_cols = [col for col in [\"n_days\", \"mean\", \"vol\", \"VaR_1\", \"ES_1\", \"VaR_5\", \"ES_5\", \"skew\", \"kurt\"] if col in phase2_q.columns]\n",
    "if available_cols:\n",
    "    display(phase2_q[available_cols])\n",
    "else:\n",
    "    # Si no existen las columnas esperadas, mostrar todas las columnas disponibles\n",
    "    print(\"Columnas disponibles en phase2_q:\")\n",
    "    print(phase2_q.columns.tolist())\n",
    "    display(phase2_q)\n",
    "\n",
    "# Análisis específico para HYG (High Yield Bonds)\n",
    "# Usar los nombres de estado correctos según los datos: \"normal\" y \"crisis\"\n",
    "hyg_n = phase2_q.loc[(\"HYG\", \"normal\")]  # Estado Normal\n",
    "hyg_s = phase2_q.loc[(\"HYG\", \"crisis\")]  # Estado Estrés\n",
    "\n",
    "# Calcular incremento porcentual de volatilidad en estrés\n",
    "hyg_vol_inc_pct = (hyg_s[\"vol_daily\"] / hyg_n[\"vol_daily\"] - 1.0) * 100.0\n",
    "\n",
    "# Mostrar resumen de volatilidad para HYG\n",
    "print(f\"Incremento de volatilidad HYG en crisis: {hyg_vol_inc_pct:.2f}%\")\n",
    "\n",
    "# Análisis específico para GLD (Oro) como activo refugio\n",
    "gld_n = phase2_q.loc[(\"GLD\", \"normal\")]  # Estado Normal\n",
    "gld_s = phase2_q.loc[(\"GLD\", \"crisis\")]  # Estado Estrés\n",
    "\n",
    "# Definir criterios para evaluar si GLD actúa como refugio en crisis\n",
    "gld_checks = {\n",
    "    # Criterio 1: Retorno positivo en crisis (refugio debería generar ganancias)\n",
    "    \"mean_positive_in_crisis\": bool(gld_s[\"mean_daily\"] > 0),\n",
    "    # Criterio 2: Volatilidad no mayor en crisis (refugio debería ser menos volátil)\n",
    "    \"vol_not_higher_in_crisis\": bool(gld_s[\"vol_daily\"] <= gld_n[\"vol_daily\"]),\n",
    "    # Criterio 3: Retorno esperado (p99) no peor en crisis (pérdidas esperadas no deberían aumentar)\n",
    "    \"p99_not_worse_in_crisis\": bool(gld_s[\"p99\"] >= gld_n[\"p99\"]),\n",
    "    # Criterio 4: Retorno mínimo (p01) no peor en crisis (pérdidas extremas no deberían aumentar)\n",
    "    \"p01_not_worse_in_crisis\": bool(gld_s[\"p01\"] >= gld_n[\"p01\"]),\n",
    "}\n",
    "\n",
    "# Calcular puntuación de refugio (número de criterios cumplidos)\n",
    "score = sum(gld_checks.values())\n",
    "\n",
    "# Evaluar conclusión basada en puntuación\n",
    "if score >= 3:\n",
    "    gld_conclusion = \"Sí: GLD actúa como refugio (al menos parcial) en este corte.\"\n",
    "elif score == 2:\n",
    "    gld_conclusion = \"Mixto: GLD no empeora claramente, pero la señal de refugio no es concluyente.\"\n",
    "else:\n",
    "    gld_conclusion = \"No: GLD no se comporta como refugio en este corte.\"\n",
    "\n",
    "# Mostrar resultados del análisis de GLD\n",
    "print(\"GLD checks:\", gld_checks)\n",
    "print(gld_conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5ddab",
   "metadata": {},
   "source": [
    "### Distribuciones Normal vs Estrés\n",
    "\n",
    "Visual comparativo de histogramas por activo para revisar cambios en forma, dispersión y cola izquierda.\n",
    "\n",
    "Para legibilidad, el eje X se acota a percentiles centrales, manteniendo referencias de VaR(5%) y VaR(1%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28432878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:17.235637Z",
     "iopub.status.busy": "2026-02-15T21:25:17.235637Z",
     "iopub.status.idle": "2026-02-15T21:25:20.092361Z",
     "shell.execute_reply": "2026-02-15T21:25:20.089628Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 2 (figura adicional): distribuciones de retornos por estado ---\n",
    "\n",
    "# Reutilizar objetos de la Fase 2\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Seleccionar algunos activos representativos (de riesgo + defensivos)\n",
    "if \"EQUITY_TICKERS\" in globals() and EQUITY_TICKERS:\n",
    "    equity_anchor = EQUITY_TICKERS[0]  # Primera acción como ancla de riesgo\n",
    "else:\n",
    "    equity_anchor = ret.columns[0] if len(ret.columns) else None\n",
    "\n",
    "hy_ticker = HY_TICKER if \"HY_TICKER\" in globals() else None      # High Yield (bonos de alto rendimiento)\n",
    "bond10_ticker = BOND_10Y_TICKER if \"BOND_10Y_TICKER\" in globals() else None  # Bonos a 10 años\n",
    "gold_ticker = GOLD_TICKER if \"GOLD_TICKER\" in globals() else None  # Oro como activo defensivo\n",
    "\n",
    "# Crear lista de candidatos para análisis\n",
    "focus_candidates = [equity_anchor, hy_ticker, bond10_ticker, gold_ticker]\n",
    "tickers_focus = [t for t in focus_candidates if t is not None and t in ret.columns]\n",
    "if not tickers_focus:\n",
    "    tickers_focus = list(ret.columns[:4])  # Si no hay candidatos, usar primeros 4 activos\n",
    "\n",
    "def _safe_quantile(x, q, default=np.nan):\n",
    "    \"\"\"Función auxiliar para calcular cuantiles de forma segura.\"\"\"\n",
    "    x = pd.Series(x).dropna()\n",
    "    return float(x.quantile(q)) if len(x) else default\n",
    "\n",
    "# Configurar la figura con subplots\n",
    "n = len(tickers_focus)\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 3.6 * nrows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "# Generar histogramas para cada activo seleccionado\n",
    "for ax, t in zip(axes, tickers_focus):\n",
    "    # Separar retornos por estado: Normal vs Estrés\n",
    "    xN = ret.loc[~flags, t].dropna()  # Retornos en estado Normal\n",
    "    xS = ret.loc[flags, t].dropna()   # Retornos en estado Estrés\n",
    "    x_all = pd.concat([xN, xS], axis=0)\n",
    "\n",
    "    # Ventana central para legibilidad (marcando líneas VaR)\n",
    "    q_lo = _safe_quantile(x_all, 0.005)  # Percentil 0.5%\n",
    "    q_hi = _safe_quantile(x_all, 0.995)  # Percentil 99.5%\n",
    "    if np.isfinite(q_lo) and np.isfinite(q_hi) and q_hi > q_lo:\n",
    "        bins = np.linspace(q_lo, q_hi, 70)\n",
    "        ax.set_xlim(q_lo, q_hi)\n",
    "    else:\n",
    "        bins = 70\n",
    "\n",
    "    # Crear histogramas de densidad para ambos estados\n",
    "    ax.hist(xN, bins=bins, density=True, alpha=0.55, color=\"#4C78A8\", label=\"Normal\")\n",
    "    ax.hist(xS, bins=bins, density=True, alpha=0.45, color=\"#E45756\", label=\"Estrés\")\n",
    "\n",
    "    # Marcar líneas de VaR (5% discontinua, 1% punteada)\n",
    "    v5N, _ = var_es(xN, 0.05)  # VaR 5% estado Normal\n",
    "    v1N, _ = var_es(xN, 0.01)  # VaR 1% estado Normal\n",
    "    v5S, _ = var_es(xS, 0.05)  # VaR 5% estado Estrés\n",
    "    v1S, _ = var_es(xS, 0.01)  # VaR 1% estado Estrés\n",
    "\n",
    "    # Dibujar líneas verticales para VaR Normal\n",
    "    if np.isfinite(v5N):\n",
    "        ax.axvline(v5N, color=\"#4C78A8\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1N):\n",
    "        ax.axvline(v1N, color=\"#4C78A8\", linestyle=\":\", linewidth=1)\n",
    "    \n",
    "    # Dibujar líneas verticales para VaR Estrés\n",
    "    if np.isfinite(v5S):\n",
    "        ax.axvline(v5S, color=\"#E45756\", linestyle=\"--\", linewidth=1)\n",
    "    if np.isfinite(v1S):\n",
    "        ax.axvline(v1S, color=\"#E45756\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "    # Configurar título y etiquetas en español\n",
    "    ax.set_title(f\"{t} — Normal vs Estrés (histograma densidad)\\nVaR5% (--) y VaR1% (:) por estado\")\n",
    "    ax.set_xlabel(\"Retorno logarítmico diario\")\n",
    "    ax.grid(alpha=0.2)\n",
    "    ax.legend()\n",
    "\n",
    "# Ocultar ejes no utilizados\n",
    "for ax in axes[len(tickers_focus):]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ecd2b",
   "metadata": {},
   "source": [
    "## Visualización de Diferencias Estadísticas Normal vs Estrés\n",
    "\n",
    "Los histogramas comparativos muestran **diferencias significativas** entre regímenes:\n",
    "\n",
    "### **Patrones Observados**\n",
    "- **Estrés (rojo)**: Mayor dispersión, colas más gruesas, VaR más extremo\n",
    "- **Normal (azul)**: Distribución más concentrada, menor volatilidad\n",
    "- **VaR desplazado**: Líneas de riesgo sistemáticamente más negativas en crisis\n",
    "\n",
    "### **Implicaciones para Gestión de Riesgos**\n",
    "- **Modelado condicional**: Estadísticas diferenciadas por régimen esencial\n",
    "- **Stress testing**: Regímenes de crisis proporcionan escenarios realistas\n",
    "- **Cópulas por estado**: Dependencia cambia significativamente entre regímenes\n",
    "\n",
    "La evidencia visual justifica completamente el enfoque de modelado HMM + cópulas condicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d09ed8",
   "metadata": {},
   "source": [
    "## Fase 3 - Dependencia entre activos\n",
    "\n",
    "Objetivo: analizar cómo se deteriora la diversificacion en estrés.\n",
    "\n",
    "Secuencia de trabajo:\n",
    "\n",
    "1. Correlaciones por estado (dependencia lineal).\n",
    "2. Dependencia en cola empírica (co-movimientos extremos).\n",
    "3. Ajuste de cópulas (gaussiana y t) por estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b94df08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:20.098114Z",
     "iopub.status.busy": "2026-02-15T21:25:20.098114Z",
     "iopub.status.idle": "2026-02-15T21:25:22.092788Z",
     "shell.execute_reply": "2026-02-15T21:25:22.090253Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 3a: correlación por estado (Normal vs Estrés) ---\n",
    "\n",
    "# Preparar datos para análisis de correlación\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Separar retornos por estado\n",
    "retN = ret.loc[~flags].dropna(how=\"all\")  # Retornos en estado Normal\n",
    "retS = ret.loc[flags].dropna(how=\"all\")   # Retornos en estado Estrés\n",
    "\n",
    "# Calcular matrices de correlación por estado\n",
    "corrN = retN.corr()  # Correlación en estado Normal\n",
    "corrS = retS.corr()  # Correlación en estado Estrés\n",
    "\n",
    "# Calcular diferencias de correlación (Estrés - Normal)\n",
    "corrD = corrS - corrN\n",
    "\n",
    "def _heatmap(ax, M, title, vmin=-1, vmax=1, cmap=\"coolwarm\"):\n",
    "    \"\"\"Función auxiliar para crear mapas de calor de correlación.\n",
    "    \n",
    "    Args:\n",
    "        ax: Eje matplotlib donde dibujar\n",
    "        M: Matriz de correlación\n",
    "        title: Título del gráfico\n",
    "        vmin, vmax: Valores mínimos y máximos para la escala de colores\n",
    "        cmap: Mapa de colores\n",
    "    \n",
    "    Returns:\n",
    "        im: Objeto imagen para barra de colores\n",
    "    \"\"\"\n",
    "    im = ax.imshow(M.values, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    ax.set_xticks(range(M.shape[1]))\n",
    "    ax.set_yticks(range(M.shape[0]))\n",
    "    ax.set_xticklabels(M.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(M.index)\n",
    "    ax.set_title(title)\n",
    "    # Ocultar bordes del gráfico para mejor visualización\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    return im\n",
    "\n",
    "# Escala fija para diferencias (específica del caso): \n",
    "# Escala vívida y comparable dentro de este informe\n",
    "max_abs_delta = float(np.nanmax(np.abs(corrD.values)))\n",
    "delta_lim = 0.15  # Escala \"razonable\" fija para este dataset/informe\n",
    "if max_abs_delta > delta_lim:\n",
    "    print(f\"[Aviso] max |Δ| = {max_abs_delta:.3f} supera ±{delta_lim:.2f}; el panel Δ saturará colores.\")\n",
    "\n",
    "# Crear figura con tres paneles: Normal, Estrés y Diferencias\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4), constrained_layout=True)\n",
    "im0 = _heatmap(axes[0], corrN, f\"Corr (Normal), n={len(retN)}\")\n",
    "im1 = _heatmap(axes[1], corrS, f\"Corr (Estres), n={len(retS)}\")\n",
    "im2 = _heatmap(\n",
    "    axes[2],\n",
    "    corrD,\n",
    "    f\"Δ Corr (Estrés − Normal)\\n(max |Δ| = {max_abs_delta:.3f}, escala ±{delta_lim:.2f})\",\n",
    "    vmin=-delta_lim,\n",
    "    vmax=delta_lim,\n",
    ")\n",
    "\n",
    "# Añadir barras de colores a cada panel\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "plt.show()\n",
    "\n",
    "# Mostrar los cambios absolutos más grandes (triángulo superior)\n",
    "pairs = []\n",
    "cols = list(corrD.columns)\n",
    "for i in range(len(cols)):\n",
    "    for j in range(i + 1, len(cols)):\n",
    "        a, b = cols[i], cols[j]\n",
    "        v = float(corrD.loc[a, b])\n",
    "        pairs.append((abs(v), v, a, b))\n",
    "\n",
    "# Ordenar por cambio absoluto y mostrar top 10\n",
    "top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "display(pd.DataFrame(top, columns=[\"abs_delta\", \"delta\", \"asset_a\", \"asset_b\"]))\n",
    "\n",
    "# Mostrar matriz de diferencias completa (redondeada a 3 decimales)\n",
    "display(corrD.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db30ab",
   "metadata": {},
   "source": [
    "## Matriz de Correlaciones y Cambios entre Estados\n",
    "\n",
    "### **Correlaciones Dominantes**\n",
    "- **GME outlier**: Correlaciones extremas (0.27-0.31) con tech giants\n",
    "- **Tech cluster**: AAPL-AMZN-NVDA con correlaciones moderadas (0.15-0.20)\n",
    "- **Bonos defensivos**: IEF-SHY correlación positiva (0.032)\n",
    "- **Oro como refugio**: Correlaciones negativas con equity (-0.107 con AAPL)\n",
    "\n",
    "### **Diferencias por Régimen (Δ)**\n",
    "- **GME como disruptor**: Máxima diferencia absoluta (0.31) con GOOGL\n",
    "- **Tech sincronización**: Cambios significativos en cluster tecnológico\n",
    "- **Flight-to-quality**: Reducción de correlaciones en crisis\n",
    "\n",
    "### **Implicaciones para Modelado**\n",
    "- **Cópulas necesarias**: Dependencia no lineal entre activos\n",
    "- **Regímenes diferenciados**: Correlaciones cambian significativamente\n",
    "- **Diversificación efectiva**: Bonos y oro como hedge en crisis\n",
    "\n",
    "La evidencia justifica modelado de dependencia condicional y estrategias de diversificación dinámica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30dd1ab",
   "metadata": {},
   "source": [
    "### 3b) Dependencia en cola (empírica)\n",
    "\n",
    "La correlación central no captura bien episodios extremos.\n",
    "\n",
    "Se estima para cuantiles bajos `q`:\n",
    "\n",
    "`lambda_L(q) = P(R_i <= Q_i(q), R_j <= Q_j(q)) / q`\n",
    "\n",
    "Interpretación: cuanto mas alto `lambda_L`, mayor probabilidad de caidas conjuntas en la cola izquierda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c938a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:22.097837Z",
     "iopub.status.busy": "2026-02-15T21:25:22.096313Z",
     "iopub.status.idle": "2026-02-15T21:25:28.426666Z",
     "shell.execute_reply": "2026-02-15T21:25:28.424614Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 3b: dependencia en cola inferior (empírica) por estado ---\n",
    "\n",
    "# Preparar datos para análisis de dependencia en cola\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Definir estados y cuantiles para análisis de cola\n",
    "states = {\"Normal\": ~flags, \"Estrés\": flags}  # Usar \"Estrés\" con tilde consistentemente\n",
    "qs = [0.05, 0.01]  # Cuantiles: 5% y 1%\n",
    "\n",
    "assets = list(ret.columns)\n",
    "\n",
    "def empirical_lambda_L(sub: pd.DataFrame, q: float) -> pd.DataFrame:\n",
    "    \"\"\"Función para calcular lambda_L empírica.\n",
    "    \n",
    "    Args:\n",
    "        sub: DataFrame con retornos del estado específico\n",
    "        q: Cuantil para análisis de cola (ej. 0.05 para 5%)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con matriz de dependencia lambda_L\n",
    "    \"\"\"\n",
    "    sub = sub.dropna(how=\"all\")\n",
    "    out = pd.DataFrame(index=assets, columns=assets, dtype=float)\n",
    "\n",
    "    # Diagonal: dependencia perfecta consigo mismo\n",
    "    for a in assets:\n",
    "        out.loc[a, a] = 1.0\n",
    "\n",
    "    # Off-diagonales: cálculo de dependencia entre diferentes activos\n",
    "    for i, a in enumerate(assets):\n",
    "        for j in range(i + 1, len(assets)):\n",
    "            b = assets[j]\n",
    "            x = sub[[a, b]].dropna()\n",
    "            if len(x) < 50:\n",
    "                out.loc[a, b] = np.nan\n",
    "                out.loc[b, a] = np.nan\n",
    "                continue\n",
    "\n",
    "            qa = float(x[a].quantile(q))  # Cuantil q para activo a\n",
    "            qb = float(x[b].quantile(q))  # Cuantil q para activo b\n",
    "            ind = (x[a].to_numpy() <= qa) & (x[b].to_numpy() <= qb)\n",
    "            p = float(ind.mean())\n",
    "            val = p / q if q > 0 else np.nan\n",
    "            out.loc[a, b] = val\n",
    "            out.loc[b, a] = val\n",
    "\n",
    "    return out\n",
    "\n",
    "# Calcular matrices lambda_L para cada estado y cuantil\n",
    "lam = {}  # lam[(estado, cuantil)] -> matriz\n",
    "for state_name, mask in states.items():\n",
    "    sub = ret.loc[mask]\n",
    "    for q in qs:\n",
    "        lam[(state_name, q)] = empirical_lambda_L(sub, q)\n",
    "\n",
    "# Generar gráficos de calor para q=5% y q=1% (Normal, Estrés, Diferencias)\n",
    "for q in qs:\n",
    "    L_N = lam[(\"Normal\", q)]      # Lambda_L en estado Normal\n",
    "    L_S = lam[(\"Estrés\", q)]     # Lambda_L en estado Estrés (corregido: usar \"Estrés\" con tilde)\n",
    "    L_D = L_S - L_N               # Diferencias (Estrés - Normal)\n",
    "\n",
    "    # Crear figura con tres paneles\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4), constrained_layout=True)\n",
    "    vmin, vmax = 0.0, 1.0  # Escala para lambda_L (0 a 1)\n",
    "\n",
    "    # Panel 1: Estado Normal\n",
    "    im0 = axes[0].imshow(L_N.values, vmin=vmin, vmax=vmax, cmap=\"viridis\")\n",
    "    axes[0].set_title(f\"λ_L empírica (q={int(q*100)}%) — Normal\")\n",
    "\n",
    "    # Panel 2: Estado Estrés\n",
    "    im1 = axes[1].imshow(L_S.values, vmin=vmin, vmax=vmax, cmap=\"viridis\")\n",
    "    axes[1].set_title(f\"λ_L empírica (q={int(q*100)}%) — Estrés\")\n",
    "\n",
    "    # Panel 3: Diferencias\n",
    "    im2 = axes[2].imshow(L_D.values, vmin=-0.5, vmax=0.5, cmap=\"coolwarm\")\n",
    "    axes[2].set_title(f\"Δ λ_L (Estrés − Normal), q={int(q*100)}%\")\n",
    "\n",
    "    # Configurar ejes para todos los paneles\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(len(assets)))\n",
    "        ax.set_yticks(range(len(assets)))\n",
    "        ax.set_xticklabels(assets, rotation=45, ha=\"right\")\n",
    "        ax.set_yticklabels(assets)\n",
    "        # Ocultar bordes para mejor visualización\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "    # Añadir barras de colores a cada panel\n",
    "    fig.colorbar(im0, ax=axes[0], fraction=0.046)\n",
    "    fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "    plt.show()\n",
    "\n",
    "# Exportar matrices (útil para reporte)\n",
    "for (state_name, q), M in lam.items():\n",
    "    out = OUT_DIR / f\"phase3_taildep_empirical_{state_name.lower()}_q{int(q*100)}.csv\"\n",
    "    M.to_csv(out)\n",
    "    print(\"Guardado:\", out)\n",
    "\n",
    "# Ranking rápido: ¿qué pares aumentan más en dependencia de cola en q=5%?\n",
    "q = 0.05\n",
    "L_D = lam[(\"Estrés\", q)] - lam[(\"Normal\", q)]  # Corregido: usar \"Estrés\" con tilde\n",
    "pairs = []\n",
    "for i, a in enumerate(assets):\n",
    "    for j in range(i + 1, len(assets)):\n",
    "        b = assets[j]\n",
    "        v = L_D.loc[a, b]\n",
    "        if np.isfinite(v):\n",
    "            pairs.append((float(v), a, b))\n",
    "\n",
    "# Ordenar por cambio absoluto y mostrar top 10\n",
    "top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "display(pd.DataFrame(top, columns=[\"delta_lambda\", \"asset_a\", \"asset_b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe2e7b",
   "metadata": {},
   "source": [
    "## Medición de Riesgo de Caídas Conjuntas Extremas\n",
    "\n",
    "### **Metodología λ_L (Lambda Lower Tail)**\n",
    "- **Definición**: `λ_L(q) = P(R_i ≤ Q_i(q), R_j ≤ Q_j(q)) / q`\n",
    "- **Interpretación**: Probabilidad de caídas conjuntas vs esperada\n",
    "- **Cuantiles**: q=5% y q=1% para análisis de extremos\n",
    "\n",
    "### **Hallazgos Principales**\n",
    "- **Aumento en crisis**: Dependencia de cola significativamente mayor en régimen de estrés\n",
    "- **Pares críticos**: GME-MSFT (+0.23), JPM-PG (+0.21) con mayor aumento\n",
    "- **Sectorización**: Tech y defensivos muestran sincronización en crisis\n",
    "\n",
    "### **Implicaciones para Gestión de Riesgos**\n",
    "- **Diversificación limitada**: Beneficio reducido en caídas extremas\n",
    "- **Modelado cópula**: Esencial para capturar dependencia no lineal\n",
    "- **Stress testing**: Escenarios basados en regímenes de crisis\n",
    "\n",
    "La evidencia demuestra que la correlación central subestima riesgo de caídas conjuntas en crisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344d217",
   "metadata": {},
   "source": [
    "### 3c) Cópulas por estado (gaussiana vs t)\n",
    "\n",
    "Se separan marginales y dependencia para modelar estructura conjunta de retornos:\n",
    "\n",
    "- Cópula gaussiana: buena para dependencia media, débil en colas.\n",
    "- Cópula t: más realista para extremos conjuntos.\n",
    "\n",
    "Se exportan matrices de correlacion y (para cópula t) dependencia de cola implícita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa82ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:28.432307Z",
     "iopub.status.busy": "2026-02-15T21:25:28.431209Z",
     "iopub.status.idle": "2026-02-15T21:25:38.286924Z",
     "shell.execute_reply": "2026-02-15T21:25:38.285381Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 3c: Cópulas Gaussiana vs t por estado (pseudo-verosimilitud) ---\n",
    "\n",
    "# Importar librerías necesarias para análisis de cópulas\n",
    "import json\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "# Preparar datos para análisis de cópulas por estado\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "states = {\"Normal\": ~flags, \"Estrés\": flags}\n",
    "assets = list(ret.columns)\n",
    "\n",
    "# Escalas delta fijas (específicas del caso): mejorar contraste en paneles Δ\n",
    "DELTA_CORR_LIM = 0.15   # para Δ correlación cópula Gaussiana\n",
    "DELTA_LAML_LIM = 0.15   # para Δ dependencia de cola cópula t (asintótica)\n",
    "\n",
    "def _pseudo_obs(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Generar pseudo-observaciones en (0,1) mediante rangos por columna.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame con retornos\n",
    "        \n",
    "    Returns:\n",
    "        Array numpy con pseudo-observaciones uniformes\n",
    "    \"\"\"\n",
    "    x = df[assets].dropna()\n",
    "    n = len(x)\n",
    "    if n < 10:\n",
    "        return np.empty((0, len(assets)))\n",
    "    r = x.rank(method=\"average\")\n",
    "    u = (r / (n + 1.0)).to_numpy()  # evitar 0/1\n",
    "    u = np.clip(u, 1e-6, 1 - 1e-6)\n",
    "    return u\n",
    "\n",
    "def _nearest_psd_corr(C: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Proyectar a matriz de correlación PSD mediante recorte de eigenvalores.\n",
    "    \n",
    "    Args:\n",
    "        C: Matriz de correlación\n",
    "        eps: Épsilon mínimo para eigenvalores\n",
    "        \n",
    "    Returns:\n",
    "        Matriz de correlación PSD más cercana\n",
    "    \"\"\"\n",
    "    C = np.asarray(C, dtype=float)\n",
    "    C = 0.5 * (C + C.T)\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    w, V = np.linalg.eigh(C)\n",
    "    w = np.maximum(w, eps)\n",
    "    C2 = (V * w) @ V.T\n",
    "    d = np.sqrt(np.diag(C2))\n",
    "    C2 = C2 / (d[:, None] * d[None, :])\n",
    "    np.fill_diagonal(C2, 1.0)\n",
    "    return 0.5 * (C2 + C2.T)\n",
    "\n",
    "def fit_gaussian_copula(U: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Ajustar cópula Gaussiana mediante pseudo-observaciones.\n",
    "    \n",
    "    Args:\n",
    "        U: Pseudo-observaciones uniformes\n",
    "        \n",
    "    Returns:\n",
    "        Matriz de correlación de cópula Gaussiana\n",
    "    \"\"\"\n",
    "    Z = norm.ppf(U)  # Transformación a normal estándar\n",
    "    C = np.corrcoef(Z, rowvar=False)\n",
    "    return _nearest_psd_corr(C)\n",
    "\n",
    "def try_multivariate_t_logpdf(Z: np.ndarray, C: np.ndarray, df: float) -> np.ndarray:\n",
    "    \"\"\"Calcular log f_T(Z; C, df). Usa scipy si está disponible.\n",
    "    \n",
    "    Args:\n",
    "        Z: Datos transformados\n",
    "        C: Matriz de correlación\n",
    "        df: Grados de libertad\n",
    "        \n",
    "    Returns:\n",
    "        Log-densidad multivariada t\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from scipy.stats import multivariate_t  # type: ignore\n",
    "        mv = multivariate_t(loc=np.zeros(Z.shape[1]), shape=C, df=df)\n",
    "        return mv.logpdf(Z)\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"multivariate_t not available\") from exc\n",
    "\n",
    "def fit_t_copula(U: np.ndarray, df_grid=(3, 4, 5, 7, 10, 15, 20, 30, 50), df_fallback=5):\n",
    "    \"\"\"Ajustar cópula t mediante búsqueda en grid sobre df usando pseudo log-verosimilitud.\n",
    "    \n",
    "    Args:\n",
    "        U: Pseudo-observaciones uniformes\n",
    "        df_grid: Grid de grados de libertad a probar\n",
    "        df_fallback: Grados de libertad por defecto si falla multivariate_t\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: (df óptimo, matriz correlación, flag de éxito)\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_ll = -np.inf\n",
    "\n",
    "    can_mv_t = True\n",
    "    # prueba rápida de capacidad\n",
    "    try:\n",
    "        Z_probe = t.ppf(U[: min(len(U), 20)], df_fallback)\n",
    "        C_probe = _nearest_psd_corr(np.corrcoef(Z_probe, rowvar=False))\n",
    "        _ = try_multivariate_t_logpdf(Z_probe, C_probe, df_fallback)\n",
    "    except Exception:\n",
    "        can_mv_t = False\n",
    "\n",
    "    if not can_mv_t:\n",
    "        df = df_fallback\n",
    "        Z = t.ppf(U, df)\n",
    "        C = _nearest_psd_corr(np.corrcoef(Z, rowvar=False))\n",
    "        return df, C, False\n",
    "\n",
    "    # Búsqueda grid sobre grados de libertad\n",
    "    for df in df_grid:\n",
    "        Z = t.ppf(U, df)\n",
    "        C = _nearest_psd_corr(np.corrcoef(Z, rowvar=False))\n",
    "        log_joint = try_multivariate_t_logpdf(Z, C, df)\n",
    "        log_marg = t.logpdf(Z, df=df).sum(axis=1)\n",
    "        log_cop = log_joint - log_marg\n",
    "        ll = float(np.nanmean(log_cop))\n",
    "        if ll > best_ll:\n",
    "            best_ll = ll\n",
    "            best = (df, C)\n",
    "\n",
    "    assert best is not None\n",
    "    return best[0], best[1], True\n",
    "\n",
    "def t_copula_taildep_lambda_L(C: np.ndarray, df: float) -> np.ndarray:\n",
    "    \"\"\"Dependencia de cola inferior asintótica para cópula t (por pares), forma matricial.\n",
    "    \n",
    "    Args:\n",
    "        C: Matriz de correlación\n",
    "        df: Grados de libertad\n",
    "        \n",
    "    Returns:\n",
    "        Matriz de dependencia de cola lambda_L\n",
    "    \"\"\"\n",
    "    d = C.shape[0]\n",
    "    out = np.zeros((d, d), dtype=float)\n",
    "    np.fill_diagonal(out, 1.0)\n",
    "    for i in range(d):\n",
    "        for j in range(i + 1, d):\n",
    "            rho = float(C[i, j])\n",
    "            # protección numérica\n",
    "            rho = max(min(rho, 0.999), -0.999)\n",
    "            a = np.sqrt((df + 1.0) * (1.0 - rho) / (1.0 + rho))\n",
    "            lam = 2.0 * t.cdf(-a, df=df + 1.0)\n",
    "            out[i, j] = lam\n",
    "            out[j, i] = lam\n",
    "    return out\n",
    "\n",
    "# Ajustar cópulas para cada estado\n",
    "fits = {}\n",
    "for state_name, mask in states.items():\n",
    "    U = _pseudo_obs(ret.loc[mask])\n",
    "    if len(U) == 0:\n",
    "        continue\n",
    "\n",
    "    # Ajustar cópula Gaussiana\n",
    "    Cg = fit_gaussian_copula(U)\n",
    "    \n",
    "    # Ajustar cópula t con optimización de grados de libertad\n",
    "    df_t, Ct, used_mvt = fit_t_copula(U)\n",
    "    \n",
    "    # Calcular dependencia de cola asintótica para cópula t\n",
    "    L_t = t_copula_taildep_lambda_L(Ct, df_t)\n",
    "\n",
    "    # Guardar resultados del ajuste\n",
    "    fits[state_name] = {\n",
    "        \"n\": int(U.shape[0]),  # número de observaciones\n",
    "        \"gaussian_corr\": Cg,  # correlación cópula Gaussiana\n",
    "        \"t_df\": float(df_t),  # grados de libertad óptimos\n",
    "        \"t_corr\": Ct,  # correlación cópula t\n",
    "        \"t_taildep_lambdaL\": L_t,  # dependencia de cola t\n",
    "        \"used_multivariate_t\": bool(used_mvt),  # si usó multivariate_t\n",
    "    }\n",
    "\n",
    "    # Exportar matrices a archivos CSV\n",
    "    pd.DataFrame(Cg, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_gaussian_corr_{state_name.lower()}.csv\")\n",
    "    pd.DataFrame(Ct, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_t_corr_{state_name.lower()}.csv\")\n",
    "    pd.DataFrame(L_t, index=assets, columns=assets).to_csv(OUT_DIR / f\"phase3_copula_t_taildep_lambdaL_{state_name.lower()}.csv\")\n",
    "\n",
    "# Guardar resumen en JSON\n",
    "summary = {\n",
    "    s: {\"n\": fits[s][\"n\"], \"t_df\": fits[s][\"t_df\"], \"used_multivariate_t\": fits[s][\"used_multivariate_t\"]}\n",
    "    for s in fits\n",
    "}\n",
    "out_json = OUT_DIR / \"phase3_copula_summary.json\"\n",
    "out_json.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"Guardado:\", out_json)\n",
    "print(summary)\n",
    "\n",
    "# Gráficos: Correlación Gaussiana y dependencia de cola t lado a lado (Normal vs Estrés)\n",
    "if \"Normal\" in fits and \"Estrés\" in fits:\n",
    "    # Extraer matrices para visualización\n",
    "    CgN = pd.DataFrame(fits[\"Normal\"][\"gaussian_corr\"], index=assets, columns=assets)\n",
    "    CgS = pd.DataFrame(fits[\"Estrés\"][\"gaussian_corr\"], index=assets, columns=assets)\n",
    "    LtN = pd.DataFrame(fits[\"Normal\"][\"t_taildep_lambdaL\"], index=assets, columns=assets)\n",
    "    LtS = pd.DataFrame(fits[\"Estrés\"][\"t_taildep_lambdaL\"], index=assets, columns=assets)\n",
    "\n",
    "    # Calcular diferencias entre estados\n",
    "    dCg = (CgS - CgN)  # diferencias correlación Gaussiana\n",
    "    dLt = (LtS - LtN)  # diferencias dependencia de cola t\n",
    "    max_abs_dCg = float(np.nanmax(np.abs(dCg.values)))\n",
    "    max_abs_dLt = float(np.nanmax(np.abs(dLt.values)))\n",
    "    \n",
    "    # Verificar si las diferencias exceden los límites de escala\n",
    "    if max_abs_dCg > DELTA_CORR_LIM:\n",
    "        print(f\"[Aviso] max |Δ corr (Gauss)| = {max_abs_dCg:.3f} supera ±{DELTA_CORR_LIM:.2f}; el panel Δ corr saturará colores.\")\n",
    "    if max_abs_dLt > DELTA_LAML_LIM:\n",
    "        print(f\"[Aviso] max |Δ λ_L (t)| = {max_abs_dLt:.3f} supera ±{DELTA_LAML_LIM:.2f}; el panel Δ λ_L saturará colores.\")\n",
    "\n",
    "    # Crear figura con 6 paneles (2 filas x 3 columnas)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 9), constrained_layout=True)\n",
    "\n",
    "    def _hm(ax, M, title, vmin, vmax, cmap):\n",
    "        \"\"\"Función auxiliar para crear mapas de calor.\"\"\"\n",
    "        im = ax.imshow(M.values, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "        ax.set_xticks(range(len(assets)))\n",
    "        ax.set_yticks(range(len(assets)))\n",
    "        ax.set_xticklabels(assets, rotation=45, ha=\"right\")\n",
    "        ax.set_yticklabels(assets)\n",
    "        ax.set_title(title)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        return im\n",
    "\n",
    "    # Fila 1: Correlaciones de cópula Gaussiana\n",
    "    im = _hm(axes[0, 0], CgN, \"Cópula Gaussiana corr — Normal\", -1, 1, \"coolwarm\")\n",
    "    fig.colorbar(im, ax=axes[0, 0], fraction=0.046)\n",
    "    im = _hm(axes[0, 1], CgS, \"Cópula Gaussiana corr — Estrés\", -1, 1, \"coolwarm\")\n",
    "    fig.colorbar(im, ax=axes[0, 1], fraction=0.046)\n",
    "    im = _hm(\n",
    "        axes[0, 2],\n",
    "        dCg,\n",
    "        f\"Δ corr (Estrés − Normal)\\n(max |Δ| = {max_abs_dCg:.3f}, escala ±{DELTA_CORR_LIM:.2f})\",\n",
    "        -DELTA_CORR_LIM,\n",
    "        DELTA_CORR_LIM,\n",
    "        \"coolwarm\",\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[0, 2], fraction=0.046)\n",
    "\n",
    "    # Fila 2: Dependencia de cola de cópula t\n",
    "    im = _hm(axes[1, 0], LtN, \"Cópula t λ_L (asint.) — Normal\", 0, 1, \"viridis\")\n",
    "    fig.colorbar(im, ax=axes[1, 0], fraction=0.046)\n",
    "    im = _hm(axes[1, 1], LtS, \"Cópula t λ_L (asint.) — Estrés\", 0, 1, \"viridis\")\n",
    "    fig.colorbar(im, ax=axes[1, 1], fraction=0.046)\n",
    "    im = _hm(\n",
    "        axes[1, 2],\n",
    "        dLt,\n",
    "        f\"Δ λ_L (Estrés − Normal)\\n(max |Δ| = {max_abs_dLt:.3f}, escala ±{DELTA_LAML_LIM:.2f})\",\n",
    "        -DELTA_LAML_LIM,\n",
    "        DELTA_LAML_LIM,\n",
    "        \"coolwarm\",\n",
    "    )\n",
    "    fig.colorbar(im, ax=axes[1, 2], fraction=0.046)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Ranking rápido de aumentos de dependencia de cola (asintótica) bajo cópula t\n",
    "    D = dLt.values\n",
    "    pairs = []\n",
    "    for i, a in enumerate(assets):\n",
    "        for j in range(i + 1, len(assets)):\n",
    "            b = assets[j]\n",
    "            pairs.append((float(D[i, j]), a, b))\n",
    "    top = sorted(pairs, key=lambda t: t[0], reverse=True)[:10]\n",
    "    display(pd.DataFrame(top, columns=[\"delta_lambdaL_t\", \"asset_a\", \"asset_b\"]))\n",
    "\n",
    "else:\n",
    "    print(\"Ajuste de cópula falta un estado; disponibles:\", list(fits.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5944492",
   "metadata": {},
   "source": [
    "## Modelado de Dependencia Condicional\n",
    "\n",
    "### **Resultados del Ajuste**\n",
    "- **Normal**: df=10.0 (colas moderadas), n=4071 obs\n",
    "- **Estrés**: df=7.0 (colas más gruesas), n=671 obs\n",
    "- **Convergencia**: Multivariate_t disponible en ambos estados\n",
    "\n",
    "### **Cambios Significativos en Crisis**\n",
    "- **Correlación Gaussiana**: Δmax=0.216 (satura escala ±0.15)\n",
    "- **Dependencia de cola t**: Δmax=0.232 (satura escala ±0.15)\n",
    "- **Top pares**: CVX-XOM (+0.23), AAPL-GOOGL (+0.18), JNJ-PG (+0.16)\n",
    "\n",
    "### **Interpretación y Aplicaciones**\n",
    "- **Cópula t superior**: Captura extremos mejor que Gaussiana\n",
    "- **Regímenes diferenciados**: Dependencia cambia significativamente\n",
    "- **Modelado avanzado**: Base para simulación Monte Carlo realista\n",
    "\n",
    "La evidencia confirma necesidad de cópulas condicionales y modelado t para capturar tail risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d8004",
   "metadata": {},
   "source": [
    "### Puente Fase 3 -> Fase 4\n",
    "\n",
    "La celda siguiente resume con evidencia cuantitativa las diferencias entre Normal y Estrés en:\n",
    "\n",
    "- riesgo marginal,\n",
    "- dependencia,\n",
    "- features del HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fb6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:38.292901Z",
     "iopub.status.busy": "2026-02-15T21:25:38.291885Z",
     "iopub.status.idle": "2026-02-15T21:25:38.364504Z",
     "shell.execute_reply": "2026-02-15T21:25:38.362973Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Transición Fase 3->4: resumen cuantitativo de diferencias entre regímenes  ---\n",
    "\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "\n",
    "# Mapeo consistente entre nombres internos (risk_by_state) y nombres de presentación\n",
    "present_order = [\"Normal\", \"Estres\"]\n",
    "internal_order = [\"normal\", \"crisis\"]\n",
    "name_map = {\"normal\": \"Normal\", \"crisis\": \"Estres\"}\n",
    "\n",
    "summary = pd.DataFrame(index=present_order)\n",
    "summary.index.name = \"state\"\n",
    "\n",
    "# Días por régimen\n",
    "summary[\"n_days\"] = [int((~flags).sum()), int(flags.sum())]\n",
    "summary[\"pct_days\"] = summary[\"n_days\"] / len(flags)\n",
    "\n",
    "# Riesgo marginal medio por estado (promediado sobre activos)\n",
    "by_state = risk_by_state.groupby(\"state\", dropna=False)\n",
    "\n",
    "vol_s = by_state[\"vol_daily\"].mean().rename(index=name_map)\n",
    "p01_s = (-by_state[\"p01\"].mean()).rename(index=name_map)\n",
    "p99_s = ( by_state[\"p99\"].mean()).rename(index=name_map)\n",
    "\n",
    "summary[\"avg_asset_vol\"] = vol_s.reindex(present_order)\n",
    "summary[\"avg_abs_p01\"]   = p01_s.reindex(present_order)\n",
    "summary[\"avg_abs_p99\"]   = p99_s.reindex(present_order)\n",
    "\n",
    "# Dependencia lineal media\n",
    "corr_n = ret.loc[~flags].corr()\n",
    "corr_s = ret.loc[flags].corr()\n",
    "tri = np.triu_indices_from(corr_n, k=1)\n",
    "\n",
    "summary.loc[\"Normal\", \"avg_pair_corr\"] = float(np.nanmean(corr_n.values[tri]))\n",
    "summary.loc[\"Estres\", \"avg_pair_corr\"] = float(np.nanmean(corr_s.values[tri]))\n",
    "\n",
    "# Features medias del HMM por estado\n",
    "feat = features.copy()\n",
    "feat[\"state\"] = np.where(\n",
    "    flags.reindex(feat.index).astype(\"boolean\").fillna(False).astype(bool),\n",
    "    \"Estres\",\n",
    "    \"Normal\",\n",
    ")\n",
    "\n",
    "feat_cols = [\"vol_equity_21\", \"vol_hy_21\", \"dd_equity\", \"r_credit\", \"r_equity\"]\n",
    "feat_means = feat.groupby(\"state\")[feat_cols].mean()\n",
    "for col in feat_cols:\n",
    "    summary[col] = feat_means[col].reindex(present_order)\n",
    "\n",
    "# Dependencia en cola t-copula (si existe fit en ambos estados)\n",
    "if \"Normal\" in fits and \"Estres\" in fits:\n",
    "    LtN = np.asarray(fits[\"Normal\"][\"t_taildep_lambdaL\"], dtype=float)\n",
    "    LtS = np.asarray(fits[\"Estres\"][\"t_taildep_lambdaL\"], dtype=float)\n",
    "    summary.loc[\"Normal\", \"avg_tcopula_lambdaL\"] = float(np.nanmean(LtN[tri]))\n",
    "    summary.loc[\"Estres\", \"avg_tcopula_lambdaL\"] = float(np.nanmean(LtS[tri]))\n",
    "\n",
    "display(summary.round(4))\n",
    "\n",
    "delta_pct = (summary.loc[\"Estres\"] / summary.loc[\"Normal\"] - 1.0) * 100.0\n",
    "key_cols = [\"avg_asset_vol\", \"avg_abs_p01\", \"avg_pair_corr\", \"vol_equity_21\", \"vol_hy_21\"]\n",
    "print()\n",
    "print(\"Cambios relativos Estres vs Normal (%) en metricas clave:\")\n",
    "print(delta_pct[key_cols].round(2))\n",
    "\n",
    "print()\n",
    "print(\"Interpretacion económica (síntesis):\")\n",
    "print(\"- El estado de Estres concentra mayor volatilidad y peores colas de pérdida.\")\n",
    "print(\"- La co-movilidad media entre activos aumenta (diversificación menos efectiva).\")\n",
    "print(\"- Suben las volatilidades realizadas (equity/HY) y se profundiza el drawdown de equity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654344fe",
   "metadata": {},
   "source": [
    "## 4) Fase 4 - Motor de simulación (10.000 trayectorias, 6 meses)\n",
    "\n",
    "Se implementa Monte Carlo con cambio de regimen diario mediante la matriz de transición del HMM y dependencia por cópula condicionada al estado.\n",
    "\n",
    "Validaciones incluidas:\n",
    "\n",
    "- wealth real vs bandas p5-p50-p95 simuladas,\n",
    "- reproduccion de regímenes,\n",
    "- reproducción de riesgo y dependencia (real vs simulado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df748d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:25:38.369574Z",
     "iopub.status.busy": "2026-02-15T21:25:38.369574Z",
     "iopub.status.idle": "2026-02-15T21:26:08.726611Z",
     "shell.execute_reply": "2026-02-15T21:26:08.725570Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 4: Simulación Monte Carlo con cambio de régimen (10k trayectorias, horizonte 6M) ---\n",
    "\n",
    "# Parámetros de simulación\n",
    "N_PATHS = 10_000          # Número de trayectorias de simulación\n",
    "HORIZON_DAYS = 126        # Horizonte de simulación (6 meses ~ 126 días hábiles)\n",
    "SEED = 42                 # Semilla para reproducibilidad\n",
    "ALPHA_99 = 0.01           # Nivel de confianza para VaR/ES (99%)\n",
    "EXPORT = True             # Exportar resultados a archivos CSV\n",
    "\n",
    "# Preparación de datos\n",
    "ret = returns.copy()\n",
    "flags = is_crisis.reindex(ret.index).astype(\"boolean\").fillna(False).astype(bool)\n",
    "assets = list(ret.columns)\n",
    "OUT_DIR = Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Función para asegurar matriz de correlación semi-definida positiva\n",
    "def _nearest_psd_corr(C, eps=1e-8):\n",
    "    \"\"\"Aproximar matriz de correlación a la más cercana semi-definida positiva.\"\"\"\n",
    "    C = np.asarray(C, float)\n",
    "    C = 0.5 * (C + C.T)\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    w, V = np.linalg.eigh(C)\n",
    "    w = np.maximum(w, eps)\n",
    "    C2 = (V * w) @ V.T\n",
    "    d = np.sqrt(np.diag(C2))\n",
    "    C2 = C2 / (d[:, None] * d[None, :])\n",
    "    np.fill_diagonal(C2, 1.0)\n",
    "    return 0.5 * (C2 + C2.T)\n",
    "\n",
    "# Función para muestrear desde cópula t\n",
    "def _sample_t_copula_u(n, C, df, rng):\n",
    "    \"\"\"Muestrear variables uniformes desde cópula t.\"\"\"\n",
    "    d = C.shape[0]\n",
    "    C = _nearest_psd_corr(C)\n",
    "    L = np.linalg.cholesky(C)\n",
    "    y = rng.standard_normal((n, d)) @ L.T\n",
    "    w = rng.chisquare(df, size=n) / df\n",
    "    z = y / np.sqrt(w)[:, None]\n",
    "    u = t.cdf(z, df=df)\n",
    "    return np.clip(u, 1e-6, 1 - 1e-6)\n",
    "\n",
    "# Función para transformación inversa empírica\n",
    "def _inv_empirical(u, sample):\n",
    "    \"\"\"Transformación inversa usando distribución empírica.\"\"\"\n",
    "    x = np.asarray(sample, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) < 20:\n",
    "        return np.full_like(u, np.nan, float)\n",
    "    xs = np.sort(x)\n",
    "    p = np.arange(1, len(xs) + 1) / (len(xs) + 1.0)\n",
    "    return np.interp(u, p, xs)\n",
    "\n",
    "# Cálculo de VaR y ES\n",
    "def _var_es(x, a=0.01):\n",
    "    \"\"\"Calcular VaR y Expected Shortfall.\"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) == 0:\n",
    "        return np.nan, np.nan\n",
    "    q = float(np.quantile(x, a))\n",
    "    tail = x[x <= q]\n",
    "    return q, float(np.mean(tail)) if len(tail) else np.nan\n",
    "\n",
    "# Cálculo de Maximum Drawdown para retornos logarítmicos\n",
    "def _mdd_logret(r):\n",
    "    \"\"\"Calcular Maximum Drawdown para serie de retornos log.\"\"\"\n",
    "    w = np.exp(np.cumsum(np.asarray(r, float)))\n",
    "    peak = np.maximum.accumulate(w)\n",
    "    return float(np.min(w / peak - 1.0))\n",
    "\n",
    "# Cargar parámetros de cópula t desde fase anterior\n",
    "def _load_params():\n",
    "    \"\"\"Cargar parámetros de cópula t ajustados en fase 3.\"\"\"\n",
    "    if 'fits' in globals() and isinstance(globals().get('fits'), dict) and len(fits) > 0:\n",
    "        if 'Normal' in fits and 'Estrés' in fits:\n",
    "            return {\n",
    "                'Normal': {\n",
    "                    'df': float(fits['Normal']['t_df']),\n",
    "                    'Ct': np.asarray(fits['Normal']['t_corr'], float)\n",
    "                },\n",
    "                'Estrés': {\n",
    "                    'df': float(fits['Estrés']['t_df']),\n",
    "                    'Ct': np.asarray(fits['Estrés']['t_corr'], float)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Cargar desde archivos si no están en memoria\n",
    "    s = json.loads((OUT_DIR / 'phase3_copula_summary.json').read_text(encoding='utf-8'))\n",
    "    out = {}\n",
    "    for nm, slug in [('Normal', 'normal'), ('Estrés', 'estres')]:\n",
    "        out[nm] = {\n",
    "            'df': float(s[nm]['t_df']),\n",
    "            'Ct': pd.read_csv(OUT_DIR / f'phase3_copula_t_corr_{slug}.csv', index_col=0)\n",
    "                .loc[assets, assets].to_numpy()\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# Muestrear retornos para un estado específico\n",
    "def _sample_state(n, Ct, df, hist_df, rng):\n",
    "    \"\"\"Muestrear retornos para un estado específico usando cópula t.\"\"\"\n",
    "    if n <= 0:\n",
    "        return np.empty((0, len(assets)))\n",
    "    \n",
    "    hist = hist_df[assets].dropna(how='any')\n",
    "    U = _sample_t_copula_u(n, Ct, df, rng)\n",
    "    R = np.zeros((n, len(assets)))\n",
    "    \n",
    "    for j, a in enumerate(assets):\n",
    "        R[:, j] = _inv_empirical(U[:, j], hist[a].to_numpy())\n",
    "    \n",
    "    return R\n",
    "\n",
    "# Estadísticas de regímenes\n",
    "def _state_stats(s01):\n",
    "    \"\"\"Calcular estadísticas de regímenes para una secuencia de estados.\"\"\"\n",
    "    x = np.asarray(s01, np.int8)\n",
    "    if x.size == 0:\n",
    "        return dict(\n",
    "            pct_normal=np.nan, pct_estres=np.nan,\n",
    "            avg_dur_normal=np.nan, avg_dur_estres=np.nan,\n",
    "            n_switches=np.nan\n",
    "        )\n",
    "    \n",
    "    runs = []\n",
    "    cur = int(x[0])\n",
    "    ln = 1\n",
    "    \n",
    "    for v in x[1:]:\n",
    "        v = int(v)\n",
    "        if v == cur:\n",
    "            ln += 1\n",
    "        else:\n",
    "            runs.append((cur, ln))\n",
    "            cur = v\n",
    "            ln = 1\n",
    "    \n",
    "    runs.append((cur, ln))\n",
    "    d0 = [l for s, l in runs if s == 0]\n",
    "    d1 = [l for s, l in runs if s == 1]\n",
    "    \n",
    "    return dict(\n",
    "        pct_normal=float(np.mean(x == 0)),\n",
    "        pct_estres=float(np.mean(x == 1)),\n",
    "        avg_dur_normal=float(np.mean(d0)) if d0 else np.nan,\n",
    "        avg_dur_estres=float(np.mean(d1)) if d1 else np.nan,\n",
    "        n_switches=float(np.sum(x[1:] != x[:-1])) if x.size > 1 else 0.0\n",
    "    )\n",
    "\n",
    "# Simular trayectorias de estados\n",
    "def _simulate_states(n_paths, horizon, P, pi, rng):\n",
    "    \"\"\"Simular trayectorias de estados usando cadena de Markov.\"\"\"\n",
    "    S = np.zeros((n_paths, horizon), dtype=np.int8)\n",
    "    S[:, 0] = (rng.random(n_paths) < float(pi[1])).astype(np.int8)\n",
    "    \n",
    "    for t in range(1, horizon):\n",
    "        prev = S[:, t - 1]\n",
    "        p1 = np.where(prev == 0, P[0, 1], P[1, 1])\n",
    "        S[:, t] = (rng.random(n_paths) < p1).astype(np.int8)\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Cargar parámetros y preparar datos históricos por estado\n",
    "params_by_state = _load_params()\n",
    "hist_by_state = {\n",
    "    'Normal': ret.loc[~flags, assets].dropna(how='any'),\n",
    "    'Estrés': ret.loc[flags, assets].dropna(how='any')\n",
    "}\n",
    "\n",
    "# Validar suficiencia de datos históricos\n",
    "if len(hist_by_state['Normal']) < 100 or len(hist_by_state['Estrés']) < 100:\n",
    "    raise ValueError(\n",
    "        f\"Datos históricos insuficientes por estado. \"\n",
    "        f\"Normal={len(hist_by_state['Normal'])}, Estrés={len(hist_by_state['Estrés'])}\"\n",
    "    )\n",
    "\n",
    "# Calibración de matriz de transición para simulación\n",
    "# Por defecto se usan transiciones empíricas de la serie histórica etiquetada\n",
    "# porque reproducen mejor frecuencias y duraciones de régimen\n",
    "USE_EMPIRICAL_TRANSITIONS = True\n",
    "\n",
    "if USE_EMPIRICAL_TRANSITIONS:\n",
    "    s = flags.astype(int).to_numpy()\n",
    "    cnt = np.zeros((2, 2), float)\n",
    "    for a, b in zip(s[:-1], s[1:]):\n",
    "        cnt[int(a), int(b)] += 1.0\n",
    "    \n",
    "    # Suavizado de Laplace para evitar ceros estructurales\n",
    "    P = (cnt + 1.0) / (cnt + 1.0).sum(axis=1, keepdims=True)\n",
    "    Psrc = 'empirical_from_flags'\n",
    "elif 'hmm' in globals() and hasattr(hmm, 'transmat_') and 'crisis_state' in globals():\n",
    "    T = np.asarray(hmm.transmat_, float)\n",
    "    c = int(crisis_state)\n",
    "    n = 1 - c\n",
    "    P = T[np.ix_([n, c], [n, c])]\n",
    "    Psrc = 'hmm.transmat_'\n",
    "else:\n",
    "    s = flags.astype(int).to_numpy()\n",
    "    cnt = np.zeros((2, 2), float)\n",
    "    for a, b in zip(s[:-1], s[1:]):\n",
    "        cnt[int(a), int(b)] += 1.0\n",
    "    P = (cnt + 1.0) / (cnt + 1.0).sum(axis=1, keepdims=True)\n",
    "    Psrc = 'empirical_from_flags_fallback'\n",
    "\n",
    "# Normalizar matriz de transición\n",
    "P = np.asarray(P, float)\n",
    "P = np.maximum(P, 0.0)\n",
    "P = P / np.where(P.sum(axis=1, keepdims=True) <= 1e-12, 1.0, P.sum(axis=1, keepdims=True))\n",
    "\n",
    "# Probabilidades estacionarias\n",
    "pi = np.array([float((~flags).mean()), float(flags.mean())], float)\n",
    "pi = pi / pi.sum()\n",
    "\n",
    "# Mostrar configuración de simulación\n",
    "print('Configuración Fase 4:', {\n",
    "    'paths': N_PATHS,\n",
    "    'horizon_days': HORIZON_DAYS,\n",
    "    'transition_source': Psrc,\n",
    "    'pi': [float(pi[0]), float(pi[1])]\n",
    "})\n",
    "\n",
    "display(pd.DataFrame(P, index=['from_Normal', 'from_Estrés'], columns=['to_Normal', 'to_Estrés']))\n",
    "\n",
    "# Simular trayectorias de estados\n",
    "rng = np.random.default_rng(SEED)\n",
    "S_paths = _simulate_states(N_PATHS, HORIZON_DAYS, P, pi, rng)\n",
    "\n",
    "# Muestrear retornos según estado\n",
    "S_flat = S_paths.reshape(-1)\n",
    "idxN = np.flatnonzero(S_flat == 0)  # Índices estado Normal\n",
    "idxS = np.flatnonzero(S_flat == 1)  # Índices estado Estrés\n",
    "\n",
    "R_flat = np.zeros((S_flat.size, len(assets)), float)\n",
    "\n",
    "# Muestrear para estado Normal\n",
    "R_flat[idxN] = _sample_state(\n",
    "    len(idxN), \n",
    "    params_by_state['Normal']['Ct'], \n",
    "    params_by_state['Normal']['df'], \n",
    "    hist_by_state['Normal'], \n",
    "    rng\n",
    ")\n",
    "\n",
    "# Muestrear para estado Estrés\n",
    "R_flat[idxS] = _sample_state(\n",
    "    len(idxS), \n",
    "    params_by_state['Estrés']['Ct'], \n",
    "    params_by_state['Estrés']['df'], \n",
    "    hist_by_state['Estrés'], \n",
    "    rng\n",
    ")\n",
    "\n",
    "# Reorganizar en trayectorias\n",
    "R_paths = R_flat.reshape(N_PATHS, HORIZON_DAYS, len(assets))\n",
    "\n",
    "# Cartera igualmente ponderada\n",
    "w = np.ones(len(assets), float) / len(assets)\n",
    "rp_paths = np.einsum('ntd,d->nt', R_paths, w)\n",
    "\n",
    "# Calcular riqueza acumulada\n",
    "wealth_paths = np.exp(np.cumsum(rp_paths, axis=1))\n",
    "\n",
    "# Comparar con ventana real de 6 meses\n",
    "rp_real = (ret[assets] @ w).dropna()\n",
    "rp_real_last = rp_real.iloc[-HORIZON_DAYS:].to_numpy()\n",
    "wealth_real_last = np.exp(np.cumsum(rp_real_last))\n",
    "\n",
    "# Retornos de 6 meses (real vs simulado)\n",
    "real_6m = np.exp(rp_real.rolling(HORIZON_DAYS).sum().dropna().to_numpy()) - 1.0\n",
    "sim_6m = np.exp(rp_paths.sum(axis=1)) - 1.0\n",
    "\n",
    "# Visualizar bandas de riqueza\n",
    "bands = pd.DataFrame({\n",
    "    'day': np.arange(1, HORIZON_DAYS + 1),\n",
    "    'p5': np.percentile(wealth_paths, 5, axis=0),\n",
    "    'p50': np.percentile(wealth_paths, 50, axis=0),\n",
    "    'p95': np.percentile(wealth_paths, 95, axis=0),\n",
    "    'real_last_window': wealth_real_last\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 5))\n",
    "ax.fill_between(bands['day'], bands['p5'], bands['p95'], alpha=0.22, color='#4C78A8', label='Sim p5-p95')\n",
    "ax.plot(bands['day'], bands['p50'], color='#1f4e79', lw=1.8, label='Sim p50')\n",
    "ax.plot(bands['day'], bands['real_last_window'], color='#E45756', lw=1.6, label='Real (últimos 6 meses)')\n",
    "ax.set_title(f'Fase 4 - Riqueza cartera (n={N_PATHS:,}, h={HORIZON_DAYS}d)')\n",
    "ax.set_xlabel('Día')\n",
    "ax.set_ylabel('Riqueza base=1')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparar distribuciones de retornos diarios\n",
    "rp_sim_daily = rp_paths.reshape(-1)\n",
    "rp_real_daily = rp_real.to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "bins = np.linspace(\n",
    "    np.nanquantile(np.concatenate([rp_real_daily, rp_sim_daily]), 0.001),\n",
    "    np.nanquantile(np.concatenate([rp_real_daily, rp_sim_daily]), 0.999),\n",
    "    100\n",
    ")\n",
    "ax.hist(rp_real_daily, bins=bins, density=True, alpha=0.45, label='Real daily', color='#72B7B2')\n",
    "ax.hist(rp_sim_daily, bins=bins, density=True, alpha=0.45, label='Sim daily', color='#F58518')\n",
    "ax.set_title('Fase 4 - Distribución retornos diarios cartera (real vs sim)')\n",
    "ax.set_xlabel('Retorno log diario')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analizar reproducción de regímenes\n",
    "reg_real = _state_stats(flags.astype(int).to_numpy())\n",
    "path_stats = [_state_stats(S_paths[i]) for i in range(S_paths.shape[0])]\n",
    "reg_sim = {k: float(np.nanmean([ps[k] for ps in path_stats])) for k in path_stats[0].keys()}\n",
    "\n",
    "# Tasas de cambio de régimen (anualizadas)\n",
    "real_len = len(flags)\n",
    "sim_len = HORIZON_DAYS\n",
    "switch_rate_real = float(reg_real['n_switches'] / max(real_len - 1, 1) * 252.0)\n",
    "switch_rate_sim = float(np.nanmean([\n",
    "    ps['n_switches'] / max(sim_len - 1, 1) * 252.0 \n",
    "    for ps in path_stats\n",
    "]))\n",
    "\n",
    "regime_tbl = pd.DataFrame([\n",
    "    {'dataset': 'Real', **reg_real, 'switches_252d': switch_rate_real},\n",
    "    {'dataset': 'Simulado (promedio paths)', **reg_sim, 'switches_252d': switch_rate_sim}\n",
    "])\n",
    "\n",
    "display(regime_tbl)\n",
    "print(\n",
    "    \"Nota: n_switches depende del tamaño de ventana. \"\n",
    "    \"Para comparar de forma homogénea, usar switches_252d.\"\n",
    ")\n",
    "\n",
    "# Calcular Maximum Drawdown simulado\n",
    "mdd_sim = np.array([_mdd_logret(rp_paths[i]) for i in range(N_PATHS)], float)\n",
    "\n",
    "# MDD comparable a 6 meses: real en ventanas rolling vs simulado por trayectoria\n",
    "if len(rp_real_daily) >= HORIZON_DAYS:\n",
    "    real_roll = np.lib.stride_tricks.sliding_window_view(rp_real_daily, HORIZON_DAYS)\n",
    "    mdd_real_6m = np.array([_mdd_logret(real_roll[i]) for i in range(real_roll.shape[0])], float)\n",
    "else:\n",
    "    mdd_real_6m = np.array([_mdd_logret(rp_real_daily)], float)\n",
    "\n",
    "# Calcular métricas de riesgo\n",
    "var99_rd, es99_rd = _var_es(rp_real_daily, ALPHA_99)\n",
    "var99_sd, es99_sd = _var_es(rp_sim_daily, ALPHA_99)\n",
    "var99_r6, es99_r6 = _var_es(real_6m, ALPHA_99)\n",
    "var99_s6, es99_s6 = _var_es(sim_6m, ALPHA_99)\n",
    "\n",
    "risk_tbl = pd.DataFrame([\n",
    "    {\n",
    "        'dataset': 'Real',\n",
    "        'vol_daily': float(np.std(rp_real_daily, ddof=1)),\n",
    "        'vol_ann': float(np.std(rp_real_daily, ddof=1) * np.sqrt(252)),\n",
    "        'max_drawdown_6m_mean': float(np.mean(mdd_real_6m)),\n",
    "        'max_drawdown_6m_p95': float(np.quantile(mdd_real_6m, 0.95)),\n",
    "        'max_drawdown_fullsample': float(_mdd_logret(rp_real_daily)),\n",
    "        'VaR99_daily': float(var99_rd),\n",
    "        'ES99_daily': float(es99_rd),\n",
    "        'VaR99_6m': float(var99_r6),\n",
    "        'ES99_6m': float(es99_r6)\n",
    "    },\n",
    "    {\n",
    "        'dataset': 'Simulado',\n",
    "        'vol_daily': float(np.std(rp_sim_daily, ddof=1)),\n",
    "        'vol_ann': float(np.std(rp_sim_daily, ddof=1) * np.sqrt(252)),\n",
    "        'max_drawdown_6m_mean': float(np.mean(mdd_sim)),\n",
    "        'max_drawdown_6m_p95': float(np.quantile(mdd_sim, 0.95)),\n",
    "        'max_drawdown_fullsample': np.nan,\n",
    "        'VaR99_daily': float(var99_sd),\n",
    "        'ES99_daily': float(es99_sd),\n",
    "        'VaR99_6m': float(var99_s6),\n",
    "        'ES99_6m': float(es99_s6)\n",
    "    }\n",
    "])\n",
    "\n",
    "display(risk_tbl)\n",
    "print(\n",
    "    \"Nota: max_drawdown_6m_* es la comparación homogénea real vs simulado a 6 meses; \"\n",
    "    \"max_drawdown_fullsample se reporta solo como referencia histórica real.\"\n",
    ")\n",
    "\n",
    "# Validar dependencia por estado\n",
    "simN = pd.DataFrame(R_flat[idxN], columns=assets)\n",
    "simS = pd.DataFrame(R_flat[idxS], columns=assets)\n",
    "\n",
    "def _avg_corr(df):\n",
    "    \"\"\"Calcular correlación promedio entre activos.\"\"\"\n",
    "    C = df.corr().to_numpy(float)\n",
    "    iu = np.triu_indices(C.shape[0], k=1)\n",
    "    v = C[iu]\n",
    "    v = v[np.isfinite(v)]\n",
    "    return float(np.mean(v)) if len(v) else np.nan\n",
    "\n",
    "dep_tbl = pd.DataFrame([\n",
    "    {\n",
    "        'state': 'Normal',\n",
    "        'n_hist': int(len(hist_by_state['Normal'])),\n",
    "        'n_sim': int(len(simN)),\n",
    "        'avg_vol_hist': float(hist_by_state['Normal'].std(ddof=1).mean()),\n",
    "        'avg_vol_sim': float(simN.std(ddof=1).mean()),\n",
    "        'avg_corr_hist': float(_avg_corr(hist_by_state['Normal'])),\n",
    "        'avg_corr_sim': float(_avg_corr(simN))\n",
    "    },\n",
    "    {\n",
    "        'state': 'Estrés',\n",
    "        'n_hist': int(len(hist_by_state['Estrés'])),\n",
    "        'n_sim': int(len(simS)),\n",
    "        'avg_vol_hist': float(hist_by_state['Estrés'].std(ddof=1).mean()),\n",
    "        'avg_vol_sim': float(simS.std(ddof=1).mean()),\n",
    "        'avg_corr_hist': float(_avg_corr(hist_by_state['Estrés'])),\n",
    "        'avg_corr_sim': float(_avg_corr(simS))\n",
    "    }\n",
    "])\n",
    "\n",
    "# Resumen de diferencias entre estados\n",
    "dep_summary = pd.DataFrame([\n",
    "    {\n",
    "        'metric': 'vol_ratio_estres_vs_normal',\n",
    "        'hist': float(\n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Estrés', 'avg_vol_hist'].iloc[0] / \n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Normal', 'avg_vol_hist'].iloc[0]\n",
    "        ),\n",
    "        'sim': float(\n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Estrés', 'avg_vol_sim'].iloc[0] / \n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Normal', 'avg_vol_sim'].iloc[0]\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        'metric': 'delta_avg_corr_estres_minus_normal',\n",
    "        'hist': float(\n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Estrés', 'avg_corr_hist'].iloc[0] - \n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Normal', 'avg_corr_hist'].iloc[0]\n",
    "        ),\n",
    "        'sim': float(\n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Estrés', 'avg_corr_sim'].iloc[0] - \n",
    "            dep_tbl.loc[dep_tbl['state'] == 'Normal', 'avg_corr_sim'].iloc[0]\n",
    "        )\n",
    "    }\n",
    "])\n",
    "\n",
    "display(dep_tbl)\n",
    "display(dep_summary)\n",
    "\n",
    "# Exportar resultados si se solicita\n",
    "if EXPORT:\n",
    "    bands.to_csv(OUT_DIR / f'phase4_wealth_bands_6m_n{N_PATHS}.csv', index=False)\n",
    "    regime_tbl.to_csv(OUT_DIR / f'phase4_regime_reproduction_6m_n{N_PATHS}.csv', index=False)\n",
    "    risk_tbl.to_csv(OUT_DIR / f'phase4_portfolio_risk_validation_6m_n{N_PATHS}.csv', index=False)\n",
    "    dep_tbl.to_csv(OUT_DIR / f'phase4_dependence_validation_6m_n{N_PATHS}.csv', index=False)\n",
    "    dep_summary.to_csv(OUT_DIR / f'phase4_dependence_summary_6m_n{N_PATHS}.csv', index=False)\n",
    "    pd.DataFrame(P, index=['from_Normal', 'from_Estrés'], columns=['to_Normal', 'to_Estrés']).to_csv(\n",
    "        OUT_DIR / 'phase4_transition_matrix_normal_estres.csv'\n",
    "    )\n",
    "    print('Resultados de Fase 4 guardados en', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570ab2b",
   "metadata": {},
   "source": [
    "## Motor de Simulación Avanzado (10k trayectorias, 6 meses)\n",
    "\n",
    "### **Configuración y Resultados**\n",
    "- **Parámetros**: 10,000 trayectorias, 126 días (6 meses), semilla 42\n",
    "- **Transiciones**: Matriz empírica con alta persistencia (>99%)\n",
    "- **Cópulas**: t-cópula con df=10 (Normal) y df=7 (Estrés)\n",
    "\n",
    "### **Validación de Regímenes**\n",
    "- **Reproducción exitosa**: pct_normal≈86% (real vs simulado)\n",
    "- **Persistencia**: Duraciones promedio consistentes entre estados\n",
    "- **Tasa de cambios**: switches_252d similares (0.32 vs 0.42)\n",
    "\n",
    "### **Métricas de Riesgo Validadas**\n",
    "- **Volatilidad**: 17.6% (real) vs 18.0% (simulado)\n",
    "- **VaR99 diario**: -3.20% (real) vs -3.14% (simulado)\n",
    "- **MDD 6m**: -9.7% (real) vs -10.7% (simulado)\n",
    "\n",
    "### **Dependencia por Estado**\n",
    "- **Vol ratio Estrés/Normal**: 1.64 (hist) vs 1.66 (sim)\n",
    "- **Delta correlación**: +0.082 (hist) vs +0.086 (sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5827993",
   "metadata": {},
   "source": [
    "## 5) Fase 5 - Escenarios de estrés (10.000 trayectorias, 6 meses)\n",
    "\n",
    "Escenarios ejecutados:\n",
    "\n",
    "- Escenario 1: Estanflación 2022\n",
    "- Escenario 2: Crisis de crédito 2008\n",
    "- Escenario 3: Alternativo de liquidez global\n",
    "\n",
    "Se reportan VaR 99% y ES/CVaR 99% para comparacion base vs escenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe34801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:26:08.734801Z",
     "iopub.status.busy": "2026-02-15T21:26:08.733748Z",
     "iopub.status.idle": "2026-02-15T21:28:26.843273Z",
     "shell.execute_reply": "2026-02-15T21:28:26.841225Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 5: Escenarios de estrés (10k trayectorias, horizonte 6 meses) ---\n",
    "\n",
    "# Parámetros de simulación de estrés\n",
    "N_STRESS = 10_000              # Número de trayectorias para escenarios de estrés\n",
    "HORIZON_STRESS_DAYS = 126     # Horizonte de simulación (6 meses ~ 126 días hábiles)\n",
    "SEED_STRESS = 123             # Semilla para reproducibilidad de escenarios\n",
    "EXPORT_STRESS = True          # Exportar resultados de estrés a archivos CSV\n",
    "ALPHA_99 = 0.01               # Nivel de confianza para VaR/ES (99%)\n",
    "\n",
    "# Verificar prerequisitos desde fase anterior\n",
    "required = ['assets', 'w', 'P', 'pi', 'params_by_state', 'hist_by_state', \n",
    "            '_simulate_states', '_sample_state', '_var_es', '_mdd_logret', '_nearest_psd_corr']\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f'Faltan prerequisitos para Fase 5: {missing}. Ejecutar Fase 4 primero.')\n",
    "\n",
    "# Configuración de directorios y datos\n",
    "OUT_DIR = Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ret = returns.copy()\n",
    "assets = list(assets)\n",
    "w = np.asarray(w, float)\n",
    "\n",
    "# Parámetros base del estado de estrés\n",
    "base_df = float(params_by_state['Estrés']['df'])\n",
    "base_Ct = np.asarray(params_by_state['Estrés']['Ct'], float)\n",
    "histS = hist_by_state['Estrés'].copy()\n",
    "\n",
    "# Función para muestrear desde cópula t (redefinida para esta fase)\n",
    "def _sample_t_copula_u(n, C, df, rng):\n",
    "    \"\"\"Muestrear variables uniformes desde cópula t.\"\"\"\n",
    "    d = C.shape[0]\n",
    "    C = _nearest_psd_corr(C)\n",
    "    L = np.linalg.cholesky(C)\n",
    "    y = rng.standard_normal((n, d)) @ L.T\n",
    "    wchi = rng.chisquare(df, size=n) / df\n",
    "    z = y / np.sqrt(wchi)[:, None]\n",
    "    from scipy.stats import t as _t\n",
    "    u = _t.cdf(z, df=df)\n",
    "    return np.clip(u, 1e-6, 1 - 1e-6)\n",
    "\n",
    "# Transformación inversa empírica (redefinida para esta fase)\n",
    "def _inv_empirical(u, sample):\n",
    "    \"\"\"Transformación inversa usando distribución empírica.\"\"\"\n",
    "    x = np.asarray(sample, float)\n",
    "    x = x[np.isfinite(x)]\n",
    "    if len(x) < 20:\n",
    "        return np.full_like(u, np.nan, float)\n",
    "    xs = np.sort(x)\n",
    "    p = np.arange(1, len(xs) + 1) / (len(xs) + 1.0)\n",
    "    return np.interp(u, p, xs)\n",
    "\n",
    "# Simular retornos desde cópula para un estado\n",
    "def _simulate_state_from_copula(n, Ct, df, hist, rng):\n",
    "    \"\"\"Simular retornos para un estado específico usando cópula t.\"\"\"\n",
    "    hist = hist.loc[:, assets].dropna(how='any')\n",
    "    U = _sample_t_copula_u(n, Ct, df, rng)\n",
    "    R = np.zeros((n, len(assets)), float)\n",
    "    \n",
    "    for j, a in enumerate(assets):\n",
    "        R[:, j] = _inv_empirical(U[:, j], hist[a].to_numpy())\n",
    "    \n",
    "    return R\n",
    "\n",
    "# Shock de dependencia por contagio\n",
    "def _shock_dependence_contagion(C, risky_idx, target_rho=0.85, alpha=0.60):\n",
    "    \"\"\"Aplicar shock de dependencia aumentando correlaciones entre activos de riesgo.\"\"\"\n",
    "    C2 = np.array(C, float, copy=True)\n",
    "    \n",
    "    for i in risky_idx:\n",
    "        for j in risky_idx:\n",
    "            if i != j:\n",
    "                C2[i, j] = (1 - alpha) * C2[i, j] + alpha * target_rho\n",
    "    \n",
    "    np.fill_diagonal(C2, 1.0)\n",
    "    return _nearest_psd_corr(C2)\n",
    "\n",
    "# Shock de correlaciones por pares específicos\n",
    "def _tweak_pairs(C, pairs, alpha=0.70):\n",
    "    \"\"\"Ajustar correlaciones para pares específicos de activos.\"\"\"\n",
    "    C2 = np.array(C, float, copy=True)\n",
    "    \n",
    "    for a, b, target in pairs:\n",
    "        if a in assets and b in assets:\n",
    "            i, j = assets.index(a), assets.index(b)\n",
    "            C2[i, j] = (1 - alpha) * C2[i, j] + alpha * float(target)\n",
    "            C2[j, i] = C2[i, j]\n",
    "    \n",
    "    np.fill_diagonal(C2, 1.0)\n",
    "    return _nearest_psd_corr(C2)\n",
    "\n",
    "# Shock de volatilidad y media marginal\n",
    "def _shock_marginal_vol_mean(R, vol_mult=1.50, mean_shift=0.0, cols=None):\n",
    "    \"\"\"Aplicar shock de volatilidad y desplazamiento de media.\"\"\"\n",
    "    X = np.array(R, float, copy=True)\n",
    "    if cols is None:\n",
    "        cols = list(range(X.shape[1]))\n",
    "    \n",
    "    mu = np.nanmean(X[:, cols], axis=0)\n",
    "    X[:, cols] = (mu + vol_mult * (X[:, cols] - mu)) + float(mean_shift)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Shock de cola izquierda (aumentar pérdidas extremas)\n",
    "def _shock_tail_left(R, q=0.01, tail_mult=1.50, cols=None):\n",
    "    \"\"\"Aumentar severidad de pérdidas en cola izquierda.\"\"\"\n",
    "    X = np.array(R, float, copy=True)\n",
    "    if cols is None:\n",
    "        cols = list(range(X.shape[1]))\n",
    "    \n",
    "    for j in cols:\n",
    "        qv = float(np.quantile(X[:, j], q))\n",
    "        m = X[:, j] <= qv\n",
    "        X[m, j] = qv + float(tail_mult) * (X[m, j] - qv)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Métricas de cartera para un conjunto de retornos\n",
    "def _portfolio_metrics(R, w):\n",
    "    \"\"\"Calcular métricas de riesgo para cartera.\"\"\"\n",
    "    rp = R @ w\n",
    "    v5, e5 = _var_es(rp, 0.05)\n",
    "    v1, e1 = _var_es(rp, 0.01)\n",
    "    \n",
    "    return {\n",
    "        'VaR5': float(v5), 'ES5': float(e5),\n",
    "        'VaR1': float(v1), 'ES1': float(e1),\n",
    "        'mean': float(np.mean(rp)),\n",
    "        'vol': float(np.std(rp, ddof=1))\n",
    "    }\n",
    "\n",
    "# Métricas de cartera para múltiples trayectorias\n",
    "def _portfolio_metrics_paths(rp_paths):\n",
    "    \"\"\"Calcular métricas de riesgo para trayectorias de cartera.\"\"\"\n",
    "    daily = rp_paths.reshape(-1)\n",
    "    sixm = np.exp(np.sum(rp_paths, axis=1)) - 1.0\n",
    "    mdd = np.array([_mdd_logret(rp_paths[i]) for i in range(rp_paths.shape[0])], float)\n",
    "    \n",
    "    v99d, e99d = _var_es(daily, ALPHA_99)\n",
    "    v99m, e99m = _var_es(sixm, ALPHA_99)\n",
    "    \n",
    "    return {\n",
    "        'vol_daily': float(np.std(daily, ddof=1)),\n",
    "        'vol_ann': float(np.std(daily, ddof=1) * np.sqrt(252)),\n",
    "        'max_drawdown_mean': float(np.mean(mdd)),\n",
    "        'max_drawdown_p95': float(np.quantile(mdd, 0.95)),\n",
    "        'VaR99_daily': float(v99d),\n",
    "        'ES99_daily': float(e99d),\n",
    "        'VaR99_6m': float(v99m),\n",
    "        'ES99_6m': float(e99m)\n",
    "    }\n",
    "\n",
    "# Identificar activos de riesgo y defensivos\n",
    "if 'EQUITY_TICKERS' in globals() and EQUITY_TICKERS:\n",
    "    EQUITY_CORE = [t for t in EQUITY_TICKERS if t in assets]\n",
    "else:\n",
    "    EQUITY_CORE = [t for t in assets if t not in ['GLD', 'IEF', 'SHY', 'HYG']]\n",
    "\n",
    "RISKY = [t for t in (EQUITY_CORE + ['HYG']) if t in assets]\n",
    "risky_idx = [assets.index(a) for a in RISKY]\n",
    "\n",
    "# Definición de escenarios de estrés\n",
    "scenario_specs = [\n",
    "    {\n",
    "        'name': 'S1_Estanflacion_2022',\n",
    "        'label': 'Escenario 1 (Estanflacion 2022)',\n",
    "        'desc': 'Inflación persistente: equity y bonos corrigen juntos, volatilidad elevada.',\n",
    "        'df': max(3.0, base_df * 0.90),\n",
    "        'Ct': np.array(base_Ct, copy=True),\n",
    "        'vol_mult': 1.35,\n",
    "        'tail_mult': 1.25,\n",
    "        'mean_shift': -0.0004,\n",
    "        'apply_cols': list(range(len(assets)))\n",
    "    },\n",
    "    {\n",
    "        'name': 'S2_Crisis_Credito_2008',\n",
    "        'label': 'Escenario 2 (Crisis de Credito 2008)',\n",
    "        'desc': 'Ruptura de crédito: HYG y equity altamente sincronizados, cola izquierda severa.',\n",
    "        'df': max(3.0, base_df * 0.65),\n",
    "        'Ct': np.array(base_Ct, copy=True),\n",
    "        'vol_mult': 1.55,\n",
    "        'tail_mult': 1.70,\n",
    "        'mean_shift': -0.0008,\n",
    "        'apply_cols': [assets.index('HYG')] + [assets.index(x) for x in EQUITY_CORE if x in assets] \n",
    "        if 'HYG' in assets else list(range(len(assets)))\n",
    "    },\n",
    "    {\n",
    "        'name': 'S3_Alternativo_Liquidez_Global',\n",
    "        'label': 'Escenario 3 (Alternativo: Contagio de liquidez)',\n",
    "        'desc': 'Shock de liquidez transversal con aumento de correlaciones y colas en todos los bloques.',\n",
    "        'df': max(3.0, base_df * 0.80),\n",
    "        'Ct': np.array(base_Ct, copy=True),\n",
    "        'vol_mult': 1.45,\n",
    "        'tail_mult': 1.45,\n",
    "        'mean_shift': -0.0006,\n",
    "        'apply_cols': list(range(len(assets)))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Definir pares de activos para shocks específicos\n",
    "pairs_eq_bond = []\n",
    "for eq in EQUITY_CORE:\n",
    "    if 'IEF' in assets:\n",
    "        pairs_eq_bond.append((eq, 'IEF', 0.25))\n",
    "    if 'SHY' in assets:\n",
    "        pairs_eq_bond.append((eq, 'SHY', 0.15))\n",
    "\n",
    "pairs_hyg_eq = [('HYG', eq, 0.85) for eq in EQUITY_CORE] if 'HYG' in assets else []\n",
    "pairs_liq = [(a, 'GLD', 0.25) for a in RISKY if 'GLD' in assets]\n",
    "\n",
    "# Aplicar shocks de dependencia específicos para cada escenario\n",
    "for spec in scenario_specs:\n",
    "    if spec['name'] == 'S1_Estanflacion_2022':\n",
    "        # Estanflación: correlaciones moderadas entre equity-bonos\n",
    "        C1 = _shock_dependence_contagion(base_Ct, risky_idx, target_rho=0.75, alpha=0.45)\n",
    "        spec['Ct'] = _tweak_pairs(C1, pairs_eq_bond, alpha=0.70)\n",
    "    \n",
    "    elif spec['name'] == 'S2_Crisis_Credito_2008':\n",
    "        # Crisis de crédito: alta correlación HYG-equity\n",
    "        C2 = _shock_dependence_contagion(base_Ct, risky_idx, target_rho=0.90, alpha=0.75)\n",
    "        spec['Ct'] = _tweak_pairs(C2, pairs_hyg_eq, alpha=0.80)\n",
    "    \n",
    "    else:\n",
    "        # Contagio de liquidez: correlaciones elevadas transversales\n",
    "        C3 = _shock_dependence_contagion(base_Ct, risky_idx, target_rho=0.82, alpha=0.60)\n",
    "        spec['Ct'] = _tweak_pairs(C3, pairs_liq, alpha=0.55)\n",
    "\n",
    "# Función para simular trayectorias bajo especificación de escenario\n",
    "def _simulate_paths_under_spec(spec, n_paths, horizon, seed):\n",
    "    \"\"\"Simular trayectorias bajo una especificación de escenario de estrés.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Simular estados\n",
    "    S = _simulate_states(n_paths, horizon, P, pi, rng)\n",
    "    sf = S.reshape(-1)\n",
    "    \n",
    "    # Índices por estado\n",
    "    idxN = np.flatnonzero(sf == 0)\n",
    "    idxS = np.flatnonzero(sf == 1)\n",
    "    \n",
    "    # Inicializar retornos\n",
    "    Rf = np.zeros((sf.size, len(assets)), float)\n",
    "    \n",
    "    # Muestrear estado Normal (sin shock)\n",
    "    Rf[idxN] = _sample_state(\n",
    "        len(idxN), \n",
    "        params_by_state['Normal']['Ct'], \n",
    "        params_by_state['Normal']['df'], \n",
    "        hist_by_state['Normal'], \n",
    "        rng\n",
    "    )\n",
    "    \n",
    "    # Muestrear estado Estrés con shocks\n",
    "    Rs = _sample_state(len(idxS), spec['Ct'], spec['df'], hist_by_state['Estrés'], rng)\n",
    "    \n",
    "    # Aplicar shocks marginales\n",
    "    Rs = _shock_marginal_vol_mean(\n",
    "        Rs, \n",
    "        vol_mult=float(spec['vol_mult']), \n",
    "        mean_shift=float(spec.get('mean_shift', 0.0)), \n",
    "        cols=spec.get('apply_cols')\n",
    "    )\n",
    "    Rs = _shock_tail_left(\n",
    "        Rs, \n",
    "        q=0.01, \n",
    "        tail_mult=float(spec['tail_mult']), \n",
    "        cols=spec.get('apply_cols')\n",
    "    )\n",
    "    \n",
    "    Rf[idxS] = Rs\n",
    "    \n",
    "    # Reorganizar y calcular métricas de cartera\n",
    "    Rp = Rf.reshape(n_paths, horizon, len(assets))\n",
    "    rpp = np.einsum('ntd,d->nt', Rp, w)\n",
    "    W = np.exp(np.cumsum(rpp, axis=1))\n",
    "    \n",
    "    return {\n",
    "        'S_paths': S, \n",
    "        'R_paths': Rp, \n",
    "        'rp_paths': rpp, \n",
    "        'wealth_paths': W, \n",
    "        'R_flat': Rf\n",
    "    }\n",
    "\n",
    "# Escenario base sin shocks adicionales\n",
    "base_spec = {\n",
    "    'name': 'BASE_HMM_COPULA',\n",
    "    'label': 'Base (HMM + copulas por estado)',\n",
    "    'desc': 'Simulación base sin shock adicional.',\n",
    "    'df': base_df,\n",
    "    'Ct': np.array(base_Ct, copy=True),\n",
    "    'vol_mult': 1.0,\n",
    "    'tail_mult': 1.0,\n",
    "    'mean_shift': 0.0,\n",
    "    'apply_cols': None\n",
    "}\n",
    "\n",
    "# Combinar escenario base con escenarios de estrés\n",
    "all_specs = [base_spec] + scenario_specs\n",
    "scenario_results = {}\n",
    "rows = []\n",
    "\n",
    "# Ejecutar simulaciones para todos los escenarios\n",
    "for i, spec in enumerate(all_specs):\n",
    "    out = _simulate_paths_under_spec(spec, N_STRESS, HORIZON_STRESS_DAYS, SEED_STRESS + i)\n",
    "    scenario_results[spec['name']] = out\n",
    "    \n",
    "    # Calcular métricas de cartera\n",
    "    m = _portfolio_metrics_paths(out['rp_paths'])\n",
    "    \n",
    "    # Estadísticas de regímenes\n",
    "    reg = np.array([\n",
    "        _state_stats(out['S_paths'][k])['pct_estres'] \n",
    "        for k in range(out['S_paths'].shape[0])\n",
    "    ], float)\n",
    "    dur = np.array([\n",
    "        _state_stats(out['S_paths'][k])['avg_dur_estres'] \n",
    "        for k in range(out['S_paths'].shape[0])\n",
    "    ], float)\n",
    "    sw = np.array([\n",
    "        _state_stats(out['S_paths'][k])['n_switches'] \n",
    "        for k in range(out['S_paths'].shape[0])\n",
    "    ], float)\n",
    "    \n",
    "    # Correlación promedio en estado de estrés\n",
    "    sf = out['S_paths'].reshape(-1)\n",
    "    idxS = np.flatnonzero(sf == 1)\n",
    "    stress_df = pd.DataFrame(out['R_flat'][idxS], columns=assets)\n",
    "    C = stress_df.corr().to_numpy(float)\n",
    "    iu = np.triu_indices(C.shape[0], k=1)\n",
    "    v = C[iu]\n",
    "    v = v[np.isfinite(v)]\n",
    "    avg_corr = float(np.mean(v)) if len(v) else np.nan\n",
    "    \n",
    "    rows.append({\n",
    "        'scenario': spec['name'],\n",
    "        'label': spec.get('label', spec['name']),\n",
    "        'desc': spec['desc'],\n",
    "        'n_paths': int(N_STRESS),\n",
    "        'horizon_days': int(HORIZON_STRESS_DAYS),\n",
    "        'df_used_stress': float(spec['df']),\n",
    "        'vol_mult_stress': float(spec['vol_mult']),\n",
    "        'tail_mult_stress': float(spec['tail_mult']),\n",
    "        'mean_shift_stress': float(spec.get('mean_shift', 0.0)),\n",
    "        'pct_estres_sim': float(np.nanmean(reg)),\n",
    "        'avg_dur_estres_sim': float(np.nanmean(dur)),\n",
    "        'n_switches_sim': float(np.nanmean(sw)),\n",
    "        'avg_corr_stress_sim': float(avg_corr),\n",
    "        **m\n",
    "    })\n",
    "\n",
    "# Resumen de escenarios de estrés\n",
    "stress_summary = pd.DataFrame(rows).sort_values('ES99_6m').reset_index(drop=True)\n",
    "display(stress_summary)\n",
    "\n",
    "# Visualizar resultados de escenarios\n",
    "rp_real = (ret[assets] @ w).dropna()\n",
    "rp_real_last = rp_real.iloc[-HORIZON_STRESS_DAYS:].to_numpy()\n",
    "wealth_real_last = np.exp(np.cumsum(rp_real_last))\n",
    "\n",
    "days = np.arange(1, HORIZON_STRESS_DAYS + 1)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Gráfico de riqueza\n",
    "base_w = scenario_results['BASE_HMM_COPULA']['wealth_paths']\n",
    "p5 = np.percentile(base_w, 5, axis=0)\n",
    "p50 = np.percentile(base_w, 50, axis=0)\n",
    "p95 = np.percentile(base_w, 95, axis=0)\n",
    "\n",
    "axes[0].fill_between(days, p5, p95, alpha=0.20, color='#4C78A8', label='Base p5-p95')\n",
    "axes[0].plot(days, p50, color='#1f4e79', lw=2.0, label='Base p50')\n",
    "axes[0].plot(days, wealth_real_last, color='#E45756', lw=1.6, label='Real (últimos 6 meses)')\n",
    "\n",
    "for spec in scenario_specs:\n",
    "    med = np.percentile(scenario_results[spec['name']]['wealth_paths'], 50, axis=0)\n",
    "    axes[0].plot(days, med, lw=1.3, label=spec['name'])\n",
    "\n",
    "axes[0].set_title('Fase 5 - Riqueza cartera (validación 6M)')\n",
    "axes[0].set_xlabel('Día')\n",
    "axes[0].set_ylabel('Riqueza base=1')\n",
    "axes[0].grid(alpha=0.25)\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Gráfico de distribución de retornos 6M\n",
    "for spec in all_specs:\n",
    "    sixm = np.exp(np.sum(scenario_results[spec['name']]['rp_paths'], axis=1)) - 1.0\n",
    "    axes[1].hist(sixm, bins=80, density=True, alpha=0.35, label=spec['name'])\n",
    "\n",
    "axes[1].set_title('Fase 5 - Distribución retorno 6M (base vs estrés)')\n",
    "axes[1].set_xlabel('Retorno simple 6M')\n",
    "axes[1].grid(alpha=0.25)\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de palancas de shock (levers analysis)\n",
    "def _avg_block_corr(C, names):\n",
    "    \"\"\"Calcular correlación promedio para un bloque de activos.\"\"\"\n",
    "    idx = [assets.index(a) for a in names if a in assets]\n",
    "    if len(idx) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    vals = []\n",
    "    for i in range(len(idx)):\n",
    "        for j in range(i + 1, len(idx)):\n",
    "            vals.append(float(C[idx[i], idx[j]]))\n",
    "    \n",
    "    return float(np.mean(vals)) if vals else np.nan\n",
    "\n",
    "def _avg_pair_corr(C, pairs):\n",
    "    \"\"\"Calcular correlación promedio para pares específicos.\"\"\"\n",
    "    vals = []\n",
    "    for a, b in pairs:\n",
    "        if a in assets and b in assets:\n",
    "            i, j = assets.index(a), assets.index(b)\n",
    "            vals.append(float(C[i, j]))\n",
    "    \n",
    "    return float(np.mean(vals)) if vals else np.nan\n",
    "\n",
    "# Definir pares para análisis\n",
    "pairs_bonds = []\n",
    "for eq in EQUITY_CORE:\n",
    "    for b in ['IEF', 'SHY']:\n",
    "        if b in assets:\n",
    "            pairs_bonds.append((eq, b))\n",
    "\n",
    "pairs_hyg = [('HYG', eq) for eq in EQUITY_CORE] if 'HYG' in assets else []\n",
    "\n",
    "# Análisis detallado de palancas\n",
    "lever_rows = []\n",
    "for spec in all_specs:\n",
    "    Ct = np.asarray(spec['Ct'], float)\n",
    "    ac = spec.get('apply_cols')\n",
    "    apply_to = ','.join([assets[i] for i in ac if 0 <= i < len(assets)]) if isinstance(ac, list) else 'all'\n",
    "    \n",
    "    lever_rows.append({\n",
    "        'scenario': spec['name'],\n",
    "        'label': spec.get('label', spec['name']),\n",
    "        'dependence_shock': 'No' if spec['name'] == 'BASE_HMM_COPULA' else 'Si',\n",
    "        'tail_shock': 'Si' if float(spec.get('tail_mult', 1.0)) > 1.0 or float(spec.get('df', base_df)) < float(base_df) else 'No',\n",
    "        'vol_shock': 'Si' if float(spec.get('vol_mult', 1.0)) > 1.0 else 'No',\n",
    "        'drift_shock': 'Si' if abs(float(spec.get('mean_shift', 0.0))) > 0 else 'No',\n",
    "        'df_used': float(spec['df']),\n",
    "        'vol_mult': float(spec['vol_mult']),\n",
    "        'tail_mult': float(spec['tail_mult']),\n",
    "        'mean_shift': float(spec.get('mean_shift', 0.0)),\n",
    "        'apply_to': apply_to,\n",
    "        'avg_corr_risky': _avg_block_corr(Ct, RISKY),\n",
    "        'avg_corr_bonds_hedge': _avg_pair_corr(Ct, pairs_bonds),\n",
    "        'avg_corr_hyg_equity': _avg_pair_corr(Ct, pairs_hyg),\n",
    "        'desc': spec['desc']\n",
    "    })\n",
    "\n",
    "# Tabla de palancas con deltas vs base\n",
    "lever_tbl = pd.DataFrame(lever_rows)\n",
    "base_row = lever_tbl[lever_tbl['scenario'] == 'BASE_HMM_COPULA'].iloc[0]\n",
    "\n",
    "lever_tbl['delta_avg_corr_risky'] = lever_tbl['avg_corr_risky'] - float(base_row['avg_corr_risky'])\n",
    "lever_tbl['delta_avg_corr_bonds_hedge'] = lever_tbl['avg_corr_bonds_hedge'] - float(base_row['avg_corr_bonds_hedge'])\n",
    "lever_tbl['delta_avg_corr_hyg_equity'] = lever_tbl['avg_corr_hyg_equity'] - float(base_row['avg_corr_hyg_equity'])\n",
    "\n",
    "display(lever_tbl)\n",
    "\n",
    "# Exportar resultados de estrés\n",
    "if EXPORT_STRESS:\n",
    "    stress_summary.to_csv(OUT_DIR / f'phase5_stress_summary_6m_n{N_STRESS}.csv', index=False)\n",
    "    lever_tbl.to_csv(OUT_DIR / f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv', index=False)\n",
    "    lever_tbl.to_csv(OUT_DIR / 'phase5_stress_levers_summary.csv', index=False)\n",
    "    print('Resultados de Fase 5 guardados en', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89719e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:28:26.847924Z",
     "iopub.status.busy": "2026-02-15T21:28:26.847924Z",
     "iopub.status.idle": "2026-02-15T21:28:26.902770Z",
     "shell.execute_reply": "2026-02-15T21:28:26.900752Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 5 (reporting): Resumen de palancas de estrés ---\n",
    "\n",
    "# Configuración de directorios\n",
    "OUT_DIR = Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "\n",
    "# Cargar tabla de palancas si no está en memoria\n",
    "if 'lever_tbl' not in globals() or lever_tbl is None:\n",
    "    p1 = OUT_DIR / f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv'\n",
    "    p2 = OUT_DIR / 'phase5_stress_levers_summary.csv'\n",
    "    \n",
    "    if p1.exists():\n",
    "        lever_tbl = pd.read_csv(p1)\n",
    "        print(f\"Cargada tabla de palancas desde: {p1.name}\")\n",
    "    elif p2.exists():\n",
    "        lever_tbl = pd.read_csv(p2)\n",
    "        print(f\"Cargada tabla de palancas desde: {p2.name}\")\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            'No se encuentra tabla de palancas. '\n",
    "            'Ejecuta primero la celda principal de Fase 5.'\n",
    "        )\n",
    "\n",
    "# Seleccionar columnas relevantes para el reporte\n",
    "cols = [\n",
    "    'scenario', 'label', 'dependence_shock', 'tail_shock', 'vol_shock', 'drift_shock',\n",
    "    'df_used', 'vol_mult', 'tail_mult', 'mean_shift', 'apply_to',\n",
    "    'avg_corr_risky', 'avg_corr_bonds_hedge', 'avg_corr_hyg_equity',\n",
    "    'delta_avg_corr_risky', 'delta_avg_corr_bonds_hedge', 'delta_avg_corr_hyg_equity',\n",
    "    'desc'\n",
    "]\n",
    "\n",
    "# Filtrar columnas existentes y mostrar tabla\n",
    "lever_tbl = lever_tbl[[c for c in cols if c in lever_tbl.columns]]\n",
    "\n",
    "print(\"\\n=== RESUMEN DE PALANCAS DE ESTRÉS ===\")\n",
    "print(\"Esta tabla muestra los diferentes shocks aplicados en los escenarios de estrés\")\n",
    "print(\"y su impacto sobre las correlaciones entre diferentes bloques de activos.\\n\")\n",
    "\n",
    "display(lever_tbl)\n",
    "\n",
    "# Guardar versión actualizada del reporte\n",
    "output_path = OUT_DIR / f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv'\n",
    "lever_tbl.to_csv(output_path, index=False)\n",
    "print(f\"\\nReporte de palancas guardado en: {output_path}\")\n",
    "\n",
    "# Análisis adicional de palancas más importantes\n",
    "print(\"\\n=== ANÁLISIS DE PALANCAS CLAVE ===\")\n",
    "\n",
    "# Identificar escenarios con mayor impacto en correlaciones de riesgo\n",
    "if 'delta_avg_corr_risky' in lever_tbl.columns:\n",
    "    max_corr_impact = lever_tbl.loc[lever_tbl['delta_avg_corr_risky'].idxmax()]\n",
    "    print(f\"\\nEscenario con mayor impacto en correlación de activos de riesgo:\")\n",
    "    print(f\"  - {max_corr_impact['label']}\")\n",
    "    print(f\"  - Incremento correlación: {max_corr_impact['delta_avg_corr_risky']:.3f}\")\n",
    "    print(f\"  - Multiplicador volatilidad: {max_corr_impact['vol_mult']:.2f}\")\n",
    "    print(f\"  - Multiplicador cola: {max_corr_impact['tail_mult']:.2f}\")\n",
    "\n",
    "# Identificar escenarios con mayor shock de dependencia\n",
    "if 'dependence_shock' in lever_tbl.columns:\n",
    "    dep_shocks = lever_tbl[lever_tbl['dependence_shock'] == 'Si']\n",
    "    if len(dep_shocks) > 0:\n",
    "        print(f\"\\nEscenarios con shock de dependencia ({len(dep_shocks)}):\")\n",
    "        for _, row in dep_shocks.iterrows():\n",
    "            print(f\"  - {row['label']}: Δcorr_risky = {row.get('delta_avg_corr_risky', 'N/A'):.3f}\")\n",
    "\n",
    "# Resumen de tipos de shocks aplicados\n",
    "print(f\"\\n=== RESUMEN DE TIPOS DE SHOCKS APLICADOS ===\")\n",
    "shock_types = ['dependence_shock', 'tail_shock', 'vol_shock', 'drift_shock']\n",
    "for shock_type in shock_types:\n",
    "    if shock_type in lever_tbl.columns:\n",
    "        count = lever_tbl[lever_tbl[shock_type] == 'Si'].shape[0]\n",
    "        print(f\"  - {shock_type.replace('_', ' ').title()}: {count} escenarios\")\n",
    "\n",
    "print(f\"\\nTotal de escenarios analizados: {len(lever_tbl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f10ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:28:26.907818Z",
     "iopub.status.busy": "2026-02-15T21:28:26.907818Z",
     "iopub.status.idle": "2026-02-15T21:28:29.360220Z",
     "shell.execute_reply": "2026-02-15T21:28:29.358202Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Phase 5 - Plots de palancas (comité-friendly) ---\n",
    "\n",
    "OUT_DIR=Path(OUT_DIR) if 'OUT_DIR' in globals() else Path('outputs_taller')\n",
    "_lever_tbl=globals().get('lever_tbl')\n",
    "if _lever_tbl is None:\n",
    "    p=OUT_DIR/f'phase5_stress_levers_summary_6m_n{N_STRESS}.csv'\n",
    "    if not p.exists(): raise RuntimeError('No encuentro tabla de palancas. Ejecuta la celda anterior de Fase 5.')\n",
    "    _lever_tbl=pd.read_csv(p)\n",
    "\n",
    "if 'scenario' in _lever_tbl.columns:\n",
    "    order=['BASE_HMM_COPULA','S1_Estanflacion_2022','S2_Crisis_Credito_2008','S3_Alternativo_Liquidez_Global']\n",
    "    cats=[c for c in order if c in set(_lever_tbl['scenario'])]\n",
    "    _lever_tbl=_lever_tbl.copy(); _lever_tbl['scenario']=pd.Categorical(_lever_tbl['scenario'],categories=cats,ordered=True); _lever_tbl=_lever_tbl.sort_values('scenario')\n",
    "\n",
    "x=_lever_tbl['scenario'].astype(str).tolist() if 'scenario' in _lever_tbl.columns else [str(i) for i in range(len(_lever_tbl))]\n",
    "fig,axes=plt.subplots(2,3,figsize=(14,7),sharex=True)\n",
    "cm={'Delta corr (risky-risky)':'delta_avg_corr_risky','Delta corr (bonos-equity)':'delta_avg_corr_bonds_hedge','Delta corr (HYG-equity)':'delta_avg_corr_hyg_equity'}\n",
    "pm={'df (colas, ?)':'df_used','vol_mult':'vol_mult','tail_mult':'tail_mult'}\n",
    "for ax,(ttl,col) in zip(axes[0],cm.items()):\n",
    "    if col in _lever_tbl.columns:\n",
    "        y=pd.to_numeric(_lever_tbl[col],errors='coerce').to_numpy(); ax.bar(x,y,color=['#6c757d' if 'BASE' in s else '#0d6efd' for s in x]); lim=max(0.02,float(np.nanmax(np.abs(y)))*1.15) if np.isfinite(y).any() else 0.05; ax.axhline(0,color='black',lw=1); ax.set_ylim(-lim,lim)\n",
    "    else: ax.text(0.5,0.5,f'Falta columna: {col}',ha='center',va='center')\n",
    "    ax.set_title(ttl); ax.grid(True,axis='y',alpha=0.25)\n",
    "for ax,(ttl,col) in zip(axes[1],pm.items()):\n",
    "    if col in _lever_tbl.columns:\n",
    "        y=pd.to_numeric(_lever_tbl[col],errors='coerce').to_numpy(); ax.bar(x,y,color=['#6c757d' if 'BASE' in s else '#198754' for s in x]); ax.axhline(0,color='black',lw=1)\n",
    "    else: ax.text(0.5,0.5,f'Falta columna: {col}',ha='center',va='center')\n",
    "    ax.set_title(ttl); ax.grid(True,axis='y',alpha=0.25)\n",
    "for ax in axes[1]: ax.tick_params(axis='x',rotation=20)\n",
    "fig.suptitle('Fase 5 - Palancas por escenario (Delta dependencia y parámetros)', y=1.02)\n",
    "plt.tight_layout(); plt.show()\n",
    "if globals().get('EXPORT_STRESS',False):\n",
    "    out=OUT_DIR/f'phase5_levers_barplot_6m_n{N_STRESS}.png'; fig.savefig(out,dpi=160,bbox_inches='tight'); print('Saved:',out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d94d3",
   "metadata": {},
   "source": [
    "## Análisis Comparativo de Escenarios de Estrés\n",
    "\n",
    "### **Palancas de Dependencia (Fila Superior)**\n",
    "- **Delta corr (risky-risky)**: Cambios correlación activos de riesgo\n",
    "- **Delta corr (bonos-equity)**: Cambios correlación bonos defensivos\n",
    "- **Delta corr (HYG-equity)**: Cambios correlación crédito vs equity\n",
    "\n",
    "### **Parámetros de Modelo (Fila Inferior)**\n",
    "- **df (colas)**: Grados de libertad cópula t (colas gruesas)\n",
    "- **vol_mult**: Multiplicador volatilidad por régimen\n",
    "- **tail_mult**: Multiplicador tail risk por escenario\n",
    "\n",
    "### **Interpretación de Escenarios**\n",
    "- **BASE_HMM_COPULA**: Referencia baseline (gris)\n",
    "- **S1 Estanflación 2022**: Shock inflación + tasas (azul)\n",
    "- **S2 Crisis Crédito 2008**: Stress financiero (azul)\n",
    "- **S3 Liquidez Global**: Crisis sistémica (azul)\n",
    "\n",
    "### **Aplicaciones de Comité**\n",
    "- **Stress testing visual**: Identificación de riesgos clave\n",
    "- **Comparación escenarios**: Impacto relativo por palanca\n",
    "- **Decisión informada**: Base para gestión de riesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9196b8",
   "metadata": {},
   "source": [
    "## 6) Fase 6 - Reverse stress testing\n",
    "\n",
    "Pregunta de gestion: cual es el shock mínimo para cruzar un umbral de pérdida inaceptable.\n",
    "\n",
    "Enfoque aplicado:\n",
    "\n",
    "- baseline en régimen Estrés,\n",
    "- interpolación de intensidad `lambda in [0,1]` sobre familias de shocks,\n",
    "- búsqueda del `lambda` mínimo que rompe el umbral objetivo.\n",
    "\n",
    "Outputs: grid completo, mínimo por familia, mejor global y análisis de sensibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd41fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:28:29.366606Z",
     "iopub.status.busy": "2026-02-15T21:28:29.365093Z",
     "iopub.status.idle": "2026-02-15T21:29:06.534922Z",
     "shell.execute_reply": "2026-02-15T21:29:06.532891Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fase 6: Reverse stress testing (intensidad mínima de shock λ) ---\n",
    "\n",
    "# Configuración de parámetros\n",
    "N_REV = 25_000          # Número de simulaciones para reverse stress\n",
    "SEED_REV = 777          # Semilla para reproducibilidad\n",
    "EXPORT_REV = True       # Exportar resultados a archivos CSV\n",
    "\n",
    "# Métrica objetivo y umbral\n",
    "TARGET_METRIC = \"ES5\"   # Métrica objetivo: {\"ES5\",\"VaR5\",\"ES1\",\"VaR1\"}\n",
    "TARGET_MULT = 1.60      # Multiplicador del umbral: umbral = métrica_base * TARGET_MULT (más negativo)\n",
    "TARGET_ABS = None       # Umbral absoluto opcional: ej. -0.04 (anula TARGET_MULT si no es None)\n",
    "\n",
    "# Multiplicadores de sensibilidad para justificación (usa misma escala)\n",
    "SENSITIVITY_MULTS = [1.4, 1.6, 1.8]\n",
    "\n",
    "# Verificación de prerequisitos\n",
    "required = [\"assets\", \"histS\", \"base_Ct\", \"base_df\", \"scenario_specs\", \"w\"]\n",
    "missing = [k for k in required if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan prerequisitos para Fase 6: {missing}. Ejecutar Fase 5 primero.\")\n",
    "\n",
    "# Configuración de directorios\n",
    "OUT_DIR = Path(OUT_DIR) if \"OUT_DIR\" in globals() else Path(\"outputs_taller\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reutilizar funciones auxiliares de Fase 5, verificar disponibilidad\n",
    "if \"_nearest_psd_corr\" not in globals():\n",
    "    raise RuntimeError(\"Falta _nearest_psd_corr. Ejecutar celda de Fase 5 primero.\")\n",
    "if \"_simulate_state_from_copula\" not in globals():\n",
    "    raise RuntimeError(\"Falta _simulate_state_from_copula. Ejecutar celda de Fase 5 primero.\")\n",
    "if \"_shock_marginal_vol_mean\" not in globals() or \"_shock_tail_left\" not in globals():\n",
    "    raise RuntimeError(\"Faltan funciones de shock marginal. Ejecutar celda de Fase 5 primero.\")\n",
    "if \"_portfolio_metrics\" not in globals():\n",
    "    raise RuntimeError(\"Falta _portfolio_metrics. Ejecutar celda de Fase 5 primero.\")\n",
    "\n",
    "def _blend_corr(C0: np.ndarray, C1: np.ndarray, lam: float) -> np.ndarray:\n",
    "    \"\"\"Mezclar matrices de correlación usando parámetro λ.\"\"\"\n",
    "    lam = float(np.clip(lam, 0.0, 1.0))\n",
    "    C = (1.0 - lam) * np.asarray(C0, dtype=float) + lam * np.asarray(C1, dtype=float)\n",
    "    np.fill_diagonal(C, 1.0)\n",
    "    return _nearest_psd_corr(C)\n",
    "\n",
    "def _lin(a: float, b: float, lam: float) -> float:\n",
    "    \"\"\"Interpolación lineal entre dos valores usando parámetro λ.\"\"\"\n",
    "    lam = float(np.clip(lam, 0.0, 1.0))\n",
    "    return float((1.0 - lam) * a + lam * b)\n",
    "\n",
    "def _eval_lambda(spec: dict, lam: float, n: int, seed: int) -> dict:\n",
    "    \"\"\"Evaluar métricas para un valor específico de λ.\"\"\"\n",
    "    # Mezclar parámetros del escenario\n",
    "    Ct_l = _blend_corr(base_Ct, np.asarray(spec[\"Ct\"], dtype=float), lam)\n",
    "    df_l = _lin(float(base_df), float(spec[\"df\"]), lam)\n",
    "    vol_mult_l = _lin(1.0, float(spec[\"vol_mult\"]), lam)\n",
    "    tail_mult_l = _lin(1.0, float(spec[\"tail_mult\"]), lam)\n",
    "\n",
    "    # Simular retornos\n",
    "    rng = np.random.default_rng(seed)\n",
    "    R = _simulate_state_from_copula(n, Ct_l, df_l, histS, rng)\n",
    "\n",
    "    # Aplicar shocks marginales\n",
    "    cols = spec.get(\"apply_cols\")\n",
    "    R = _shock_marginal_vol_mean(R, vol_mult=vol_mult_l, mean_shift=0.0, cols=cols)\n",
    "    R = _shock_tail_left(R, q=0.05, tail_mult=tail_mult_l, cols=cols)\n",
    "\n",
    "    # Calcular métricas de cartera\n",
    "    m = _portfolio_metrics(R, w)\n",
    "\n",
    "    # Agregar información de palancas para reporte\n",
    "    apply_cols = spec.get(\"apply_cols\")\n",
    "    if isinstance(apply_cols, list):\n",
    "        apply_names = [assets[i] for i in apply_cols if 0 <= i < len(assets)]\n",
    "        apply_to = \",\".join(apply_names) if apply_names else \"(subset)\"\n",
    "    else:\n",
    "        apply_to = \"all\"\n",
    "\n",
    "    return {\n",
    "        \"family\": spec[\"name\"],\n",
    "        \"lambda\": float(lam),\n",
    "        \"n\": int(n),\n",
    "        \"seed\": int(seed),\n",
    "        \"df_used\": float(df_l),\n",
    "        \"vol_mult\": float(vol_mult_l),\n",
    "        \"tail_mult\": float(tail_mult_l),\n",
    "        \"apply_to\": apply_to,\n",
    "        **m,\n",
    "    }\n",
    "\n",
    "# Calcular métrica base bajo modelo Estrés (λ=0)\n",
    "if \"R_base\" in globals() and isinstance(globals().get(\"R_base\"), np.ndarray):\n",
    "    base_metrics = _portfolio_metrics(R_base, w)\n",
    "else:\n",
    "    rng0 = np.random.default_rng(SEED_REV)\n",
    "    R0 = _simulate_state_from_copula(N_REV, base_Ct, float(base_df), histS, rng0)\n",
    "    base_metrics = _portfolio_metrics(R0, w)\n",
    "\n",
    "baseline_value = float(base_metrics.get(TARGET_METRIC, np.nan))\n",
    "if not np.isfinite(baseline_value):\n",
    "    raise RuntimeError(f\"Métrica base {TARGET_METRIC} es NaN; no se puede hacer reverse stress.\")\n",
    "\n",
    "# Determinar valor objetivo\n",
    "if TARGET_ABS is not None:\n",
    "    target_value = float(TARGET_ABS)\n",
    "else:\n",
    "    target_value = float(baseline_value * TARGET_MULT)\n",
    "\n",
    "print(f\"Métrica base {TARGET_METRIC} (modelo Estrés): {baseline_value:.4%}\")\n",
    "print(f\"Objetivo   {TARGET_METRIC}: {target_value:.4%}  (TARGET_MULT={TARGET_MULT}, TARGET_ABS={TARGET_ABS})\")\n",
    "\n",
    "# Búsqueda en escala sobre λ para cada familia de escenarios de Fase 5\n",
    "lams = np.linspace(0.0, 1.0, 21)\n",
    "rows = []\n",
    "\n",
    "print(f\"\\nEjecutando búsqueda en escala para {len(scenario_specs)} familias de escenarios...\")\n",
    "for spec in scenario_specs:\n",
    "    print(f\"  Procesando familia: {spec['name']}\")\n",
    "    for lam in lams:\n",
    "        rows.append(_eval_lambda(spec, float(lam), n=N_REV, seed=SEED_REV))\n",
    "\n",
    "grid = pd.DataFrame(rows)\n",
    "print(f\"Completada evaluación de {len(rows)} puntos de escala\")\n",
    "\n",
    "# Encontrar λ mínimo que cruza el objetivo (para métricas de pérdida, más negativo es \"peor\")\n",
    "# Asumimos que VaR/ES son negativos en espacio de retornos\n",
    "def _crosses(val: float, target: float) -> bool:\n",
    "    \"\"\"Determinar si un valor cruza el umbral objetivo.\"\"\"\n",
    "    return float(val) <= float(target)\n",
    "\n",
    "# Análisis de sensibilidad en el umbral (sin simulaciones adicionales, reusa escala)\n",
    "if TARGET_ABS is None:\n",
    "    print(f\"\\nAnalizando sensibilidad para multiplicadores: {SENSITIVITY_MULTS}\")\n",
    "    sens_rows = []\n",
    "    \n",
    "    for mult in SENSITIVITY_MULTS:\n",
    "        target_val = float(baseline_value * mult)\n",
    "        print(f\"  Evaluando multiplicador {mult}: objetivo = {target_val:.4%}\")\n",
    "        \n",
    "        for fam, g in grid.groupby(\"family\", sort=False):\n",
    "            g = g.sort_values(\"lambda\")\n",
    "            metric_series = g[TARGET_METRIC]\n",
    "            idx = None\n",
    "            \n",
    "            for i in range(len(g)):\n",
    "                v = float(metric_series.iloc[i])\n",
    "                if np.isfinite(v) and _crosses(v, target_val):\n",
    "                    idx = i\n",
    "                    break\n",
    "            \n",
    "            sens_rows.append({\n",
    "                \"target_mult\": mult,\n",
    "                \"target_value\": target_val,\n",
    "                \"family\": fam,\n",
    "                \"min_lambda\": float(g.iloc[idx][\"lambda\"]) if idx is not None else np.nan,\n",
    "                \"metric_at_min\": float(g.iloc[idx][TARGET_METRIC]) if idx is not None else np.nan,\n",
    "                \"status\": \"CROSS\" if idx is not None else \"NO_CROSS\",\n",
    "            })\n",
    "    \n",
    "    sens_tbl = pd.DataFrame(sens_rows)\n",
    "    print(\"\\n=== TABLA DE SENSIBILIDAD ===\")\n",
    "    display(sens_tbl)\n",
    "    \n",
    "    if EXPORT_REV:\n",
    "        out_sens = OUT_DIR / f\"phase6_reverse_stress_sensitivity_1d_n{N_REV}.csv\"\n",
    "        sens_tbl.to_csv(out_sens, index=False)\n",
    "        print(f\"Tabla de sensibilidad guardada en: {out_sens}\")\n",
    "else:\n",
    "    print(\"[Info] TARGET_ABS establecido; tabla de sensibilidad usa TARGET_MULTs solo cuando TARGET_ABS es None.\")\n",
    "\n",
    "# Encontrar λ mínimo para cada familia\n",
    "print(f\"\\nBuscando λ mínimo que cruza el objetivo {target_value:.4%}...\")\n",
    "min_rows = []\n",
    "\n",
    "for fam, g in grid.groupby(\"family\", sort=False):\n",
    "    g = g.sort_values(\"lambda\")\n",
    "    metric_series = g[TARGET_METRIC]\n",
    "    idx = None\n",
    "    \n",
    "    for i in range(len(g)):\n",
    "        v = float(metric_series.iloc[i])\n",
    "        if np.isfinite(v) and _crosses(v, target_value):\n",
    "            idx = i\n",
    "            break\n",
    "    \n",
    "    if idx is None:\n",
    "        min_rows.append({\"family\": fam, \"lambda_min\": np.nan, \"status\": \"NO_CROSS\"})\n",
    "        print(f\"  {fam}: NO CRUZA el objetivo\")\n",
    "        continue\n",
    "\n",
    "    lam_hit = float(g[\"lambda\"].iloc[idx])\n",
    "    metric_hit = float(g[TARGET_METRIC].iloc[idx])\n",
    "    print(f\"  {fam}: λ_min = {lam_hit:.3f} (métrica = {metric_hit:.4%})\")\n",
    "    \n",
    "    min_rows.append({\n",
    "        \"family\": fam,\n",
    "        \"lambda_min\": lam_hit,\n",
    "        \"status\": \"CROSS\",\n",
    "        TARGET_METRIC: float(g[TARGET_METRIC].iloc[idx]),\n",
    "        \"VaR5\": float(g[\"VaR5\"].iloc[idx]),\n",
    "        \"ES5\": float(g[\"ES5\"].iloc[idx]),\n",
    "        \"VaR1\": float(g[\"VaR1\"].iloc[idx]),\n",
    "        \"ES1\": float(g[\"ES1\"].iloc[idx]),\n",
    "        \"df_used\": float(g[\"df_used\"].iloc[idx]),\n",
    "        \"vol_mult\": float(g[\"vol_mult\"].iloc[idx]),\n",
    "        \"tail_mult\": float(g[\"tail_mult\"].iloc[idx]),\n",
    "        \"apply_to\": str(g[\"apply_to\"].iloc[idx]),\n",
    "    })\n",
    "\n",
    "min_tbl = pd.DataFrame(min_rows)\n",
    "\n",
    "# Encontrar mejor opción (mínimo λ entre familias)\n",
    "best = min_tbl[min_tbl[\"status\"] == \"CROSS\"].copy()\n",
    "if len(best) > 0:\n",
    "    best = best.sort_values([\"lambda_min\", TARGET_METRIC], ascending=[True, True]).head(1)\n",
    "    best_family = str(best[\"family\"].iloc[0])\n",
    "    best_lambda = float(best[\"lambda_min\"].iloc[0])\n",
    "    best_metric = float(best[TARGET_METRIC].iloc[0])\n",
    "    \n",
    "    print(f\"\\n=== MEJOR REVERSE STRESS (λ mínimo) ===\")\n",
    "    print(f\"Familia: {best_family}\")\n",
    "    print(f\"λ mínimo: {best_lambda:.3f}\")\n",
    "    print(f\"Métrica {TARGET_METRIC}: {best_metric:.4%}\")\n",
    "    print(f\"Intensidad de shock requerida: {(best_lambda * 100):.1f}% del shock máximo\")\n",
    "else:\n",
    "    best_family, best_lambda = None, np.nan\n",
    "    print(f\"\\nADVERTENCIA: Ninguna familia cruza el objetivo en esta escala de λ.\")\n",
    "    print(f\"Considerar disminuir TARGET_MULT o establecer TARGET_ABS más cercano a la línea base.\")\n",
    "\n",
    "# Mostrar tabla de resultados\n",
    "show_cols = [\"family\", \"lambda_min\", \"status\"]\n",
    "extra = [c for c in [TARGET_METRIC, \"ES5\", \"VaR5\", \"ES1\", \"VaR1\", \"df_used\", \"vol_mult\", \"tail_mult\", \"apply_to\"] \n",
    "          if c in min_tbl.columns]\n",
    "show_cols = show_cols + [c for c in extra if c not in show_cols]\n",
    "\n",
    "print(f\"\\n=== RESUMEN DE λ MÍNIMOS POR FAMILIA ===\")\n",
    "display(min_tbl[show_cols])\n",
    "\n",
    "# Visualizar métrica vs λ\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "for fam, g in grid.groupby(\"family\", sort=False):\n",
    "    g = g.sort_values(\"lambda\")\n",
    "    ax.plot(g[\"lambda\"], g[TARGET_METRIC], marker=\"o\", lw=1.6, label=fam)\n",
    "\n",
    "ax.axhline(target_value, color=\"black\", ls=\"--\", lw=1, label=f\"objetivo {TARGET_METRIC}\")\n",
    "ax.set_title(f\"Fase 6 — Reverse stress: {TARGET_METRIC} vs λ (n={N_REV})\")\n",
    "ax.set_xlabel(\"Intensidad de shock λ (0=base Estrés, 1=stress Fase5)\")\n",
    "ax.set_ylabel(TARGET_METRIC)\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(ncol=2, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exportar resultados\n",
    "if EXPORT_REV:\n",
    "    out_grid = OUT_DIR / f\"phase6_reverse_stress_grid_1d_n{N_REV}.csv\"\n",
    "    grid.to_csv(out_grid, index=False)\n",
    "    print(f\"\\nEscala de resultados guardada en: {out_grid}\")\n",
    "\n",
    "    out_min = OUT_DIR / f\"phase6_reverse_stress_minimum_1d_n{N_REV}.csv\"\n",
    "    min_tbl.to_csv(out_min, index=False)\n",
    "    print(f\"Tabla de λ mínimos guardada en: {out_min}\")\n",
    "\n",
    "    if globals().get(\"EXPORT_STRESS\", False):\n",
    "        out_png = OUT_DIR / f\"phase6_reverse_stress_plot_1d_n{N_REV}.png\"\n",
    "        fig.savefig(out_png, dpi=160, bbox_inches=\"tight\")\n",
    "        print(f\"Gráfico guardado en: {out_png}\")\n",
    "\n",
    "print(f\"\\n=== RESUMEN EJECUTIVO DE REVERSE STRESS ===\")\n",
    "if best_family is not None:\n",
    "    print(f\"• El escenario {best_family} requiere la menor intensidad de shock ({best_lambda:.1%})\")\n",
    "    print(f\"• Para alcanzar {TARGET_METRIC} de {target_value:.4%} vs base {baseline_value:.4%}\")\n",
    "    print(f\"• Esto representa un shock de {(best_lambda * 100):.1f}% de la intensidad máxima del escenario\")\n",
    "else:\n",
    "    print(f\"• No se encontró solución factible con los parámetros actuales\")\n",
    "    print(f\"• Considerar ajustar TARGET_MULT o TARGET_ABS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fed514",
   "metadata": {},
   "source": [
    "## 7) Conclusiones técnicas\n",
    "\n",
    "- El esquema CORE/FULL/diagnostico 2006 preserva 2008, incorpora ENPH con proxy de forma controlada y limita el sesgo de GME mediante politica configurable.\n",
    "- La clasificación de regímenes separa períodos de calma y estrés con señales macro consistentes (volatilidad, drawdown y crédito), incluyendo el bloque 2008-2009.\n",
    "- En riesgo marginal, HYG muestra un salto fuerte de volatilidad en estrés (x2.70 vs normal), mientras que GLD no mantiene un perfil de refugio robusto en este corte.\n",
    "- En dependencia, el estado de estrés presenta mas correlación media y mayor dependencia de cola que el estado normal, lo que confirma deterioro de diversificación.\n",
    "- El motor Monte Carlo replica bien magnitudes de riesgo base (volatilidad y cola diaria) y reproduce de forma coherente el peso relativo de cada régimen.\n",
    "- En escenarios forzados, la Crisis de Crédito 2008 es el caso mas severo en VaR/ES 99% a 6 meses.\n",
    "- En reverse stress, el umbral crítico se alcanza antes con el escenario de crédito, lo que lo posiciona como principal vulnerabilidad de la cartera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85272f01",
   "metadata": {},
   "source": [
    "## Control de entregables\n",
    "\n",
    "La celda final verifica automaticamente que existan los outputs tecnicos clave generados por el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef0467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T21:29:06.541163Z",
     "iopub.status.busy": "2026-02-15T21:29:06.540155Z",
     "iopub.status.idle": "2026-02-15T21:29:06.562593Z",
     "shell.execute_reply": "2026-02-15T21:29:06.560534Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Checklist técnico de entregables ---\n",
    "\n",
    "out = Path(OUT_DIR)\n",
    "checks = [\n",
    "    (\"Fase 2: tabla riesgo por estado\", out / \"phase2_risk_by_state.csv\"),\n",
    "    (\"Fase 3: copula summary\", out / \"phase3_copula_summary.json\"),\n",
    "    (\"Fase 4: wealth bands 6m 10k\", out / \"phase4_wealth_bands_6m_n10000.csv\"),\n",
    "    (\"Fase 4: reproducción regímenes\", out / \"phase4_regime_reproduction_6m_n10000.csv\"),\n",
    "    (\"Fase 4: validación riesgo cartera\", out / \"phase4_portfolio_risk_validation_6m_n10000.csv\"),\n",
    "    (\"Fase 4: validación dependencia\", out / \"phase4_dependence_validation_6m_n10000.csv\"),\n",
    "    (\"Fase 5: resumen escenarios estres\", out / \"phase5_stress_summary_6m_n10000.csv\"),\n",
    "    (\"Fase 5: palancas escenarios\", out / \"phase5_stress_levers_summary_6m_n10000.csv\"),\n",
    "    (\"Fase 6: reverse stress mínimo\", out / \"phase6_reverse_stress_minimum_1d_n25000.csv\"),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for item, path in checks:\n",
    "    rows.append({\"item\": item, \"status\": \"OK\" if path.exists() else \"FALTA\", \"path\": str(path)})\n",
    "\n",
    "check_df = pd.DataFrame(rows)\n",
    "display(check_df)\n",
    "\n",
    "n_missing = int((check_df[\"status\"] == \"FALTA\").sum())\n",
    "if n_missing == 0:\n",
    "    print(\"Checklist técnico: completo.\")\n",
    "else:\n",
    "    print(f\"Checklist técnico: faltan {n_missing} outputs.\")\n",
    "\n",
    "print(\"Control de entregables: se han verificado los archivos técnicos clave en outputs_taller/.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
